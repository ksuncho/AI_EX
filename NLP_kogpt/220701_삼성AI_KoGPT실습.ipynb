{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"220701_삼성AI_KoGPT실습.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Z2LGcoawrlQ7"},"source":["# 0. Install Packages"]},{"cell_type":"code","metadata":{"id":"9xwaztFKrmDt"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2TQx6kl-rkdM"},"source":["# 1. Import Packages"]},{"cell_type":"markdown","metadata":{"id":"YuKXZmdcrkdS"},"source":[" - 본 실습에 필요한 패키지들을 불러옵니다."]},{"cell_type":"code","metadata":{"id":"fR-OY7uhrkdS"},"source":["from transformers import GPT2Model\n","from transformers import GPT2LMHeadModel\n","from transformers import PreTrainedTokenizerFast\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import urllib\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvObBbHnrkdT"},"source":["# 2. KoGPT2 Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"PEiF3thorkdU"},"source":[" - 사전 학습된 KoGPT2 Tokenizer를 불러옵니다."]},{"cell_type":"code","metadata":{"id":"mn3NQZyrrkdV"},"source":["tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', padding_side='right') \n","sample_text = \"근육이 커지기 위해서는\"\n","\n","tokens = tokenizer.tokenize(sample_text)\n","token_ids = tokenizer.encode(sample_text)\n","\n","print(f' Sentence: {sample_text}')\n","print(f'   Tokens: {tokens}')\n","print(f'Token IDs: {token_ids}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iyd2Ya1RrkdW"},"source":["# 3. KoGPT2 Models"]},{"cell_type":"markdown","metadata":{"id":"q7O35G28rkdW"},"source":[" - GPT2Model과 GPT2LMHeadModel을 불러옵니다."]},{"cell_type":"markdown","metadata":{"id":"vqeeLgoQrkdW"},"source":["## 3-1. GPT2Model"]},{"cell_type":"markdown","metadata":{"id":"YvrfHXB6rkdX"},"source":[" - GPT2Model은 hidden state를 출력합니다.\n"," \n"," - 본 예제에서는 네 개의 토큰에 대한 768차원의 벡터가 도출됩니다."]},{"cell_type":"code","metadata":{"id":"NiWFyvYLrkdX"},"source":["gpt2_model = GPT2Model.from_pretrained('skt/kogpt2-base-v2')\n","hidden_states = gpt2_model(torch.tensor([token_ids]))\n","last_hidden_state = hidden_states[0]\n","print(last_hidden_state.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PUbrSbg9rkdX"},"source":["## 3-2. GPT2LMHeadModel"]},{"cell_type":"markdown","metadata":{"id":"KM2GGr6prkdX"},"source":[" - GPT2LMHead는 next word prediction을 출력합니다.\n"," \n"," - 본 예제에서는 네 개의 토큰에 대한 51200 차원의 단어 확률 분포가 도출됩니다."]},{"cell_type":"code","metadata":{"id":"cFyDm2aprkdY"},"source":["gpt2lm_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","outputs = gpt2lm_model(torch.tensor([token_ids]))\n","next_word_predictions = outputs[0]\n","print(next_word_predictions.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2hAXUowrkdY"},"source":[" - 단어 확률 분포에 대해 argmax를 취해 가장 높은 확률을 보이는 단어를 찾습니다.\n"," \n"," - 본 예제에서는 \"무엇보다\" 라는 단어가 가장 높은 확률을 나타냅니다."]},{"cell_type":"code","metadata":{"id":"MXKCYYAIrkdY"},"source":["next_word_distribution = next_word_predictions[0, -1, :]\n","next_word_id = torch.argmax(next_word_distribution)\n","next_word = tokenizer.decode(next_word_id)\n","\n","print(f'Next word: {next_word}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dm63zr0yrkdZ"},"source":["# 4. Text Generation Examples (Pre-trained model)"]},{"cell_type":"markdown","metadata":{"id":"eC7r6SZorkdZ"},"source":[" - 두 가지 Text Generation 방법을 실험해봅니다."]},{"cell_type":"markdown","metadata":{"id":"NSAms2NyrkdZ"},"source":["## 4-1. Greedy Search"]},{"cell_type":"markdown","metadata":{"id":"Nzw0MSrhrkdZ"},"source":[" - Greedy Search는 가장 높은 확률의 단어를 Greedy하게 찾는 방식으로 텍스트를 생성합니다."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"DQcKnXhUrkdZ"},"source":["gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]),\n","                           max_length=127,\n","                           repetition_penalty=2.0,\n","                           )\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hB3dzvRRrkda"},"source":["## 4-2. Beam Search"]},{"cell_type":"markdown","metadata":{"id":"VYeC4nggrkda"},"source":[" - Beam Search는 매 step마다 num_beams 개 만큼의 Top word selection path를 찾습니다.\n"]},{"cell_type":"code","metadata":{"id":"2NxnOmsg1viV"},"source":["gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]),\n","                           max_length=127,\n","                           repetition_penalty=2.0,\n","                           num_beams=5, \n","                           )\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EeNelLxerkda"},"source":["# 5. Fine tunning (Naver Movie review)"]},{"cell_type":"markdown","metadata":{"id":"t4MoKU6nrkda"},"source":[" - 네이버 영화 리뷰데이터를 활용하여 모델을 Fine Tuning 합니다."]},{"cell_type":"markdown","metadata":{"id":"mREdF7iarkda"},"source":["## 5-1. Get Datasets"]},{"cell_type":"markdown","metadata":{"id":"gJre_mPgrkda"},"source":[" - github으로부터 네이버 영화 리뷰데이터를 요청하여 내 pc에 저장합니다.\n"," \n"," - 데이터의 크기가 너무 큰 관계로, 본 실험에서는 테스트 데이터 셋만을 활용하여 모델을 학습시킵니다. "]},{"cell_type":"code","metadata":{"id":"xUdSUHaRrkdb"},"source":["def get_naver_review_examples():\n","    #urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n","\n","    #train_data = pd.read_table('ratings_train.txt')\n","    test_data = pd.read_table('ratings_test.txt')\n","    \n","    return test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9y-DCJAki786"},"source":["naver_data = get_naver_review_examples()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEw-UUV6i9Z-"},"source":["naver_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ljJUSVn3rkdb"},"source":[" - Dataset Loader를 정의합니다."]},{"cell_type":"code","metadata":{"id":"NmEdH-fdrkdb"},"source":["class NaverReviewDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __getitem__(self, item):\n","        text = str(self.texts[item])\n","        label = self.labels[item]\n","\n","        encoding = self.tokenizer.encode_plus(\n","          text,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          return_token_type_ids=False,\n","          padding='max_length',\n","          return_attention_mask=True,\n","          return_tensors='pt',\n","          truncation=True,\n","        )\n","\n","        return {\n","          'text': text,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","    \n","    def __len__(self):\n","        return len(self.texts)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUcXquqfjZAn"},"source":["dataset = NaverReviewDataset(naver_data['document'], naver_data['label'], tokenizer, 100)\n","train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [40000, 5000, 5000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set[0]"],"metadata":{"id":"RrkzJ8KroI4K"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHXFcBEerkdb"},"source":["batch_size = 8\n","\n","train_dataloader = DataLoader(train_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","valid_dataloader = DataLoader(valid_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","test_dataloader = DataLoader(test_set, batch_size=batch_size,\n","                        shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_data = next(iter(test_dataloader))\n","sample_data"],"metadata":{"id":"KzN_EHt7op82"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMj2_y_Zrkdc"},"source":["## 5-2. Model Settings"]},{"cell_type":"markdown","metadata":{"id":"MoEvXVKbrkdc"},"source":[" - Model의 환경을 설정합니다."]},{"cell_type":"code","metadata":{"id":"wn3YXDc5rkdc"},"source":["gpt2lm_model.train()\n","\n","learning_rate = 1e-5\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(gpt2lm_model.parameters(), lr=learning_rate)\n","\n","device = 'cuda'\n","\n","epochs = 10\n","count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_inputs = sample_data['input_ids'].to(device)\n","sample_outputs = gpt2lm_model(sample_inputs, labels=sample_inputs)"],"metadata":{"id":"eZaZLaCqph3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_outputs['loss']"],"metadata":{"id":"dSAgM2yZp78r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m15km-r2rkdc"},"source":["## 5-3. Model Training"]},{"cell_type":"markdown","metadata":{"id":"58yCR-xarkdd"},"source":[" - Model의 학습을 시작합니다."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"iUoZ4cP8rkdd"},"source":["tot_train_loss = 0.0\n","tot_valid_loss = 0.0\n","prev_valid_loss = 10000\n","\n","print('KoGPT-2 Training Start!')\n","\n","for epoch in range(epochs):\n","    for batch, train_data in enumerate(train_dataloader):\n","        # train data를 모델에 입력하여 출력 값을 얻습니다.\n","        gpt2lm_model.to(device)\n","        train_inputs = train_data['input_ids'].to(device)\n","        train_outputs = gpt2lm_model(train_inputs, labels=train_inputs) # train_outputs = (train_loss, train_logits, train_past_hidden_states)\n","        \n","        train_loss, _ = train_outputs[:2]\n","        \n","        # valid data를 모델에 입력하여 출력 값을 얻습니다.\n","            \n","        valid_data = next(iter(valid_dataloader))\n","\n","        gpt2lm_model.to(device)\n","        valid_inputs = valid_data['input_ids'].to(device)     \n","        valid_outputs = gpt2lm_model(valid_inputs, labels=valid_inputs)\n","        \n","        valid_loss, _ = valid_outputs[:2]\n","        \n","        gpt2lm_model.to(device)\n","        \n","        # train loss를 토대로 모델을 학습합니다.\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","        \n","        tot_train_loss += train_loss.item()\n","        tot_valid_loss += valid_loss.item()\n","               \n","        # 200 batch 마다 학습 상황을 화면에 출력합니다.\n","        if count % 200 == 0:\n","            cnt = ((count+1) * batch_size)\n","            current_train_loss = tot_train_loss / cnt\n","            current_valid_loss = tot_valid_loss / cnt\n","            \n","            print(f'epoch : %5d | batch : %5d | train_loss : %.5f | valid_loss : %.5f' %(epoch+1, batch+1, current_train_loss, current_valid_loss))\n","            \n","            tot_train_loss = 0.0\n","            tot_valid_loss = 0.0\n","            \n","            count = 0\n","            \n","            # 이전 valid_loss 보다 현재의 valid_loss가 더 낮을 경우, 모델을 저장합니다.\n","            if prev_valid_loss > current_valid_loss:\n","                prev_valid_loss = current_valid_loss\n","                torch.save(gpt2lm_model.state_dict(), f'./KoGPT-model.pth')\n","        \n","        count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mpX6-BeE9NcR"},"source":["train_set[5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKp_ygcR8VhK"},"source":["kogpt_load_path = f\"./KoGPT-model.pth\"\n","\n","gpt2lm_model.load_state_dict(torch.load(kogpt_load_path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVNd9zhPrkde"},"source":["gpt2lm_model.to(device)\n","\n","sample_text = \"정말 재미\"\n","\n","tokens = tokenizer.tokenize(sample_text)\n","token_ids = tokenizer.encode(sample_text)\n","\n","gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]).to(device),\n","                           max_length=127,\n","                           repetition_penalty=1.0,\n","                           num_beams=5)\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Fs-HK23-BH4"},"source":["import re\n","\n","p = re.compile('<pad>')\n","re.sub(p, '', generated)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiDq9EVFrkde"},"source":["# 6. Fine Tuning 2 (Classification Task)"]},{"cell_type":"markdown","metadata":{"id":"j4VEfnherkde"},"source":[" - Dateset을 가져옵니다."]},{"cell_type":"code","metadata":{"id":"9Xcj-OEirkde"},"source":["tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', padding_side='left') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzabGZvIrkde"},"source":["batch_size = 16\n","\n","naver_data = get_naver_review_examples()\n","\n","dataset = NaverReviewDataset(naver_data['document'], naver_data['label'], tokenizer, 100)\n","train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [40000, 5000, 5000])\n","\n","train_dataloader = DataLoader(train_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","valid_dataloader = DataLoader(valid_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","test_dataloader = DataLoader(test_set, batch_size=batch_size,\n","                        shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJPwD9Xerkde"},"source":[" - GPT Classifier를 정의합니다."]},{"cell_type":"code","metadata":{"id":"Wod_VKl7rkdf"},"source":["class GPT2SentimentClassifier(torch.nn.Module):\n","\n","    def __init__(self, n_classes):\n","        super(GPT2SentimentClassifier, self).__init__()\n","\n","        self.gpt_model = GPT2Model.from_pretrained('skt/kogpt2-base-v2')\n","        self.drop = torch.nn.Dropout(p=0.1)\n","        self.out = torch.nn.Linear(self.gpt_model.config.hidden_size, n_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        hidden_states = self.gpt_model(\n","          input_ids=input_ids,\n","          attention_mask=attention_mask\n","        )\n","        last_hidden_state = hidden_states[0]\n","        \n","        output = self.drop(last_hidden_state[:, -1, :])\n","\n","        return self.out(output)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zBBgK4LDrkdf"},"source":[" - Model의 환경을 설정합니다."]},{"cell_type":"code","metadata":{"id":"KS8te_26rkdf"},"source":["gpt_clf = GPT2SentimentClassifier(n_classes=1)\n","gpt_clf.train()\n","\n","learning_rate = 5e-5\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(gpt_clf.parameters(), lr=learning_rate)\n","\n","device = 'cuda'\n","\n","epochs = 1\n","count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zu6wBkkgrkdf"},"source":[" - 정확도 계산 함수를 정의합니다. "]},{"cell_type":"code","metadata":{"id":"bLNnDiNgrkdf"},"source":["def cal_correct_num(predicts, labels):\n","    predicts_ = predicts >= 0.5\n","    correct_num = torch.sum(predicts_ == labels)\n","        \n","    return correct_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ad3KaQHCrkdg"},"source":[" - Model의 학습을 시작합니다"]},{"cell_type":"code","metadata":{"id":"38LkHRZ1rkdg"},"source":["tot_train_loss = 0.0\n","tot_valid_loss = 0.0\n","\n","train_correct_num = 0\n","valid_correct_num = 0\n","\n","prev_valid_loss = 10000\n","\n","print('KoGPT-2 Training Start!')\n","\n","for epoch in range(epochs):\n","    for batch, train_data in enumerate(train_dataloader):\n","        # train data를 모델에 입력하여 출력 값을 얻습니다.\n","        gpt_clf.to(device)\n","        train_inputs = train_data['input_ids'].to(device)\n","        train_masks = train_data['attention_mask'].to(device)\n","        train_labels = train_data['labels'].to(device)\n","        \n","        train_outputs = gpt_clf(train_inputs, train_masks)\n","        \n","        train_loss = criterion(train_outputs.view(-1), train_labels.float())\n","        \n","        # valid data를 모델에 입력하여 출력 값을 얻습니다.\n","            \n","        valid_data = next(iter(valid_dataloader))\n","\n","        gpt_clf.to(device)\n","        valid_inputs = valid_data['input_ids'].to(device)    \n","        valid_masks = valid_data['attention_mask'].to(device)\n","        valid_labels = valid_data['labels'].to(device)\n","        \n","        valid_outputs = gpt_clf(valid_inputs, valid_masks)\n","        \n","        valid_loss = criterion(valid_outputs.view(-1), valid_labels.float())\n","        \n","        gpt_clf.to(device)\n","        \n","        # train loss를 토대로 모델을 학습합니다.\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","        \n","        tot_train_loss += train_loss.item()\n","        tot_valid_loss += valid_loss.item()\n","        \n","        train_correct_num += cal_correct_num(torch.sigmoid(train_outputs.view(-1)), train_labels.float())\n","        valid_correct_num += cal_correct_num(torch.sigmoid(valid_outputs.view(-1)), valid_labels.float())\n","               \n","        # 200 batch 마다 학습 상황을 화면에 출력합니다.\n","        if count % 200 == 0:\n","            cnt = ((count+1) * batch_size)\n","            current_train_loss = tot_train_loss / cnt\n","            current_valid_loss = tot_valid_loss / cnt\n","            \n","            train_acc = train_correct_num / cnt\n","            valid_acc = valid_correct_num / cnt\n","            \n","            print(f'epoch : %5d | batch : %5d | train_loss : %.5f | valid_loss : %.5f | train_acc : %.5f | valid_acc : %.5f' %(epoch+1, batch+1, current_train_loss, current_valid_loss, train_acc, valid_acc))\n","            \n","            tot_train_loss = 0.0\n","            tot_valid_loss = 0.0\n","            \n","            train_correct_num = 0\n","            valid_correct_num = 0\n","            \n","            count = 0\n","            \n","            # 이전 test_loss 보다 현재의 test_loss가 더 낮을 경우, 모델을 저장합니다.\n","            if prev_valid_loss > current_valid_loss:\n","                prev_valid_loss = current_valid_loss\n","                torch.save(gpt_clf.state_dict(), f'./KoGPT-Classifier-model.pth')\n","        \n","        count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWY67d9Xrkdh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aqpcaPSrkdh"},"source":[""],"execution_count":null,"outputs":[]}]}