{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from module.dataset import ModelNet40\n",
    "from module.utils import *\n",
    "\n",
    "import os, sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init 함수에서 feature_dim을 input으로 받아 Batch Normalization을 수행합니다.\n",
    "BatchNorm(3), BatchNorm(64), BatchNorm(1024)와 같이 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    '''\n",
    "        Perform batch normalization.\n",
    "        Input: A tensor of size (N, M, feature_dim), or (N, feature_dim, M) (available when feature_dim != M), \n",
    "                or (N, feature_dim)\n",
    "        Output: A tensor of the same size as input.\n",
    "    '''\n",
    "    # BatchNorm(3)\n",
    "    # BatchNorm(64)\n",
    "    # BatchNorm(1024)\n",
    "    def __init__(self, feature_dim):        \n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.batchnorm = nn.BatchNorm1d(feature_dim)\n",
    "        self.permute = Permute((0, 2, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (len(x.shape) == 3) and (x.shape[-1] == self.feature_dim):\n",
    "            return self.permute(self.batchnorm(self.permute(x)))\n",
    "        else:\n",
    "            return self.batchnorm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init 함수에서 받은 tuple형태의 param 파라미터를 통해 torch.permute_(param)을 실행합니다.\n",
    "예를 들어, x가 (100, 200, 300)의 shape를 갖고 있을 때 x.permute((0, 1, 2))는 x와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permute(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(Permute, self).__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(self.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers로 이루어진 MLP를 구성합니다.\n",
    "일반적으로 Fully connected layer, Batch normalization, Activation function이 set로 구성됩니다.\n",
    "\n",
    "init 함수에서 hidden_size 파라미터는 \n",
    "input dimension부터 hidden dimension들, output dimension까지의 tuple로 입력받습니다.\n",
    "\n",
    "input dimension부터 output dimension까지를 한번에 입력받는다는 점에 주의해주세요.\n",
    "\n",
    "batchnorm과 last_activation argument에 따라 옵션을 줄 수 있도록 작성하면 좋지만, 무시하셔도 괜찮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, batchnorm = True, last_activation = True):\n",
    "        super(MLP, self).__init__()\n",
    "        q = []\n",
    "        for i in range(len(hidden_size)-1):\n",
    "            in_dim = hidden_size[i]\n",
    "            out_dim = hidden_size[i+1]\n",
    "            q.append((\"Linear_%d\" %i, nn.Linear(in_dim,out_dim)))\n",
    "            if (i < len(hidden_size)-2) or ((i == len(hidden_size)-2) and (last_activation)):\n",
    "                if(batchnorm):\n",
    "                    q.append((\"Batchnorm_%d\" %i, nn.BatchNorm(out_dim)))\n",
    "                q.append((\"ReLU_%d\" %i, nn.ReLU(inplace=True)))  #inplace=True: memory를 저장할 필요가 없을 때\n",
    "        self.mlp = nn.Sequential(OrderedDict(q))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 max pooling이 아닌 global maxpooling입니다.\n",
    "input의 shape가 (B, N, D)일 때, output의 shape가 (B,)\n",
    "\n",
    "torch.max의 return값은 max value와 max값이 나온 위치, max value만 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()\n",
    "\n",
    "    def forward(self, x, dim=1, keepdim = False):\n",
    "        res, _ = torch.max(x, dim=dim, keepdim=keepdim)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input 또는 중간의 feature를 permutation과 rigid motion에 invariant하도록 만들기 위한 모듈입니다.\n",
    "- nfeat, 64, 128, 1024로 mapping되는 MLP\n",
    "- max pooling, batch normalization\n",
    "- 다시 1024, 512, 256, nfeat*nfeat로 mapping되는 MLP로 구성됩니다.\n",
    "\n",
    "최종적으로 (B, n_feat*n_feat)의 output을 (B, n_feat, n_feat)로 shape을 변경해 return해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNet(nn.Module):\n",
    "    def __init__(self, nfeat):\n",
    "        super(TNet, self).__init__()\n",
    "        self.nfeat = nfeat\n",
    "        self.tnet = nn.Sequential(MLP(nfeat, 64, 128, 1024), MaxPooling(), BatchNorm(1024), MLP(1024,512,256, nfeat*nfeat))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return self.tnet(x).view(batch_size, self.nfeat, self.nfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 모듈들을 모두 조합해 최종적으로 point cloud classification을 위한 pointNet을 구성해보겠습니다.\n",
    "TNet의 output은 transform matrix이므로 TNet의 input과 matrix multiplication을 해야합니다.\n",
    "\n",
    "- TNet : (B,N,3) > (B,N,3)\n",
    "- BatchNorm : (B,N,3) > (B,N,3)\n",
    "- MLP : (B,N,3) > (B,N,64) > (B,N,64)\n",
    "- TNet : (B,N,64) > (B,N,64)\n",
    "- BatchNorm : (B,N,64) > (B,N,64)\n",
    "- MLP : (B,N,64) > (B,N,64) > (B,N,128) > (B,N,1024)\n",
    "- Maxpooling : (B,N,1024) > (B,1024)\n",
    "- BatchNorm : (B,1024) > (B,1024)\n",
    "- MLP : (B,1024) > (B,512) > (B,256)\n",
    "- Dropout : (B,256) > (B,256)\n",
    "- FC-layer (Linear or Conv1d) for k classes : (B,256) > (B,nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, nfeat, nclass, dropout = 0):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.input_transform = TNet(nfeat)\n",
    "        self_mlp1= nn.Sequential(BatchNorm(3), MLP(nfeat,64,64))\n",
    "        self.feature_transform= TNet(64)\n",
    "        self.mlp2 = nn.Sequential(BatchNorm(64), MLP(64,128,1024))\n",
    "        self.maxpooling = MaxPooling()\n",
    "        self.mlp3 = nn.Sequential(BatchNorm(1024), MLP(51024,512,256), nn.Dropout(dropout), nn.Linear(256, nclass))\n",
    "        \n",
    "        self.eye64 = torch.eye(64).to(device)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        batch_size = xs.shape[0]\n",
    "        \n",
    "        transform = self.input_transform(xs)\n",
    "        xs = torch.stack([torch.mm(xs[i]) for i in range(batch_size)])\n",
    "        xs = self.mlp1(xs)\n",
    "\n",
    "        transform = self.input_transform(xs)\n",
    "        xs = torch.stack([torch.mm(xs[i]) for i in range(batch_size)])\n",
    "        xs = self.mlp2(xs)\n",
    "\n",
    "        xs = self.mlp3(self.maxpooling(xs))\n",
    "\n",
    "        if (self.training):\n",
    "            transform_transpose = transform.transpose(1, 2)\n",
    "            #torch.bmm 사용해도 됨\n",
    "            tmp = torch.stack([torch.mm(transform[i], transform_transpose[i]) for i in range(batch_size)])\n",
    "            L_reg = ((tmp - self.eye64) ** 2).sum() / batch_size\n",
    "            \n",
    "        return (F.log_softmax(xs, dim=1), L_reg) if self.training else F.log_softmax(xs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CV_PointNet\\PointCloud-Tutorial-1-master\\4_point_net_blank.ipynb 셀 14\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_PointNet/PointCloud-Tutorial-1-master/4_point_net_blank.ipynb#ch0000013?line=5'>6</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_PointNet/PointCloud-Tutorial-1-master/4_point_net_blank.ipynb#ch0000013?line=7'>8</a>\u001b[0m \u001b[39m################## loading data ##################\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_PointNet/PointCloud-Tutorial-1-master/4_point_net_blank.ipynb#ch0000013?line=9'>10</a>\u001b[0m train_data \u001b[39m=\u001b[39m ModelNet40(num_points)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_PointNet/PointCloud-Tutorial-1-master/4_point_net_blank.ipynb#ch0000013?line=10'>11</a>\u001b[0m test_data \u001b[39m=\u001b[39m ModelNet40(num_points, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_PointNet/PointCloud-Tutorial-1-master/4_point_net_blank.ipynb#ch0000013?line=12'>13</a>\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(train_data))\n",
      "File \u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CV_PointNet\\PointCloud-Tutorial-1-master\\module\\dataset.py:55\u001b[0m, in \u001b[0;36mModelNet40.__init__\u001b[1;34m(self, num_points, partition)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, num_points, partition\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m load_data(partition)\n\u001b[0;32m     56\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_points \u001b[39m=\u001b[39m num_points\n\u001b[0;32m     57\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartition \u001b[39m=\u001b[39m partition\n",
      "File \u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CV_PointNet\\PointCloud-Tutorial-1-master\\module\\dataset.py:34\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(partition)\u001b[0m\n\u001b[0;32m     31\u001b[0m     all_data\u001b[39m.\u001b[39mappend(data)\n\u001b[0;32m     32\u001b[0m     all_label\u001b[39m.\u001b[39mappend(label)\n\u001b[1;32m---> 34\u001b[0m all_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(all_data, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m all_label \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(all_label, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m all_data, all_label\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "################### Arguments ###################\n",
    "\n",
    "lr = 0.001\n",
    "num_points = 128\n",
    "save_name = \"PointNet.pt\"\n",
    "batch_size = 512\n",
    "\n",
    "################## loading data ##################\n",
    "\n",
    "train_data = ModelNet40(num_points)\n",
    "test_data = ModelNet40(num_points, 'test')\n",
    "\n",
    "train_size = int(0.9 * len(train_data))\n",
    "valid_size = len(train_data) - train_size\n",
    "train_data, valid_data = Data.random_split(train_data, [train_size, valid_size])\n",
    "valid_data.partition = 'valid'\n",
    "train_data.partition = 'train'\n",
    "\n",
    "print(\"train data size: \", len(train_data))\n",
    "print(\"valid data size: \", len(valid_data))\n",
    "print(\"test data size: \", len(test_data))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs = torch.stack([X for X, _ in batch])\n",
    "    Ys = torch.tensor([Y for _, Y in batch], dtype = torch.long)\n",
    "    return Xs, Ys\n",
    "\n",
    "train_iter  = Data.DataLoader(train_data, shuffle = True, batch_size = batch_size, collate_fn = collate_fn)\n",
    "valid_iter = Data.DataLoader(valid_data, batch_size = batch_size, collate_fn = collate_fn)\n",
    "test_iter = Data.DataLoader(test_data, batch_size = batch_size, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### loading model ####################\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = PointNet(nfeat=3, nclass=40, dropout=0.3)\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### training #########################\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, weight_decay = 0.0001)\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "def adjust_lr(optimizer, decay_rate=0.95):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= decay_rate\n",
    "\n",
    "retrain = True\n",
    "if os.path.exists(save_name):\n",
    "    print(\"Model parameters have already been trained before. Retrain ? (y/n)\")\n",
    "    ans = input()\n",
    "    if (ans == 'y'):\n",
    "        checkpoint = torch.load(save_name, map_location = device)\n",
    "        net.load_state_dict(checkpoint[\"net\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "train_model(train_iter, valid_iter, net, loss, optimizer, device = device, max_epochs = int(1000/(batch_size/64)), \n",
    "            adjust_lr = adjust_lr, early_stop = EarlyStop(patience = 20, save_name = save_name))\n",
    "    \n",
    "\n",
    "############### testing ##########################\n",
    "\n",
    "loss, acc = evaluate_model(test_iter, net, loss)\n",
    "print('test acc = %.6f' % (acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
