{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from module.dataset import ModelNet40\n",
    "from module.utils import *\n",
    "\n",
    "import os, sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    '''\n",
    "        Perform batch normalization.\n",
    "        Input: A tensor of size (N, M, feature_dim), or (N, feature_dim, M) \n",
    "                                               ( it would be the former case if feature_dim == M ), \n",
    "                or (N, feature_dim)\n",
    "        Output: A tensor of the same size as input.\n",
    "    '''\n",
    "    def __init__(self, feature_dim):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.batchnorm = nn.BatchNorm1d(feature_dim)\n",
    "        self.permute = Permute((0, 2, 1))\n",
    "        \n",
    "    def forward(self, x, _ = None):\n",
    "        if (len(x.shape) == 3) and (x.shape[-1] == self.feature_dim):\n",
    "            return self.permute(self.batchnorm(self.permute(x)))\n",
    "        else:\n",
    "            return self.batchnorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permute(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(Permute, self).__init__()\n",
    "        self.param = param\n",
    "    def forward(self, x):\n",
    "        return x.permute(self.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, batchnorm = True, last_activation = True):\n",
    "        super(MLP, self).__init__()\n",
    "        q = []\n",
    "        for i in range(len(hidden_size)-1):\n",
    "            in_dim = hidden_size[i]\n",
    "            out_dim = hidden_size[i+1]\n",
    "            q.append((\"Linear_%d\" % i, nn.Linear(in_dim, out_dim)))\n",
    "            if (i < len(hidden_size) - 2) or ((i == len(hidden_size) - 2) and (last_activation)):\n",
    "                if (batchnorm):\n",
    "                    q.append((\"Batchnorm_%d\" % i, BatchNorm(out_dim)))\n",
    "                q.append((\"ReLU_%d\" % i, nn.ReLU(inplace=True)))\n",
    "        self.mlp = nn.Sequential(OrderedDict(q))\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()\n",
    "    def forward(self, x, dim=1, keepdim = False):\n",
    "        res, _ = torch.max(x, dim=dim, keepdim = keepdim)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNet(nn.Module):\n",
    "    def __init__(self, nfeat, dropout = 0):\n",
    "        super(TNet, self).__init__()\n",
    "        self.nfeat = nfeat\n",
    "        self.encoder = MLP((nfeat, 64, 128, 128))\n",
    "        self.gcn = GCN(128, 128, 256, 512)\n",
    "        self.decoder = nn.Sequential(MaxPooling(), BatchNorm(1024), \n",
    "                                     MLP((1024, 512, 256)), nn.Dropout(dropout), MLP((256, nfeat*nfeat)))\n",
    "        \n",
    "    def forward(self, x, adjs):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.decoder(self.gcn(self.encoder(x), adjs))\n",
    "        return x.view(batch_size, self.nfeat, self.nfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def __repr__() 은 custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicGraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "\n",
    "    Build graph structure from a 3D point cloud using k-nearest neighbor(KNN)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, k, bias=True):\n",
    "        super(DynamicGraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.k = k\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj=None):\n",
    "        if adj is None:\n",
    "            adj = self.build_knn(input, self.k)\n",
    "\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.bmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'       \n",
    "\n",
    "    def build_knn(self, input, k):\n",
    "        batch_size, num_points, _ = input.size()\n",
    "\n",
    "        inner = torch.matmul(input, input.transpose(1,2)) # (B,N,D) @ (B,D,N) > (B,N,N)\n",
    "        xx = torch.sum(input**2, dim=2, keepdim=True) # (B,N,D) > (B,N,1)\n",
    "        pairwise_distance = -1 * (xx + xx.transpose(2, 1) - 2*inner) # (B,N,1) + (B,1,N) -2*(B,N,N) > (B,N,N)\n",
    "    \n",
    "        _, topk_indices = pairwise_distance.topk(k=k, dim=-1) # (B,N,K)\n",
    "        base_indices = torch.arange(num_points)[None, :, None].repeat(batch_size, 1, k).to(topk_indices.device) # (B,N,K)\n",
    "        indices = torch.stack([base_indices.view(batch_size, -1), topk_indices.view(batch_size, -1)], dim=1) # (B, 2, N*K)\n",
    "        \n",
    "        adj = torch.stack([torch.sparse.FloatTensor(indices[b], (1/k) * torch.ones_like(indices[b][0], dtype=torch.float), torch.Size([num_points, num_points])) for b in range(batch_size)])\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, n_in, n_hid1, n_hid2, n_out):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(n_in, n_hid1)\n",
    "        self.batchnorm1 = BatchNorm(n_hid1)\n",
    "        self.gc2 = GraphConvolution(n_in+n_hid1, n_hid2)\n",
    "        self.batchnorm2 = BatchNorm(n_hid2)\n",
    "        self.gc3 = GraphConvolution(n_in+n_hid1+n_hid2, n_out)\n",
    "        self.batchnorm3 = BatchNorm(n_out)\n",
    "    \n",
    "    def forward(self, xs, adjs = None):\n",
    "        if (adjs is None):\n",
    "            xs, adjs = xs\n",
    "        \n",
    "        num_points = xs.shape[1]\n",
    "        \n",
    "        xs = torch.cat(tuple(xs), dim=0)\n",
    "        xs = xs.to(device)\n",
    "        adjs = adjs.to(device)\n",
    "        \n",
    "        xs1 = torch.cat( (xs, F.relu(self.batchnorm1(self.gc1(xs, adjs)))), dim = 1)\n",
    "        del xs\n",
    "        xs2 = torch.cat( (xs1, F.relu(self.batchnorm2(self.gc2(xs1, adjs)))), dim = 1)\n",
    "        del xs1\n",
    "        xs3 = torch.cat( (xs2, F.relu(self.batchnorm3(self.gc3(xs2, adjs)))), dim = 1)\n",
    "        del xs2\n",
    "        \n",
    "        res = xs3\n",
    "        ys = torch.stack(torch.split(res, num_points, dim=0)).to(device)\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 PointNet에서 GlobalMaxPooling 전에 (N,64)에서 (N,1024)로 mapping하기 위해 MLP(64, 128, 1024)를 사용했던 것과 비교해서\n",
    "\n",
    "PointNet+GCN은 (N,64)에서 (N,128)로 mapping하는 MLP(64, 64, 128, 128)을 수행한 후에,\n",
    "\n",
    "GCN(128, 128, 256, 512)를 수행합니다.\n",
    "\n",
    "위에서 정의한 Graph Convolution layer는 마지막 레이어에서 input features(N, 512)과 output features(N, 512)를\n",
    "\n",
    "concatentation하기 때문에 최종적으로 (N, 512+512)=(N, 1024)를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetGCN(nn.Module):\n",
    "    def __init__(self, nfeat, nclass, dropout = 0):\n",
    "        super(PointNetGCN, self).__init__()\n",
    "\n",
    "        self.input_transform = TNet(nfeat, 0.1)\n",
    "        self.encoder = nn.Sequential(BatchNorm(3), MLP((nfeat, 64, 64)))\n",
    "        self.feature_transform = TNet(64, 0.1)\n",
    "        self.batchnorm = BatchNorm(64)\n",
    "        self.mlp = MLP((64, 64, 128, 128))\n",
    "        self.gcn = GCN(128, 128, 256, 512)\n",
    "        self.maxpooling = MaxPooling()\n",
    "        self.decoder = nn.Sequential(BatchNorm(1024), MLP((1024, 512, 256)), nn.Dropout(dropout), nn.Linear(256, nclass))\n",
    "\n",
    "        self.eye64 = torch.eye(64).to(device)\n",
    "        \n",
    "    def forward(self, xs, adjs):\n",
    "        batch_size = xs.shape[0]\n",
    "        \n",
    "        transform = self.input_transform(xs, adjs)\n",
    "        xs = torch.stack([torch.mm(xs[i],transform[i]) for i in range(batch_size)])\n",
    "        xs = self.encoder(xs)\n",
    "        \n",
    "        transform = self.feature_transform(xs, adjs)\n",
    "        xs = torch.stack([torch.mm(xs[i],transform[i]) for i in range(batch_size)])\n",
    "        \n",
    "        xs = self.gcn(self.mlp(self.batchnorm(xs)), adjs)\n",
    "        xs = self.decoder(self.maxpooling(xs))\n",
    "        \n",
    "        if (self.training):\n",
    "            transform_transpose = transform.transpose(1, 2)\n",
    "            tmp = torch.stack([torch.mm(transform[i], transform_transpose[i]) for i in range(batch_size)])\n",
    "            L_reg = ((tmp - self.eye64) ** 2).sum() / batch_size\n",
    "            \n",
    "        return (F.log_softmax(xs, dim=1), L_reg) if self.training else F.log_softmax(xs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 0.001\n",
    "num_points = 128\n",
    "save_name = \"PointNet.pt\"\n",
    "\n",
    "########### loading data ###########\n",
    "train_data = ModelNet40(num_points)\n",
    "test_data = ModelNet40(num_points, 'test')\n",
    "\n",
    "train_size = int(0.9 * len(train_data))\n",
    "valid_size = len(train_data) - train_size\n",
    "train_data, valid_data = Data.random_split(train_data, [train_size, valid_size])\n",
    "valid_data.partition = 'valid'\n",
    "train_data.partition = 'train'\n",
    "\n",
    "print(\"train data size: \", len(train_data))\n",
    "print(\"valid data size: \", len(valid_data))\n",
    "print(\"test data size: \", len(test_data))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs = torch.stack([X for X, _, _ in batch])\n",
    "    #adjs = [adj for _, adj, _ in batch]\n",
    "    \n",
    "    global num_points\n",
    "    batch_size = len(batch)\n",
    "    edges = torch.cat( tuple(batch[i][1][0] + i*num_points for i in range(batch_size)), dim=0)\n",
    "    values = torch.cat( tuple(batch[i][1][1] for i in range(batch_size)), dim=0)\n",
    "    N = num_points * batch_size\n",
    "    adjs = torch.sparse.FloatTensor(edges.t(), values, torch.Size([N,N]))\n",
    "    \n",
    "    Ys = torch.tensor([Y for _,_, Y in batch], dtype = torch.long)\n",
    "    return Xs, adjs, Ys\n",
    "\n",
    "train_iter  = Data.DataLoader(train_data, shuffle = True, batch_size = 32, collate_fn = collate_fn)\n",
    "valid_iter = Data.DataLoader(valid_data, batch_size = 32, collate_fn = collate_fn)\n",
    "test_iter = Data.DataLoader(test_data, batch_size = 32, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### loading model ####################\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = PointNetGCN(nfeat=3, nclass=40, dropout=0.3)\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### training #########################\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, weight_decay = 0.0001)\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "def adjust_lr(optimizer, decay_rate=0.95):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= decay_rate\n",
    "\n",
    "retrain = True\n",
    "if os.path.exists(save_name):\n",
    "    print(\"Model parameters have already been trained before. Retrain (y) or tune (n) ?\")\n",
    "    ans = input()\n",
    "    if not (ans == 'y'):\n",
    "        checkpoint = torch.load(save_name, map_location = device)\n",
    "        net.load_state_dict(checkpoint[\"net\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "        \n",
    "train_model(train_iter, valid_iter, net, loss, optimizer, device = device, max_epochs = 1000, adjust_lr = adjust_lr,\n",
    "            early_stop = EarlyStop(patience = 20, save_name = save_name))\n",
    "    \n",
    "\n",
    "############### testing ##########################\n",
    "\n",
    "loss, acc = evaluate_model(test_iter, net, loss)\n",
    "print('test acc = %.6f' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
