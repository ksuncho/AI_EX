{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "BYOL Practice Answer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC3TkQ3gJbBP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import copy\n",
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvdDhbPFJbBR"
      },
      "source": [
        "### Step 1. Design BYOL Model & Loss\n",
        "\n",
        "#### Implementation 1-1. Create modules\n",
        "\n",
        "#### Implementation 1-2. Design BYOL loss function\n",
        "\n",
        "#### Implementation 1-3. Design forward function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ftel14QJbBR"
      },
      "source": [
        "import math\n",
        "from torchvision.models.resnet import conv3x3\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, norm_layer, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        \n",
        "        self.bn1 = norm_layer(inplanes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        \n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x \n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.relu1(residual)\n",
        "        residual = self.conv1(residual)\n",
        "\n",
        "        residual = self.bn2(residual)\n",
        "        residual = self.relu2(residual)\n",
        "        residual = self.conv2(residual)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x + residual\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.avg = nn.AvgPool2d(stride)\n",
        "        assert nOut % nIn == 0\n",
        "        self.expand_ratio = nOut // nIn\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n",
        "\n",
        "\n",
        "class ResNetCifar(nn.Module):\n",
        "    def __init__(self, depth=26, width=1, channels=3, norm_layer=nn.BatchNorm2d):\n",
        "        assert (depth - 2) % 6 == 0         # depth is 6N+2\n",
        "        self.N = (depth - 2) // 6\n",
        "        super(ResNetCifar, self).__init__()\n",
        "\n",
        "        # Following the Wide ResNet convention, we fix the very first convolution\n",
        "        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.inplanes = 16\n",
        "        self.layer1 = self._make_layer(norm_layer, 16 * width)\n",
        "        self.layer2 = self._make_layer(norm_layer, 32 * width, stride=2)\n",
        "        self.layer3 = self._make_layer(norm_layer, 64 * width, stride=2)\n",
        "        self.bn = norm_layer(64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                \n",
        "    def _make_layer(self, norm_layer, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = Downsample(self.inplanes, planes, stride)\n",
        "        layers = [BasicBlock(self.inplanes, planes, norm_layer, stride, downsample)]\n",
        "        self.inplanes = planes\n",
        "        for i in range(self.N - 1):\n",
        "            layers.append(BasicBlock(self.inplanes, planes, norm_layer))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, idim, hdim, odim, width=1.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(idim * width, hdim * width)\n",
        "        self.bn1 = nn.BatchNorm1d(hdim * width)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hdim * width, odim * width)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BYOLNet(nn.Module):\n",
        "    def __init__(self, \n",
        "                 width=1, \n",
        "                 feat_dim=64, \n",
        "                 hidden_dim=128,\n",
        "                 byol_feat_dim=32,\n",
        "                 num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        # f: convolutional feature encoder, use ResNetCifar class above\n",
        "        # g: projection MLP, use MLP class above\n",
        "        # q: prediction MLP, use MLP class above\n",
        "        # h: linear classifier, use nn.Linear class\n",
        "        self.f = ResNetCifar(width=width)\n",
        "        self.g = MLP(idim=feat_dim, hdim=hidden_dim, odim=byol_feat_dim, width=width)\n",
        "        self.q = MLP(idim=byol_feat_dim, hdim=hidden_dim, odim=byol_feat_dim, width=width)\n",
        "        self.h = nn.Linear(feat_dim, num_classes)\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "\n",
        "        self.online_net = nn.Sequential(self.f, self.g, self.q)\n",
        "\n",
        "        self.f_target = copy.deepcopy(self.f)\n",
        "        self.g_target = copy.deepcopy(self.g)\n",
        "\n",
        "        self.target_net = nn.Sequential(self.f_target, self.g_target)\n",
        "\n",
        "        for p in self.target_net.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def byol_loss(self, pred, proj):\n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        # pred: prediction vectors from online net\n",
        "        # proj: projection vectors from target net (must not have grad)\n",
        "        # return the loss values of shape (batch, 1)\n",
        "        # Note that the last dimension should contain the value of dot product\n",
        "        pred = F.normalize(pred, dim=-1)\n",
        "        proj = F.normalize(proj, dim=-1)\n",
        "        return -2. * (pred * proj).sum(dim=-1)\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "\n",
        "    def byol_forward(self, x1, x2):\n",
        "        ### IMPLEMENTATION 1-3 ###\n",
        "        # x1: tensor containing input image (view #1)\n",
        "        # x2: tensor containing input image (view #2)\n",
        "        # Compute 2 loss values using self.byol_loss method twice\n",
        "        pred1 = self.online_net(x1)\n",
        "        pred2 = self.online_net(x2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            proj1 = self.target_net(x2)\n",
        "            proj2 = self.target_net(x1)\n",
        "\n",
        "        loss1 = self.byol_loss(pred1, proj1)\n",
        "        loss2 = self.byol_loss(pred2, proj2)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        loss = (loss1 + loss2).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def update_target_net(self, decay):\n",
        "        for p_online, p_target in zip(self.online_net.parameters(), self.target_net.parameters()):\n",
        "            p_target.data = p_target.data * decay + p_online.data * (1 - decay)\n",
        "\n",
        "    def finetune_forward(self, x):\n",
        "        return self.h(self.f(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ZCWWJ5JbBT"
      },
      "source": [
        "### Step 2. Prepare datasets & data augmentations\n",
        "\n",
        "For contrastaive learning, a set of random augmentation functions is defined.\n",
        "\n",
        "Then, the set is applied twice to each image, which is implemented as in provided DoubleCompose module.\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "Refer to the torchvision.transforms documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5kHhEH-JbBU"
      },
      "source": [
        "class DoubleCompose(object):\n",
        "    def __init__(self, trf1, trf2):\n",
        "        self.trf1 = trf1\n",
        "        self.trf2 = trf2\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img1 = img.copy()\n",
        "        img2 = img.copy()\n",
        "        for t1 in self.trf1:\n",
        "            img1 = t1(img1)\n",
        "        for t2 in self.trf2:\n",
        "            img2 = t2(img2)\n",
        "        return img1, img2\n",
        "\n",
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "from PIL import Image\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0, p=1.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        if self.kernel_size % 2 == 0:\n",
        "            self.kernel_size += 1\n",
        "\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample,):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < self.p:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return Image.fromarray(sample.astype(np.uint8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUWa_UJnJbBU"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "img_size = (32, 32)\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)\n",
        "\n",
        "transform1 = [\n",
        "    transforms.RandomResizedCrop(size=img_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([color_jitter], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(3, p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "]\n",
        "\n",
        "transform2 = [\n",
        "    transforms.RandomResizedCrop(size=img_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([color_jitter], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(3, p=0.1),\n",
        "    transforms.RandomSolarize(5, p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "]\n",
        "\n",
        "train_transform = DoubleCompose(transform1, transform2)\n",
        "\n",
        "finetune_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5txTPQjntcL"
      },
      "source": [
        "Note that we should make three dataloaders:\n",
        "\n",
        "1. For pre-training with two heterogeneous data augmentations\n",
        "2. For fine-tuning with one basic or no data augmentation\n",
        "3. For testing with no data augmentation\n",
        "\n",
        "And also note that we should always contain **transforms.ToTensor()** to make sure that the input values are normalized into the range [0, 1]. Dataset-specific normalization (whitening) is recommended, but not mandatory (it is good to testify its effectiveness as an ablation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTIHLlI1JbBV",
        "outputId": "2f8e01e7-ce33-4887-96be-6451a3f48ba7"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=train_transform\n",
        "                                )\n",
        "\n",
        "finetune_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=finetune_transform\n",
        "                                )\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=False,\n",
        "                                 download=True,\n",
        "                                 transform=test_transform\n",
        "                                )\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )\n",
        "\n",
        "finetune_loader = DataLoader(finetune_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )\n",
        "\n",
        "test_loader = DataLoader(finetune_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=False,\n",
        "                          drop_last=False\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU5_4d7BJbBW"
      },
      "source": [
        "### Step 3. Run pre-training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCaPUWVLprMr"
      },
      "source": [
        "import math\n",
        "def get_decay_value(base_decay, global_iter, total_iter):\n",
        "    return 1 - (1 - base_decay) * (math.cos(math.pi * global_iter / total_iter) + 1)/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GATvSb8JbBW"
      },
      "source": [
        "def train(net, loader):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(net.online_net.parameters(), 3e-4)\n",
        "    \n",
        "    train_start = time.time()\n",
        "\n",
        "    global_iter = 0\n",
        "    total_iter = 100 * len(loader)\n",
        "    \n",
        "    for epoch in range(1, 100 + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        \n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            xi, xj = data[0].cuda(), data[1].cuda()\n",
        "        \n",
        "            loss = net.byol_forward(xi, xj)\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            net.update_target_net(get_decay_value(0.996, global_iter, total_iter))\n",
        "            global_iter += 1\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeVkjz5MJbBW"
      },
      "source": [
        "GPU_NUM = '0'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_NUM\n",
        "\n",
        "net = BYOLNet()\n",
        "\n",
        "net = net.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4geMfDbuJbBW",
        "outputId": "76bf335f-1259-40b3-9a8b-6d6062dd09b8"
      },
      "source": [
        "train(net, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch\t 1 \tLoss\t -2.5252958173171067 \tTime\t 77.15889763832092\n",
            "Epoch\t 2 \tLoss\t -2.8267525501740285 \tTime\t 77.42291712760925\n",
            "Epoch\t 3 \tLoss\t -2.893999861448239 \tTime\t 75.89947175979614\n",
            "Epoch\t 4 \tLoss\t -2.915124855286036 \tTime\t 75.66414618492126\n",
            "Epoch\t 5 \tLoss\t -2.9624405775314724 \tTime\t 75.71107459068298\n",
            "Epoch\t 6 \tLoss\t -3.011826112942818 \tTime\t 75.26566457748413\n",
            "Epoch\t 7 \tLoss\t -3.0483511166694837 \tTime\t 75.5340051651001\n",
            "Epoch\t 8 \tLoss\t -3.086755563051273 \tTime\t 77.46459317207336\n",
            "Epoch\t 9 \tLoss\t -3.1007195240412004 \tTime\t 75.89870977401733\n",
            "Epoch\t 10 \tLoss\t -3.1071740810687727 \tTime\t 75.6028892993927\n",
            "Epoch\t 11 \tLoss\t -3.126673698425293 \tTime\t 75.96816873550415\n",
            "Epoch\t 12 \tLoss\t -3.1513938781542654 \tTime\t 74.55140852928162\n",
            "Epoch\t 13 \tLoss\t -3.17892715380742 \tTime\t 74.37857723236084\n",
            "Epoch\t 14 \tLoss\t -3.2068138098105408 \tTime\t 74.99794864654541\n",
            "Epoch\t 15 \tLoss\t -3.217320384734716 \tTime\t 74.65372800827026\n",
            "Epoch\t 16 \tLoss\t -3.2302121871556992 \tTime\t 74.0640139579773\n",
            "Epoch\t 17 \tLoss\t -3.24819694788028 \tTime\t 74.08501386642456\n",
            "Epoch\t 18 \tLoss\t -3.262733587851891 \tTime\t 74.17049217224121\n",
            "Epoch\t 19 \tLoss\t -3.276531180357322 \tTime\t 73.97228622436523\n",
            "Epoch\t 20 \tLoss\t -3.271485790839562 \tTime\t 75.20289945602417\n",
            "Epoch\t 21 \tLoss\t -3.268268826068976 \tTime\t 75.94617080688477\n",
            "Epoch\t 22 \tLoss\t -3.275246463677822 \tTime\t 75.95245790481567\n",
            "Epoch\t 23 \tLoss\t -3.265764670494275 \tTime\t 78.17485666275024\n",
            "Epoch\t 24 \tLoss\t -3.262997374167809 \tTime\t 75.24381852149963\n",
            "Epoch\t 25 \tLoss\t -3.2546704133351643 \tTime\t 74.46444845199585\n",
            "Epoch\t 26 \tLoss\t -3.2533656523777887 \tTime\t 74.18366432189941\n",
            "Epoch\t 27 \tLoss\t -3.2585581192603477 \tTime\t 72.91074347496033\n",
            "Epoch\t 28 \tLoss\t -3.2663131420428937 \tTime\t 73.50030422210693\n",
            "Epoch\t 29 \tLoss\t -3.265275230163183 \tTime\t 73.53347754478455\n",
            "Epoch\t 30 \tLoss\t -3.262868801752726 \tTime\t 74.04403066635132\n",
            "Epoch\t 31 \tLoss\t -3.2694192629594068 \tTime\t 73.74061107635498\n",
            "Epoch\t 32 \tLoss\t -3.269827559055426 \tTime\t 72.90782356262207\n",
            "Epoch\t 33 \tLoss\t -3.278054563815777 \tTime\t 73.77964615821838\n",
            "Epoch\t 34 \tLoss\t -3.272608634753105 \tTime\t 73.75942778587341\n",
            "Epoch\t 35 \tLoss\t -3.2704673644823905 \tTime\t 74.44919776916504\n",
            "Epoch\t 36 \tLoss\t -3.271216615041097 \tTime\t 75.05397987365723\n",
            "Epoch\t 37 \tLoss\t -3.2641303331424028 \tTime\t 77.30792546272278\n",
            "Epoch\t 38 \tLoss\t -3.2556820086943796 \tTime\t 74.48476982116699\n",
            "Epoch\t 39 \tLoss\t -3.251510007564838 \tTime\t 76.22802257537842\n",
            "Epoch\t 40 \tLoss\t -3.2551084726284714 \tTime\t 76.51412606239319\n",
            "Epoch\t 41 \tLoss\t -3.2626541822384567 \tTime\t 73.20145988464355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y5bvN78pYXo"
      },
      "source": [
        "### Step 4. Run fine-tuning step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GGJQMMfJbBX"
      },
      "source": [
        "def finetune(net, loader, test_loader):\n",
        "    \n",
        "    ### IMPORTANT ###\n",
        "    # When fine-tuning your network, all parameters except the linear classifier must be frozen\n",
        "    # f, g, and q in the BYOLNet instance will not be updated\n",
        "    for p in net.online_net.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # To make sure, pass only the classifier parameters to optimizer\n",
        "    params = list(net.h.parameters())\n",
        "    optimizer = torch.optim.Adam(params, 3e-4)\n",
        "    \n",
        "    train_start = time.time()\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    global_iter = 0\n",
        "    total_iter = 100 * len(loader)\n",
        "    \n",
        "    for epoch in range(1, 100 + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        net.f.eval()\n",
        "        net.g.eval()\n",
        "        net.q.eval()\n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            x, target = data.cuda(), target.cuda()\n",
        "        \n",
        "            y = net.finetune_forward(x)\n",
        "            # Or use this\n",
        "            # y = net.finetune_forward_no_grad(x)\n",
        "\n",
        "            loss = loss_fn(y, target)\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_iter += 1\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "              \"\\tAcc.\", test(net, test_loader),\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ4aJAKptUpI"
      },
      "source": [
        "def test(net, test_loader):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            y = net.finetune_forward(data)\n",
        "\n",
        "            correct += (y.argmax(1) == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    \n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiFJLykCt6lx"
      },
      "source": [
        "finetune(net, finetune_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDDXtJvuNwcF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}