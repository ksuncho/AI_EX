{"cells":[{"cell_type":"markdown","metadata":{"id":"WjDdziEN_VCt"},"source":["# BERT를 활용한 Dense Passage Retrieval 실습"]},{"cell_type":"markdown","metadata":{"id":"1NWluWk3_VCu"},"source":["### Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":19761,"status":"ok","timestamp":1658734168084,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"eGqFS4EEBF_Z"},"outputs":[{"name":"stderr","output_type":"stream","text":["������ ��θ� ã�� �� �����ϴ�.\n","������ ��θ� ã�� �� �����ϴ�.\n"]}],"source":["# > /dev/null 2>&1 # execute command in silence\n","!pip install datasets==1.4.1 > /dev/null 2>&1 # execute command in silence\n","!pip install transformers==4.20.1 > /dev/null 2>&1"]},{"cell_type":"markdown","metadata":{"id":"CYUkp06Y_VCv"},"source":["## 데이터셋 로딩\n"]},{"cell_type":"markdown","metadata":{"id":"KMrZa4uql_nx"},"source":["KorQuAD train 데이터셋을 학습 데이터로 활용"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272,"referenced_widgets":["cfdf21900e3b47ed9a241a275596adf1","8635a8297d7d451ba258f9400c812a95","e3bf0ef13c3f45f2953f1ed0ee871275","8fcbb4b4be2c4d2484b73b0a59f791e6","719f8fa7c823455ba980fe44d5b197f1","0e09a7896b464f6ea9f178809dfb602a","eff79f1e5189420b8542e497535acdf5","9452949bced246a3b5836672bb64b0b1","f23292867a1242e98d7c404308a8bf29","4433e92fa1cd45f9b9ca8059815ddb46","e7389412279248f5a288e6c3ee9a00fa","dd17cceb388142139065c455e070b170","46bd393c1cdb4856a4c8cac744c33e03","f87214c9c27d4253bd1190f230764148","550a559e977b4dc58ab9c961b7b0fff9","9179d2ba6ac94120a35a2afdf9dacdf4","fa17d14263d34a8cbc36a8f5d6f3cf6e","8f831b68e6ca482db36f7ccacbccde30","0c14b3dab4374f50b45c3ff336ad874d","0b4987d3f2c943e48cdb538649d16eb4","932a52e72ee540f7a365702713b9533d","4906498dc2104672aeb25a5bfa58e8bd","e7e2670aaab747c8a93df57925d3a294","f4fa398a13854ec6b85e787e69e9da5a","5d3f6edae58649c4aca490fe617486f5","f35f8171392c4367a9bb4f6d07ddd4c8","c35b2dc92ab34645b88bfed5517f5187","8aedc23beece413ca2d2553a1937925d","28914da5acd440aea31de315ec453c2b","175a62ae56354ee881db9221d95b7165","b9d8632873ff4deea29f7fbfb52d484c","ea073b6c203549a8b29dc885cc0436bc","54d50f65f13c41e5abb5ca35502a04ea","7f44b72f57f14ea7aaec4e5682a04ac4","efadd2807af14d56a43a5dcd37e38f4b","06539b3346084f6badb98cb30acdaf76","3a9bda724b434b688d56a33ca225821f","af82a5abec7e49ae9fa0b640b26684d9","a9ad2a0792c4480da5e0f00826fc334b","25d58375a88f47c3939665e846184cda","d3730832e76e4a57b223f00f99a92786","476193bb5f4a4e0da534c3cb2d2da4fd","9930891b9d934935b14ece78fb240b76","a145c9254b5b45eea3c29200b11b533e","f338007664d4406d80b96cda58220447","668e663d6c9b4c469eb540e1b573f8b2","8d8f5e009fb141e78187e44d35f48e44","83f6e7e524b146329befb45ddcb47598"]},"executionInfo":{"elapsed":18527,"status":"ok","timestamp":1658734186604,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"4IUxepuj_VCv","outputId":"573b4e17-7907-4722-fece-e20f44ea1752"},"outputs":[{"name":"stderr","output_type":"stream","text":["Reusing dataset squad_kor_v1 (C:\\Users\\AI_15\\.cache\\huggingface\\datasets\\squad_kor_v1\\squad_kor_v1\\1.0.0\\18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a691d1e36ded4e07a093f584a1d1ef80","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"squad_kor_v1\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4239,"status":"ok","timestamp":1658734190838,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"4pwBKTm2egdq","outputId":"ac6adc68-8c74-4a05-8a9e-067eaf197bb0"},"outputs":[{"data":{"text/plain":["9606"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["corpus = list(set([example['context'] for example in dataset['train']]))\n","len(corpus)"]},{"cell_type":"markdown","metadata":{"id":"lJtECqpB_VCx"},"source":["## 토크나이저 준비 - Huggingface 제공 tokenizer 이용"]},{"cell_type":"markdown","metadata":{"id":"X0Fu2WaqpUB8"},"source":["BERT를 encoder로 사용하므로, hugginface에서 제공하는 \"bert-base-multilingual-cased\" tokenizer를 활용"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166,"referenced_widgets":["bf6d289894854d81aa695251c985bf3a","61cae5a7e9a843b282238b978baaef9e","221fa80b0c544e4a903a5dd8f1b086f1","b46f78a1f98a49e6938ee954df3c839d","dcdb84db38344248821b6a2f542940aa","d92d6fabf8174ec7b40d734921718325","4629172ce3a94189a5beb1f90382d2ac","9a2ef5ce42f64adcb2cebbc744730da8","028c8597a911485a8ba0da41ef5807ca","22e308be2cd84ac1a8fe75396d989e68","cf77c57b5a724ac3b329e6157e95147d","db1215991a8f433dac8d5262d72bd982","9aa72c7ad6de403b9d1af9d66bc6f9ac","924e1b1ec5e441c297d17f02caa12a66","9a2309e171814aa99a6fd9ff2b6888a2","19d6fee0d4bb41caa5952462527b0cf3","acaa992ad16b4ec1bda7bdbbb910ad77","e3917508c2134e99bfe4376faab26a23","5f7926cbead54f6999c56b188c9b3038","1678f427ba3747c3ac6204c683287ee6","65f88172c01f4e7884edfca2688d9435","5c98997be7484f779f6708e16297b00e","f7ed8d296c344ef98b1ef51f770c13e9","07d527d2ba38401180172ebde131e379"]},"executionInfo":{"elapsed":12481,"status":"ok","timestamp":1658734203314,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"AoB8BHGDmVIK","outputId":"06ae7443-b7e5-4d6e-f1a0-cbc97f00bd46"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdaedb76b77948f5a3b1d808b04d92f9","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"629add490d7c4114957e8df4cac89ad5","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/725 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f35ad19cc0774325bb06eaa6dba6854f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/336k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","import numpy as np\n","\n","# model_checkpoint = \"bert-base-multilingual-cased\" # out of memory issue\n","model_checkpoint = \"kykim/bert-kor-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1658734203314,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"WPxRvMjdvh4y","outputId":"d9c54316-3bed-4ce5-8da2-91af9bd3d82e"},"outputs":[{"data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='kykim/bert-kor-base', vocab_size=42000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1658734203315,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"0U7sn3jsu44O","outputId":"3099bab7-11dd-49d1-cff9-4738ef51d516"},"outputs":[{"data":{"text/plain":["'[CLS] 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡 ( 1악장 ) 을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_input = tokenizer(dataset['train'][0]['context'], padding=\"max_length\", truncation=True)\n","tokenizer.decode(tokenized_input['input_ids'])"]},{"cell_type":"markdown","metadata":{"id":"zpoTleVJjp5x"},"source":["## Dense encoder (BERT) 학습 시키기"]},{"cell_type":"markdown","metadata":{"id":"_nrxmtmfkRVb"},"source":["HuggingFace BERT를 활용하여 question encoder, passage encoder 학습"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658734203315,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"6b215ZfJ_EOc"},"outputs":[],"source":["from tqdm import tqdm, trange\n","import argparse\n","import random\n","import torch\n","import torch.nn.functional as F\n","from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n","\n","torch.manual_seed(2021)\n","torch.cuda.manual_seed(2021)\n","np.random.seed(2021)\n","random.seed(2021)"]},{"cell_type":"markdown","metadata":{"id":"N-bKwkxTpoje"},"source":["1) Training Dataset 준비하기 (question, passage pairs)\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658734203315,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"E_FQ1kcazxge"},"outputs":[],"source":["# Use subset (10 example) of original training dataset \n","sample_idx = np.random.choice(range(len(dataset['train'])), 20)\n","training_dataset = dataset['train'][sample_idx]"]},{"cell_type":"markdown","metadata":{"id":"ALNnnBJTxeU4"},"source":["Negative sampling을 위한 negative sample들을 샘플링"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":944,"status":"ok","timestamp":1658734204253,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"aZFjZifDxd1Z"},"outputs":[],"source":["# set number of neagative sample\n","num_neg = 3\n","\n","corpus = np.array(corpus)\n","p_with_neg = []\n","\n","for c in training_dataset['context']:\n","  while True:\n","    neg_idxs = np.random.randint(len(corpus), size=num_neg)\n","\n","    if not c in corpus[neg_idxs]:\n","      p_neg = corpus[neg_idxs]\n","\n","      p_with_neg.append(c)\n","      p_with_neg.extend(p_neg)\n","      break\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1658734204254,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"wbGW7PRJ7Yv5","outputId":"601cf5c6-139e-4291-aa96-0706bf89374d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Positive context]\n","무한도전은 회를 거듭할수록 소재가 진부해진다는 평을 듣기도 한다. 대표적인 예가 지난 2008년 1월 5일 방송된 무한도전 새해 특집이다. 바스켓을 타고 올라가는 것에 대해 두려워하는 것이 같은 방송사 프로그램 일요일 일요일 밤에의 코너인 〈불가능은 없다〉와 흡사하다는 내용이었다. 또, 일본의 프로그램을 표절하였다는 의혹도 받기도 하였다. 그 예로 2007년 12월 15일 방송된 달력 만들기 특집이 일본 요미우리 TV의 프로그램 \"하마찬토\"의 “2007년판 남자의 달력 인(in) 베트남”과 흡사하다는 의혹과, 2007년 11월에 방송된 대체 에너지 특집의 자전거 발전이 니혼 TV의 프로그램인 \"디노아라시\"의 “자전거 발전 시험”과 흡사하다는 의혹도 받고 있다. 그 밖에도 뺨을 때리는 기계와 모래공 차기, 브레이크 고장 난 자전거 타기 등은 일본의 후지 TV와 TBS의 프로그램을 표절하였다는 의혹을 받기도 하였다. \n","\n","[Negative context]\n","데이비드 베컴과 빅토리아 애덤스의 슬하에는 네 자식을 두고 있다: 세 아들 브루클린 조지프, (1999년 3월 4일, 런던 웨스트민스터 출생) 로미오 제임스, (2002년 9월 1일, 런던 웨스트민스터 출생) 크루스 다비드, (2005년 2월 20일, 스페인 마드리드 지방 마드리드 출생) 그리고 딸 하퍼 세븐 (2011년 7월 10일, 미국 캘리포니아 주 로스앤젤레스 출생) 이 그들이다. 엘튼 존이 브루클린과 로미오 베컴의 대부를 맡았다. 그들의 대모는 엘리자베스 헐리였다. 베컴의 세 아들 모두 아스널 유소년부에서 축구를 했다. 부친과 마찬가지로, 브루클린과 로미오 모두 모델일을 한 적이 있고, GQ에 영국의 가장 옷을 잘 입는 남자들로 명명되기도 했다. 브루클린은 아스널 U-16부에서 축구를 한 경력이 있지만, 2014-15 시즌 이후에 방출되었다. \n"," 하루아침에 잘나가는 검사에서 범죄자로 전락한 재욱, 그는 자신의 결백을 주장하기 위해 몇번이나 재심신청을 했지만 당연히 모두 기각됐다. 그러다 간수들에게 부동산과 관련된 법률조언을 해준 일을 계기로 교도소 간부들에게 검사일을 하며 얻은 법조계의 노하우를 전수하기 시작했고 5년뒤에는 '9번방 영감님'이라는 이름으로 교도소를 군림하는 경지에 이른다. 하지만 이런 와중에도 결백을 주장하려는 일은 남몰래 계속 펼치고 있던 그때. 엄청난 인연과 마주친다. 5년전 죽은 진석과 똑같은 말을 하던 젊은 남자였다. 그의 이름은 한치원. 빼어난 미모를 가졌지만 입만 열었다 하면 쉴새없이 거짓말을 늘어놓고 숨쉬는것 빼고는 모두 거짓말일 정도의 프로사기꾼이었다. 재욱은 자신이 부탁하는 일만 도와준다면 무죄로 출소하게 해주겠다는 조건으로 치원을 꼬드겼고 치원은 그런 재욱을 수상히 여기면서도 출소라는 말에 덜컥 요구를 수락했다.\n"]}],"source":["print('[Positive context]')\n","print(p_with_neg[0], '\\n')\n","print('[Negative context]')\n","print(p_with_neg[1], '\\n', p_with_neg[2])"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1658734204254,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"jiacVxXFeBbK"},"outputs":[],"source":["from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n","\n","q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n","p_seqs = tokenizer(p_with_neg, padding=\"max_length\", truncation=True, return_tensors='pt')\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1658734204255,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"dvzIayy79Mdy","outputId":"b0d92692-780d-4352-a36e-4175d7ef8e4a"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([20, 4, 512])\n"]}],"source":["max_len = p_seqs['input_ids'].size(-1)\n","p_seqs['input_ids'] = p_seqs['input_ids'].view(-1, num_neg+1, max_len)\n","p_seqs['attention_mask'] = p_seqs['attention_mask'].view(-1, num_neg+1, max_len)\n","p_seqs['token_type_ids'] = p_seqs['token_type_ids'].view(-1, num_neg+1, max_len)\n","\n","print(p_seqs['input_ids'].size())  #(num_example, pos + neg, max_len)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658734204256,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"bAplp66Pkayy"},"outputs":[],"source":["train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n","                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])"]},{"cell_type":"markdown","metadata":{"id":"JwMvVH1e3h99"},"source":["2) BERT encoder 학습시키기"]},{"cell_type":"markdown","metadata":{"id":"vW7Oc7Zd9kkm"},"source":["BertEncoder 모델 정의 후, question encoder, passage encoder에 pre-trained weight 불러오기"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658734204256,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"oKKkTlh_l5VL"},"outputs":[],"source":["class BertEncoder(BertPreTrainedModel):\n","  def __init__(self, config):\n","    super(BertEncoder, self).__init__(config)\n","\n","    self.bert = BertModel(config)\n","    self.init_weights()\n","      \n","  def forward(self, input_ids, \n","              attention_mask=None, token_type_ids=None): \n","  \n","      outputs = self.bert(input_ids,\n","                          attention_mask=attention_mask,\n","                          token_type_ids=token_type_ids)\n","      \n","      pooled_output = outputs[1]\n","\n","      return pooled_output\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194,"referenced_widgets":["173a5aa180af440ebbb53d69ee1a8247","3ca7740331cc4ff080aec7a5af4329bd","c449710c5c4247d3b84b16226856df00","680d5661b04f4beb8db7d588beff7d64","863b82fef28a487db392150058ffd871","b021ca029c234dc9bead398d21654da4","e45ff997653049d0aa169477eb0ea2d9","c4840585bc4d4a9896b098954fb47690"]},"executionInfo":{"elapsed":42496,"status":"ok","timestamp":1658734246745,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"wnO1b30SomBP","outputId":"fec38bd6-ecda-44e4-c0f6-59ddad67732e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a15d01a374844ad3ac8c32b45470bb4d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/454M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertEncoder: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertEncoder: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# load pre-trained model on cuda (if available)\n","p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n","q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n","\n","if torch.cuda.is_available():\n","  p_encoder.cuda()\n","  q_encoder.cuda()"]},{"cell_type":"markdown","metadata":{"id":"f3Dgo8U997HD"},"source":["Train function 정의 후, 두개의 encoder fine-tuning 하기 (In-batch negative 활용) \n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1658734246747,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"VAb7NpUc8YRo"},"outputs":[],"source":["def train(args, num_neg, dataset, p_model, q_model):\n","  \n","  # Dataloader\n","  train_sampler = RandomSampler(dataset)\n","  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n","\n","  # Optimizer\n","  no_decay = ['bias', 'LayerNorm.weight']\n","  optimizer_grouped_parameters = [\n","        {'params': [p for n, p in p_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in p_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","        {'params': [p for n, p in q_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in q_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n","\n","  # Start training!\n","  global_step = 0\n","  \n","  p_model.zero_grad()\n","  q_model.zero_grad()\n","  torch.cuda.empty_cache()\n","  \n","  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n","\n","  for _ in train_iterator:\n","    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n","\n","    for step, batch in enumerate(epoch_iterator):\n","      q_encoder.train()\n","      p_encoder.train()\n","      \n","      targets = torch.zeros(args.per_device_train_batch_size).long()\n","      if torch.cuda.is_available():\n","        batch = tuple(t.cuda() for t in batch)\n","        targets = targets.cuda()\n","\n","      p_inputs = {'input_ids': batch[0].view(\n","                                    args.per_device_train_batch_size*(num_neg+1), -1),\n","                  'attention_mask': batch[1].view(\n","                                    args.per_device_train_batch_size*(num_neg+1), -1),\n","                  'token_type_ids': batch[2].view(\n","                                    args.per_device_train_batch_size*(num_neg+1), -1)\n","                  }\n","      \n","      q_inputs = {'input_ids': batch[3],\n","                  'attention_mask': batch[4],\n","                  'token_type_ids': batch[5]}\n","      \n","      p_outputs = p_model(**p_inputs)  #(batch_size*(num_neg+1), emb_dim)\n","      q_outputs = q_model(**q_inputs)  #(batch_size*, emb_dim)\n","\n","      # Calculate similarity score & loss\n","      p_outputs = p_outputs.view(args.per_device_train_batch_size, -1, num_neg+1)\n","      q_outputs = q_outputs.view(args.per_device_train_batch_size, 1, -1)\n","\n","      sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()  #(batch_size, num_neg+1)\n","      sim_scores = sim_scores.view(args.per_device_train_batch_size, -1)\n","      sim_scores = F.log_softmax(sim_scores, dim=1)\n","\n","      loss = F.nll_loss(sim_scores, targets)\n","      print(loss)\n","\n","      loss.backward()\n","      optimizer.step()\n","      scheduler.step()\n","      q_model.zero_grad()\n","      p_model.zero_grad()\n","      global_step += 1\n","      \n","      torch.cuda.empty_cache()\n","\n","\n","    \n","  return p_model, q_model\n","\n","\n"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1658734246747,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"ICSJoJrUDGZ5"},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir=\"dense_retireval\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    num_train_epochs=2, # For demonstration\n","    weight_decay=0.01\n",")\n"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24091,"status":"ok","timestamp":1658734270827,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"E8a7ww3WgsaZ","outputId":"61305269-9f68-44b3-b6c0-e4a8bf692fa6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9e76fa727824a49a6e15614a867b47e","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 8.00 GiB total capacity; 6.31 GiB already allocated; 4.83 MiB free; 6.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\NLP_MRC\\MRC Mission 5 Answer - Dense Passage Retrieval.ipynb 셀 31\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=0'>1</a>\u001b[0m \u001b[39m# 훈련이 되어있는 모델 사용\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=1'>2</a>\u001b[0m p_encoder, q_encoder \u001b[39m=\u001b[39m train(args, num_neg, train_dataset, p_encoder, q_encoder)\n","\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\NLP_MRC\\MRC Mission 5 Answer - Dense Passage Retrieval.ipynb 셀 31\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, num_neg, dataset, p_model, q_model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=39'>40</a>\u001b[0m p_inputs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=40'>41</a>\u001b[0m                               args\u001b[39m.\u001b[39mper_device_train_batch_size\u001b[39m*\u001b[39m(num_neg\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=41'>42</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=44'>45</a>\u001b[0m                               args\u001b[39m.\u001b[39mper_device_train_batch_size\u001b[39m*\u001b[39m(num_neg\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=45'>46</a>\u001b[0m             }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=47'>48</a>\u001b[0m q_inputs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m3\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=48'>49</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m4\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=49'>50</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m5\u001b[39m]}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=51'>52</a>\u001b[0m p_outputs \u001b[39m=\u001b[39m p_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mp_inputs)  \u001b[39m#(batch_size*(num_neg+1), emb_dim)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=52'>53</a>\u001b[0m q_outputs \u001b[39m=\u001b[39m q_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mq_inputs)  \u001b[39m#(batch_size*, emb_dim)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=54'>55</a>\u001b[0m \u001b[39m# Calculate similarity score & loss\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\NLP_MRC\\MRC Mission 5 Answer - Dense Passage Retrieval.ipynb 셀 31\u001b[0m in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=8'>9</a>\u001b[0m             attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m): \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=10'>11</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(input_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=11'>12</a>\u001b[0m                         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=12'>13</a>\u001b[0m                         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=14'>15</a>\u001b[0m     pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/NLP_MRC/MRC%20Mission%205%20Answer%20-%20Dense%20Passage%20Retrieval.ipynb#ch0000030?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pooled_output\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1009\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1011\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1012\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1013\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1017\u001b[0m )\n\u001b[1;32m-> 1018\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1019\u001b[0m     embedding_output,\n\u001b[0;32m   1020\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1021\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1022\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1023\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1024\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1025\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1026\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1027\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1029\u001b[0m )\n\u001b[0;32m   1030\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1031\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    598\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    600\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    608\u001b[0m         hidden_states,\n\u001b[0;32m    609\u001b[0m         attention_mask,\n\u001b[0;32m    610\u001b[0m         layer_head_mask,\n\u001b[0;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    613\u001b[0m         past_key_value,\n\u001b[0;32m    614\u001b[0m         output_attentions,\n\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    482\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    483\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    491\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    492\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    494\u001b[0m         hidden_states,\n\u001b[0;32m    495\u001b[0m         attention_mask,\n\u001b[0;32m    496\u001b[0m         head_mask,\n\u001b[0;32m    497\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    498\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    500\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    502\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    414\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    415\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 423\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    424\u001b[0m         hidden_states,\n\u001b[0;32m    425\u001b[0m         attention_mask,\n\u001b[0;32m    426\u001b[0m         head_mask,\n\u001b[0;32m    427\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    428\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    429\u001b[0m         past_key_value,\n\u001b[0;32m    430\u001b[0m         output_attentions,\n\u001b[0;32m    431\u001b[0m     )\n\u001b[0;32m    432\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    433\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:327\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    324\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    326\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    330\u001b[0m     seq_length \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 8.00 GiB total capacity; 6.31 GiB already allocated; 4.83 MiB free; 6.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["# 훈련이 되어있는 모델 사용\n","p_encoder, q_encoder = train(args, num_neg, train_dataset, p_encoder, q_encoder)"]},{"cell_type":"markdown","metadata":{"id":"BGOw-k7Ln85t"},"source":["## Dense Embedding을 활용하여 passage retrieval 실습해보기"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":717,"status":"ok","timestamp":1658734271532,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"NouB9uBcTaws","outputId":"87cbd7fd-0ac1-4b5d-ab2a-f59bf4e6d8f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["유아인에게 타고난 배우라고 말한 드라마 밀회의 감독은?\n","화보 촬영을 위해 미국에 있을 때, 김희애의 연락을 통해 JTBC 드라마 《밀회》의 캐스팅을 제안받았다. 당시 영화 《베테랑》에 이미 캐스팅된 상태였으나, 유아인은 류승완 감독과 제작사의 양해를 얻어 《밀회》에 출연한다. 천재 피아니스트 ‘이선재’ 역할을 위해 피아니스트들의 영상을 보고 곡의 스피드와 건반 위치 등을 외워 실제 타건을 하며 촬영했다. 피아노 울림판을 수건으로 막고 타건을 하면, 그 후 대역 피아니스트의 소리를 덧입히는 방식이었다. 《밀회》는 작품성을 인정받고 숱한 화제를 낳으며 당시 종편으로서는 높은 시청률을 기록했다. 유아인은 섬세한 연기력을 선보여 순수함으로 시청자들을 매료시켰다는 호평을 얻었고, 특히 피아노 연주에 있어서 클래식 종사자들에게 인정을 받았다. 연출을 맡은 안판석 감독은 유아인에 대해 “느낌으로만 연기를 하는 게 아니고 감성을 지적으로 통제해 가면서 연기한다. 그 나이에”라며 “타고난 배우”라고 말했다. 유아인은 《밀회》를 통해 예술적인 면모를 구체화할 수 있어서 만족감을 느꼈다고 밝혔으며, 종영 후 자신의 페이스북 계정에 긴 소감글을 남겼다. 특히 ‘이선재’ 캐릭터를 배우 유아인이 가진 소년성의 엑기스로 생각하며, 2015년 10월 부산국제영화제 오픈토크에서는 본인이 가장 좋아하는 캐릭터로 꼽았다. \n","\n","\n"]}],"source":["valid_corpus = list(set([example['context'] for example in dataset['validation']]))[:10]\n","sample_idx = random.choice(range(len(dataset['validation'])))\n","query = dataset['validation'][sample_idx]['question']\n","ground_truth = dataset['validation'][sample_idx]['context']\n","\n","if not ground_truth in valid_corpus:\n","  valid_corpus.append(ground_truth)\n","\n","print(query)\n","print(ground_truth, '\\n\\n')\n","\n","# valid_corpus"]},{"cell_type":"markdown","metadata":{"id":"05D8GzFrJhHO"},"source":["앞서 학습한 passage encoder, question encoder을 이용해 dense embedding 생성"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658734271533,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"ba-hH3NQOEWJ"},"outputs":[],"source":["def to_cuda(batch):\n","  return tuple(t.cuda() for t in batch)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658734271533,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"YufA_ayPJBRg","outputId":"df47b5fd-8a6f-4539-b4eb-562f92a1aab7"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([11, 768]) torch.Size([1, 768])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\AI_15\\AppData\\Local\\Temp\\ipykernel_15848\\1112144075.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n","  p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n"]}],"source":["with torch.no_grad():\n","  p_encoder.eval()\n","  q_encoder.eval()\n","\n","  q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n","  q_emb = q_encoder(**q_seqs_val).to('cpu')  #(num_query, emb_dim)\n","\n","  p_embs = []\n","  for p in valid_corpus:\n","    p = tokenizer(p, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n","    p_emb = p_encoder(**p).to('cpu').numpy()\n","    p_embs.append(p_emb)\n","\n","p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n","\n","print(p_embs.size(), q_emb.size())"]},{"cell_type":"markdown","metadata":{"id":"pOHHak7WS1ko"},"source":["생성된 embedding에 dot product를 수행 => Document들의 similarity ranking을 구함"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":639,"status":"ok","timestamp":1658734272169,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"xn5Cx5JkKZJB","outputId":"cd1baca3-225d-4bac-b949-eb426473d310"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 11])\n","tensor([[ 48.7925, 101.2781, -30.6381, 138.7635,  39.2943, 139.0693, 136.8548,\n","          37.1513,  11.3480,  -5.7385, 105.6256]])\n","tensor([ 5,  3,  6, 10,  1,  0,  4,  7,  8,  9,  2])\n"]}],"source":["dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n","print(dot_prod_scores.size())\n","\n","rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n","print(dot_prod_scores)\n","print(rank)"]},{"cell_type":"markdown","metadata":{"id":"Oq2Oiv8MKVS6"},"source":["Top-5개의 passage를 retrieve 하고 ground truth와 비교하기"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658734272170,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"WaStRXYdJ-wI","outputId":"324f10ce-8db6-4277-87f1-e8dbb545bd0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Search query]\n"," 유아인에게 타고난 배우라고 말한 드라마 밀회의 감독은? \n","\n","[Ground truth passage]\n","화보 촬영을 위해 미국에 있을 때, 김희애의 연락을 통해 JTBC 드라마 《밀회》의 캐스팅을 제안받았다. 당시 영화 《베테랑》에 이미 캐스팅된 상태였으나, 유아인은 류승완 감독과 제작사의 양해를 얻어 《밀회》에 출연한다. 천재 피아니스트 ‘이선재’ 역할을 위해 피아니스트들의 영상을 보고 곡의 스피드와 건반 위치 등을 외워 실제 타건을 하며 촬영했다. 피아노 울림판을 수건으로 막고 타건을 하면, 그 후 대역 피아니스트의 소리를 덧입히는 방식이었다. 《밀회》는 작품성을 인정받고 숱한 화제를 낳으며 당시 종편으로서는 높은 시청률을 기록했다. 유아인은 섬세한 연기력을 선보여 순수함으로 시청자들을 매료시켰다는 호평을 얻었고, 특히 피아노 연주에 있어서 클래식 종사자들에게 인정을 받았다. 연출을 맡은 안판석 감독은 유아인에 대해 “느낌으로만 연기를 하는 게 아니고 감성을 지적으로 통제해 가면서 연기한다. 그 나이에”라며 “타고난 배우”라고 말했다. 유아인은 《밀회》를 통해 예술적인 면모를 구체화할 수 있어서 만족감을 느꼈다고 밝혔으며, 종영 후 자신의 페이스북 계정에 긴 소감글을 남겼다. 특히 ‘이선재’ 캐릭터를 배우 유아인이 가진 소년성의 엑기스로 생각하며, 2015년 10월 부산국제영화제 오픈토크에서는 본인이 가장 좋아하는 캐릭터로 꼽았다. \n","\n","Top-1 passage with score 139.0693\n","김태희는 이후 지하철에서 광고대행사 디자이너를 우연히 만나 2000년 \"화이트\" 광고를 찍으면서 연예계에 데뷔했다. 광고 촬영 이후 여러 곳에서 모델 제의가 들어왔고 그녀가 서울대학교 출신이라는 점 때문에 더 화제가 되었다. 2001년 영화 《선물》에서 중등 정연 역을 맡으며 스크린에 첫 데뷔했다. 당시 영화 감독 오기환은 잡지 표지모델로 섰던 김태희를 보고 테스트도 안 해도되니 그냥 데려 오라고 말했다. 이후 대학생활의 숙원이었던 어학연수를 가기 전 홍두현 감독의 독립영화 《신도시인》 출연을 제안받아 출연했고 2002년 개봉했다. 같은 해 경험삼아 SBS 시트콤 《레츠고》에 출연했으나, 2개월만에 종영되고 배우의 길에 대한 고민을 했다.\n","Top-2 passage with score 138.7635\n","그러나 이 기간에도 1986년 FIFA 월드컵과 1990년 FIFA 월드컵에서 연속 16강에 진출했으며(다만 1986년에는 서독, 스코틀랜드와 비기고 덴마크에 1:6으로 패했음에도 조 3위 와일드카드를 얻어 16강에 진출한 것. 1990년에도 대한민국을 상대로 오프사이드 오심 골로 1:0으로 승리했을 뿐 1승 1무 2패에 겨우 2득점만 얻었다.) 1983년 코파 아메리카, 1987년 코파 아메리카 2연패에 1989년 코파 아메리카, 1999년 코파 아메리카 준우승을 하는 등 나름대로 남미 안에선 강한 모습을 보였다. 워낙 이전의 성적이 좋았기 때문에 상대적으로 암흑기였을 뿐 실력 자체는 꾸준히 중상위급 수준을 유지하고 있었다.\n","Top-3 passage with score 136.8548\n","충주중학교를 졸업하였고, 충주고등학교 2학년 때 미국 적십자사에서 주최하는 영어경시대회에서 최고점수를 받았다. 부상으로 '외국 학생의 미국 방문 프로그램(VISTA)'에 선발되어 1962년 고등학교 3학년 때 미국을 방문했다. 한 달간 미국 연수 및 봉사활동에서 존 F. 케네디 대통령을 만나 외교관의 꿈을 키웠다. 1963년 충주고등학교를 수석으로 졸업하고 서울대학교 외교학과에 진학했다. 1965년 4월부터 약 2년 6개월간 육군 병장으로 군 복무를 마쳤다. 1970년 2월 서울대학교 외교학과 졸업과 동시에 제3회 외무고시에 차석으로 합격해 그 해 3월 외무부에 들어갔다. 신입 외교관 연수를 마칠 때 수석을 차지했다. UN 국제 영어, 프랑스어, 독일어, 일본어 소통이 가능하다.\n","Top-4 passage with score 105.6256\n","화보 촬영을 위해 미국에 있을 때, 김희애의 연락을 통해 JTBC 드라마 《밀회》의 캐스팅을 제안받았다. 당시 영화 《베테랑》에 이미 캐스팅된 상태였으나, 유아인은 류승완 감독과 제작사의 양해를 얻어 《밀회》에 출연한다. 천재 피아니스트 ‘이선재’ 역할을 위해 피아니스트들의 영상을 보고 곡의 스피드와 건반 위치 등을 외워 실제 타건을 하며 촬영했다. 피아노 울림판을 수건으로 막고 타건을 하면, 그 후 대역 피아니스트의 소리를 덧입히는 방식이었다. 《밀회》는 작품성을 인정받고 숱한 화제를 낳으며 당시 종편으로서는 높은 시청률을 기록했다. 유아인은 섬세한 연기력을 선보여 순수함으로 시청자들을 매료시켰다는 호평을 얻었고, 특히 피아노 연주에 있어서 클래식 종사자들에게 인정을 받았다. 연출을 맡은 안판석 감독은 유아인에 대해 “느낌으로만 연기를 하는 게 아니고 감성을 지적으로 통제해 가면서 연기한다. 그 나이에”라며 “타고난 배우”라고 말했다. 유아인은 《밀회》를 통해 예술적인 면모를 구체화할 수 있어서 만족감을 느꼈다고 밝혔으며, 종영 후 자신의 페이스북 계정에 긴 소감글을 남겼다. 특히 ‘이선재’ 캐릭터를 배우 유아인이 가진 소년성의 엑기스로 생각하며, 2015년 10월 부산국제영화제 오픈토크에서는 본인이 가장 좋아하는 캐릭터로 꼽았다.\n","Top-5 passage with score 101.2781\n","대한올림픽위원회(KOC)는 무주와 평창의 개최 후보 도시 중 평창을 개최 후보 도시로 선정하였다. 두 도시의 개최 후보 도시 경쟁은 2000년 10월부터 시작되었다. 강원도가 평창을 후보지로 앞세워 2010년 동계 올림픽 유치를 선언하자, 전라북도에서도 1997년 동계 유니버시아드의 개최지인 무주를 후보지로 앞세워 유치를 선언하였다. 지역적 대립 양상을 보이던 유지 후보 선정 과정이 2001년 11월 16일에 대한올림픽위원회에서 두 후보지를 공동 개최지로 결정하여 대립이 마무리 되는 듯 하였으나, 국제스키연맹이 무주에 대해 실사하여 부적격 판정을 하자 두 지역간의 대립 끝에 결국 2002년 1월 9일 대한올림픽위원회 임시위원회에서 평창을 주 개최지로 선정하였다. 그 후, 전라북도와 강원도, 그리고 대한올림픽위원회가 2014년 동계 올림픽 후보지 선정 과정까지 무주가 국제 시설기준을 충족하는 것을 전제로 하여, 2010년 동계올림픽 유치 도시는 무주가 평창에 양보하고, 그 대신 2014년 단독 유치 신청은 우선권을 갖는다는 합의문을 작성하여, 이 내용을 바탕으로 2010년 5월 25일에 대한올림픽위원회 제7차 상임위원회에서 평창의 단독 유치안을 통과하면서 2010년 동계 올림픽의 대한민국의 후보 도시는 평창으로 선정되었고, 2014년 동계 올림픽에서는 무주의 단독 유치 우선권을 인정하였다.\n"]}],"source":["k = 5\n","print(\"[Search query]\\n\", query, \"\\n\")\n","print(\"[Ground truth passage]\")\n","print(ground_truth, \"\\n\")\n","\n","for i in range(k):\n","  print(\"Top-%d passage with score %.4f\" % (i+1, dot_prod_scores.squeeze()[rank[i]]))\n","  print(valid_corpus[rank[i]])"]},{"cell_type":"markdown","metadata":{"id":"exmKvxN2Wb_h"},"source":["## Wikipedia documents에 대해 Dense Passage Retrieval 실습하기"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2811,"status":"ok","timestamp":1658734274977,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"HUOhUIkc-BIJ","outputId":"81eb55b6-8f25-4462-e066-95f853b0cb1f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Cannot open cookies file '/tmp/cookies.txt': No such file or directory\n","--2022-07-28 11:21:48--  https://docs.google.com/uc?export=download&confirm=$(wget%20--quiet%20--save-cookies%20/tmp/cookies.txt%20--keep-session-cookies%20--no-check-certificate%20'https://docs.google.com/uc?export=download&id=1wVIgtc0YoQEwXB3JAsUud_86fRzrFCBd'%20-O-%20%7C%20sed%20-rn%20's/.*confirm=([0-9A-Za-z_]+).*/%5C1%5Cn/p')&id=1wVIgtc0YoQEwXB3JAsUud_86fRzrFCBd\n","Resolving docs.google.com (docs.google.com)... 142.250.207.110\n","Connecting to docs.google.com (docs.google.com)|142.250.207.110|:443... connected.\n","HTTP request sent, awaiting response... 404 Not Found\n","2022-07-28 11:21:49 ERROR 404: Not Found.\n","\n"]}],"source":["!wg --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wVIgtc0YoQEwXB3JAsUud_86fRzrFCBd' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1wVIgtc0YoQEwXB3JAsUud_86fRzrFCBd\" -O wikipedia_documenet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quietts.json && rm -rf /tmp/cookies.txt"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":1193,"status":"ok","timestamp":1658734276167,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"ZVquUAGQWb_h"},"outputs":[],"source":["# First load wikipedia dump\n","import json\n","\n","dump_path = 'wikipedia_documents.json' \n","with open(dump_path, 'r', encoding='UTF-8') as f:\n","    wiki = json.load(f)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1658734276168,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"ybxCqzl_Wb_h","outputId":"db72e965-bf90-457d-bf56-0a9dd125aa19"},"outputs":[{"data":{"text/plain":["(60613, dict)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["len(wiki), type(wiki)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1658734276169,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"8YkE3rjFWb_i","outputId":"ad3f50bd-0c43-490f-d2ba-db9c5173f6d9"},"outputs":[{"data":{"text/plain":["{'text': '이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개요 형태로 나열하고 있다.\\n\\n이 목록은 명료화를 위해 두 부분으로 나뉘어 있다.\\n\\n# 첫 번째 부분은 바티칸 시국과 팔레스타인을 포함하여 유엔 등 국제 기구에 가입되어 국제적인 승인을 널리 받았다고 여기는 195개 나라를 나열하고 있다.\\n# 두 번째 부분은 일부 지역의 주권을 사실상 (데 팍토) 행사하고 있지만, 아직 국제적인 승인을 널리 받지 않았다고 여기는 11개 나라를 나열하고 있다.\\n\\n두 목록은 모두 가나다 순이다.\\n\\n일부 국가의 경우 국가로서의 자격에 논쟁의 여부가 있으며, 이 때문에 이러한 목록을 엮는 것은 매우 어렵고 논란이 생길 수 있는 과정이다. 이 목록을 구성하고 있는 국가를 선정하는 기준에 대한 정보는 \"포함 기준\" 단락을 통해 설명하였다. 나라에 대한 일반적인 정보는 \"국가\" 문서에서 설명하고 있다.',\n"," 'corpus_source': '위키피디아',\n"," 'url': 'TODO',\n"," 'domain': None,\n"," 'title': '나라 목록',\n"," 'author': None,\n"," 'html': None,\n"," 'document_id': 0}"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["wiki['0']"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658734276169,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"dEN0QBDQTkgH"},"outputs":[],"source":["corpus = [document['text'] for document_id, document in wiki.items()]"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["39d0c8df3fcb4439972019ee23720b3f","67b251c48c8b4b349dfbbebdaae9a2ee","89ed08bacaa545aca48b40c683ff2754","89953d5228154815a292e39b4e4f0a98","745662a91275484e8c204cbe4de7812f","1461e1df2a054f63a27072651317b8c5","d024701890f341b6bb3dc929a1873626","9d9c3bb754344861af4c31740436ab32"]},"executionInfo":{"elapsed":2220380,"status":"ok","timestamp":1658736496539,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"ZIyQp-NpWb_i","outputId":"0d11d7b7-2f10-4406-93f1-4103f568b7fc"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9edd4e7f497b40bead9c5044551ba3a1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/60613 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tqdm.auto import tqdm\n","# wiki_embs = embedded dense vectors of documents\n","with torch.no_grad():\n","  wiki_embs = []\n","  for text in tqdm(corpus):\n","    p = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n","    wiki_emb = p_encoder(**p).to('cpu').numpy()\n","    wiki_embs.append(wiki_emb)\n","\n","wiki_embs = torch.Tensor(wiki_embs).squeeze()  # (num_passage, emb_dim)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1658736496540,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"-mp5M2nhWb_i","outputId":"a8b70ac5-5c8e-4055-eb1f-b3bc3874cba1"},"outputs":[{"data":{"text/plain":["torch.Size([60613, 768])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# Answer: (60613, 768) (num_doc, emb_dim)\n","wiki_embs.shape"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1658736496540,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"IfWc8QecWb_i"},"outputs":[],"source":["# TODO: embed query to dense vector\n","query = '대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?'\n","with torch.no_grad():\n","  q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n","  q_emb = q_encoder(**q_seqs_val).to('cpu')  #(num_query, emb_dim)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1658736496541,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"3iXRmAnMWb_i","outputId":"4a900270-47a2-4851-dbda-ab39f73b8827"},"outputs":[{"data":{"text/plain":["torch.Size([1, 60613])"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["result = torch.matmul(q_emb, torch.transpose(wiki_embs, 0, 1))\n","result.shape"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1658736496541,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"SLvUQ5chWb_i"},"outputs":[],"source":["k = 5\n","rank = torch.argsort(result, dim=1, descending=True).squeeze()"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1658736496541,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"96FUB64OTiRD","outputId":"943c4754-9c35-4046-b69d-e3f8bae4f9ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Search query]\n"," 대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은? \n","\n","Top-1 passage with score 212.0865\n","야음초등학교  \n","약사초등학교  \n","약수초등학교  \n","양사초등학교  \n","양지초등학교  \n","언양초등학교  \n","여천초등학교  \n","연암초등학교  \n","염포초등학교  \n","영화초등학교  \n","옥동초등학교  \n","옥산초등학교  \n","옥서초등학교  \n","옥성초등학교  \n","옥현초등학교  \n","온남초등학교  \n","온산초등학교  \n","온양초등학교  \n","용연초등학교  \n","우정초등학교  \n","울산남부초등학교  \n","울산양정초등학교  \n","울산중앙초등학교  \n","울산초등학교  \n","울주명지초등학교  \n","웅촌초등학교  \n","웅촌초등학교 검단분교  \n","월계초등학교  \n","월봉초등학교  \n","월평초등학교  \n","이화초등학교  \n","일산초등학교  \n","외솔초등학교\n","Top-2 passage with score 208.5267\n","자인초등학교 \n","자천초등학교 보현분교 \n","자천초등학교 \n","장곡초등학교 \n","장기초등학교 \n","장기초등학교 모포분교 \n","장량초등학교 \n","장산초등학교 \n","장성초등학교 \n","장수초등학교 \n","장천초등학교 \n","재산초등학교 \n","저동초등학교 \n","점곡초등학교 \n","점촌북초등학교 \n","점촌중앙초등학교 \n","점촌초등학교 \n","정수초등학교 \n","정평초등학교 \n","조마초등학교 \n","죽도초등학교 \n","죽변초등학교 \n","죽장초등학교 상옥분교 \n","죽장초등학교 \n","죽천초등학교 \n","중동초등학교 \n","중모초등학교 \n","증산초등학교 \n","지곡초등학교 \n","지동초등학교 \n","지례초등학교 \n","지방초등학교 \n","지보초등학교 \n","지사초등학교 \n","지산초등학교 \n","지천초등학교 \n","지품초등학교 \n","직지초등학교 \n","진량초등학교 \n","진보초등학교 \n","진성초등학교 \n","진평초등학교\n","Top-3 passage with score 207.2272\n","자은초등학교 \n","작천초등학교 \n","장동초등학교 \n","장산초등학교 \n","장성성산초등학교 \n","장성중앙초등학교 \n","장천초등학교 \n","장평초등학교 \n","장흥남초등학교 \n","장흥서초등학교 \n","장흥초등학교 \n","점암초등학교 신안분교 \n","점암초등학교 화계분교 \n","점암초등학교 \n","조도초등학교 거차분교 \n","조도초등학교 관사분교 \n","조도초등학교 대마분교 \n","조도초등학교 \n","조성남초등학교 \n","조성초등학교 \n","주암초등학교 \n","죽곡초등학교 \n","죽림초등학교 \n","중동초등학교 \n","중흥초등학교 \n","증도초등학교 병풍도분교 \n","증도초등학교 소악분교 \n","증도초등학교 \n","지도초등학교 \n","지도초등학교 선치분교 \n","지도초등학교 어의분교 \n","지산초등학교 \n","진도서초등학교 가사도분교 \n","진도서초등학교 \n","진도초등학교 \n","진상초등학교 황죽분교 \n","진상초등학교 \n","진원동초등학교 \n","진원초등학교 \n","진월초등학교\n","Top-4 passage with score 204.1842\n","삼봉초등학교  \n","삼봉초등학교 난지분교  \n","삼성초등학교  \n","삼은초등학교  \n","삽교초등학교  \n","상곡초등학교  \n","상록초등학교  \n","상서초등학교  \n","상월초등학교  \n","서남초등학교  \n","서도초등학교  \n","서동초등학교  \n","서령초등학교  \n","서림초등학교  \n","서면초등학교  \n","서부초등학교  \n","서산대진초등학교  \n","서산동문초등학교  \n","서산석림초등학교  \n","서산예천초등학교  \n","서산부성초등학교 \n","서산초등학교  \n","서정초등학교  \n","서천초등학교  \n","서해삼육초등학교  \n","석문초등학교  \n","석성초등학교  \n","석송초등학교  \n","석양초등학교  \n","선장초등학교  \n","성거초등학교  \n","성광초등학교  \n","성남초등학교  \n","성당초등학교  \n","성대초등학교  \n","성덕초등학교  \n","성동초등학교  \n","성신초등학교  \n","성연초등학교  \n","성주초등학교  \n","성환초등학교  \n","세도초등학교  \n","소망초등학교  \n","소원초등학교 의항분교  \n","소원초등학교  \n","송간초등학교  \n","송곡초등학교  \n","송남초등학교  \n","송림초등학교  \n","송림초등학교 유부도분교  \n","송산초등학교  \n","송석초등학교  \n","송악초등학교  \n","송암초등학교  \n","송학초등학교  \n","수덕초등학교  \n","수신초등학교  \n","수정초등학교  \n","수촌초등학교  \n","순성초등학교  \n","시량초등학교  \n","시목초등학교  \n","시초초등학교  \n","신가초등학교  \n","신계초등학교  \n","신관초등학교  \n","신광초등학교  \n","신당초등학교  \n","신대초등학교  \n","신도초등학교  \n","신례원초등학교  \n","신리초등학교  \n","신방초등학교  \n","신사초등학교  \n","신암초등학교  \n","신양초등학교  \n","신창초등학교  \n","신촌초등학교  \n","신평초등학교  \n","신풍초등학교  \n","신화초등학교  \n","신흥초등학교  \n","쌍룡초등학교\n","Top-5 passage with score 203.0470\n","차동초등학교  \n","창기초등학교  \n","채운초등학교  \n","천동초등학교  \n","천북초등학교  \n","천안가온초등학교  \n","천안구성초등학교  \n","천안남산초등학교  \n","천안두정초등학교  \n","천안미라초등학교  \n","천안백석초등학교  \n","천안봉명초등학교  \n","천안봉서초등학교  \n","천안부대초등학교  \n","천안부성초등학교  \n","천안부영초등학교  \n","천안불당초등학교  \n","천안삼거리초등학교  \n","천안새샘초등학교  \n","천안서당초등학교  \n","천안서초등학교  \n","천안성정초등학교  \n","천안수곡초등학교  \n","천안신대초등학교  \n","천안신부초등학교  \n","천안신안초등학교  \n","천안신용초등학교  \n","천안신촌초등학교  \n","천안쌍용초등학교  \n","천안쌍정초등학교  \n","천안업성초등학교  \n","천안오성초등학교  \n","천안와촌초등학교  \n","천안용곡초등학교  \n","천안용소초등학교  \n","천안용암초등학교  \n","천안월봉초등학교  \n","천안일봉초등학교  \n","천안중앙초등학교  \n","천안청당초등학교  \n","천안청룡초등학교  \n","천안청수초등학교  \n","천안초등학교  \n","천안희망초등학교  \n","천의초등학교  \n","청남초등학교  \n","청동초등학교  \n","청라초등학교  \n","청룡초등학교  \n","청룡초등학교 고대분교  \n","청룡초등학교 장고분교  \n","청보초등학교  \n","청소초등학교  \n","청송초등학교  \n","청양초등학교  \n","청파초등학교 호도분교  \n","청파초등학교  \n","초락초등학교  \n","초촌초등학교  \n","추부초등학교  \n","충무초등학교  \n","충화초등학교\n"]}],"source":["k = 5\n","print(\"[Search query]\\n\", query, \"\\n\")\n","\n","for i in range(k):\n","  print(\"Top-%d passage with score %.4f\" % (i+1, result.squeeze()[rank[i]]))\n","  print(corpus[rank[i]])"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1658736496542,"user":{"displayName":"Hyunji Lee","userId":"07448774681661174344"},"user_tz":-540},"id":"SvjJCYxZFPhz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Jul 28 13:36:57 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 451.67       Driver Version: 451.67       CUDA Version: 11.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  GeForce RTX 207... WDDM  | 00000000:01:00.0  On |                  N/A |\n","| N/A   49C    P0    28W /  N/A |   7813MiB /  8192MiB |      2%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      1128    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n","|    0   N/A  N/A      1552    C+G   Insufficient Permissions        N/A      |\n","|    0   N/A  N/A      2696    C+G   ...ge\\Application\\msedge.exe    N/A      |\n","|    0   N/A  N/A      3784    C+G   C:\\Windows\\explorer.exe         N/A      |\n","|    0   N/A  N/A      4340    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n","|    0   N/A  N/A      6044    C+G   ...artMenuExperienceHost.exe    N/A      |\n","|    0   N/A  N/A      6048    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n","|    0   N/A  N/A      6496    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n","|    0   N/A  N/A      6648    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n","|    0   N/A  N/A      9972    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n","|    0   N/A  N/A     11272    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n","|    0   N/A  N/A     12772    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n","|    0   N/A  N/A     14048    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n","|    0   N/A  N/A     15848      C   ...ython\\Python38\\python.exe    N/A      |\n","|    0   N/A  N/A     17284    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n","|    0   N/A  N/A     21376    C+G   ...kyb3d8bbwe\\HxAccounts.exe    N/A      |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"MRC Mission 5 Answer - Dense Passage Retrieval.ipynb","provenance":[{"file_id":"1c9Vr7z_LBG2l9K4lVb40pu7Kk22hXQCp","timestamp":1614240569955},{"file_id":"1Q7iAXm_kwF_NHfOEGdViMCiPHnqoZlXe","timestamp":1613491158162}]},"kernelspec":{"display_name":"pointnet","language":"python","name":"pointnet"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"b7a88ccba101d5627a87a914a30c229ccd2d0ee8a235abb18b45220b524f9c40"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"028c8597a911485a8ba0da41ef5807ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22e308be2cd84ac1a8fe75396d989e68","IPY_MODEL_cf77c57b5a724ac3b329e6157e95147d"],"layout":"IPY_MODEL_db1215991a8f433dac8d5262d72bd982"}},"06539b3346084f6badb98cb30acdaf76":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07d527d2ba38401180172ebde131e379":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b4987d3f2c943e48cdb538649d16eb4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c14b3dab4374f50b45c3ff336ad874d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7e2670aaab747c8a93df57925d3a294","placeholder":"​","style":"IPY_MODEL_f4fa398a13854ec6b85e787e69e9da5a","value":" 38.5M/? [00:01&lt;00:00, 36.6MB/s]"}},"0e09a7896b464f6ea9f178809dfb602a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"1461e1df2a054f63a27072651317b8c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"1678f427ba3747c3ac6204c683287ee6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"173a5aa180af440ebbb53d69ee1a8247":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ca7740331cc4ff080aec7a5af4329bd","IPY_MODEL_c449710c5c4247d3b84b16226856df00"],"layout":"IPY_MODEL_680d5661b04f4beb8db7d588beff7d64"}},"175a62ae56354ee881db9221d95b7165":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"19d6fee0d4bb41caa5952462527b0cf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"221fa80b0c544e4a903a5dd8f1b086f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4629172ce3a94189a5beb1f90382d2ac","placeholder":"​","style":"IPY_MODEL_9a2ef5ce42f64adcb2cebbc744730da8","value":" 80.0/80.0 [00:01&lt;00:00, 42.9B/s]"}},"22e308be2cd84ac1a8fe75396d989e68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_9aa72c7ad6de403b9d1af9d66bc6f9ac","max":725,"min":0,"orientation":"horizontal","style":"IPY_MODEL_924e1b1ec5e441c297d17f02caa12a66","value":725}},"25d58375a88f47c3939665e846184cda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28914da5acd440aea31de315ec453c2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d0c8df3fcb4439972019ee23720b3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67b251c48c8b4b349dfbbebdaae9a2ee","IPY_MODEL_89ed08bacaa545aca48b40c683ff2754"],"layout":"IPY_MODEL_89953d5228154815a292e39b4e4f0a98"}},"3a9bda724b434b688d56a33ca225821f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3ca7740331cc4ff080aec7a5af4329bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_863b82fef28a487db392150058ffd871","max":475782997,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b021ca029c234dc9bead398d21654da4","value":475782997}},"4433e92fa1cd45f9b9ca8059815ddb46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: ","description_tooltip":null,"layout":"IPY_MODEL_46bd393c1cdb4856a4c8cac744c33e03","max":962,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f87214c9c27d4253bd1190f230764148","value":962}},"4629172ce3a94189a5beb1f90382d2ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46bd393c1cdb4856a4c8cac744c33e03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"476193bb5f4a4e0da534c3cb2d2da4fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_f338007664d4406d80b96cda58220447","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_668e663d6c9b4c469eb540e1b573f8b2","value":1}},"4906498dc2104672aeb25a5bfa58e8bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"54d50f65f13c41e5abb5ca35502a04ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f44b72f57f14ea7aaec4e5682a04ac4","IPY_MODEL_efadd2807af14d56a43a5dcd37e38f4b"],"layout":"IPY_MODEL_06539b3346084f6badb98cb30acdaf76"}},"550a559e977b4dc58ab9c961b7b0fff9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c98997be7484f779f6708e16297b00e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"5d3f6edae58649c4aca490fe617486f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f35f8171392c4367a9bb4f6d07ddd4c8","IPY_MODEL_c35b2dc92ab34645b88bfed5517f5187"],"layout":"IPY_MODEL_8aedc23beece413ca2d2553a1937925d"}},"5f7926cbead54f6999c56b188c9b3038":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7ed8d296c344ef98b1ef51f770c13e9","placeholder":"​","style":"IPY_MODEL_07d527d2ba38401180172ebde131e379","value":" 336k/336k [00:05&lt;00:00, 62.4kB/s]"}},"61cae5a7e9a843b282238b978baaef9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_dcdb84db38344248821b6a2f542940aa","max":80,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d92d6fabf8174ec7b40d734921718325","value":80}},"65f88172c01f4e7884edfca2688d9435":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"668e663d6c9b4c469eb540e1b573f8b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"67b251c48c8b4b349dfbbebdaae9a2ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_745662a91275484e8c204cbe4de7812f","max":60613,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1461e1df2a054f63a27072651317b8c5","value":60613}},"680d5661b04f4beb8db7d588beff7d64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"719f8fa7c823455ba980fe44d5b197f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"745662a91275484e8c204cbe4de7812f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f44b72f57f14ea7aaec4e5682a04ac4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a9bda724b434b688d56a33ca225821f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af82a5abec7e49ae9fa0b640b26684d9","value":1}},"83f6e7e524b146329befb45ddcb47598":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8635a8297d7d451ba258f9400c812a95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: ","description_tooltip":null,"layout":"IPY_MODEL_719f8fa7c823455ba980fe44d5b197f1","max":1745,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e09a7896b464f6ea9f178809dfb602a","value":1745}},"863b82fef28a487db392150058ffd871":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89953d5228154815a292e39b4e4f0a98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89ed08bacaa545aca48b40c683ff2754":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d024701890f341b6bb3dc929a1873626","placeholder":"​","style":"IPY_MODEL_9d9c3bb754344861af4c31740436ab32","value":" 60613/60613 [36:58&lt;00:00, 27.32it/s]"}},"8aedc23beece413ca2d2553a1937925d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d8f5e009fb141e78187e44d35f48e44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f831b68e6ca482db36f7ccacbccde30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: ","description_tooltip":null,"layout":"IPY_MODEL_932a52e72ee540f7a365702713b9533d","max":7568316,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4906498dc2104672aeb25a5bfa58e8bd","value":7568316}},"8fcbb4b4be2c4d2484b73b0a59f791e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9179d2ba6ac94120a35a2afdf9dacdf4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"924e1b1ec5e441c297d17f02caa12a66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"932a52e72ee540f7a365702713b9533d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9452949bced246a3b5836672bb64b0b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9930891b9d934935b14ece78fb240b76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d8f5e009fb141e78187e44d35f48e44","placeholder":"​","style":"IPY_MODEL_83f6e7e524b146329befb45ddcb47598","value":"5774 examples [00:00, 18.06 examples/s]"}},"9a2309e171814aa99a6fd9ff2b6888a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a2ef5ce42f64adcb2cebbc744730da8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9aa72c7ad6de403b9d1af9d66bc6f9ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d9c3bb754344861af4c31740436ab32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a145c9254b5b45eea3c29200b11b533e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9ad2a0792c4480da5e0f00826fc334b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acaa992ad16b4ec1bda7bdbbb910ad77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3917508c2134e99bfe4376faab26a23","IPY_MODEL_5f7926cbead54f6999c56b188c9b3038"],"layout":"IPY_MODEL_1678f427ba3747c3ac6204c683287ee6"}},"af82a5abec7e49ae9fa0b640b26684d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b021ca029c234dc9bead398d21654da4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b46f78a1f98a49e6938ee954df3c839d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9d8632873ff4deea29f7fbfb52d484c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf6d289894854d81aa695251c985bf3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61cae5a7e9a843b282238b978baaef9e","IPY_MODEL_221fa80b0c544e4a903a5dd8f1b086f1"],"layout":"IPY_MODEL_b46f78a1f98a49e6938ee954df3c839d"}},"c35b2dc92ab34645b88bfed5517f5187":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9d8632873ff4deea29f7fbfb52d484c","placeholder":"​","style":"IPY_MODEL_ea073b6c203549a8b29dc885cc0436bc","value":" 3.88M/? [00:00&lt;00:00, 16.6MB/s]"}},"c449710c5c4247d3b84b16226856df00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e45ff997653049d0aa169477eb0ea2d9","placeholder":"​","style":"IPY_MODEL_c4840585bc4d4a9896b098954fb47690","value":" 454M/454M [00:30&lt;00:00, 15.5MB/s]"}},"c4840585bc4d4a9896b098954fb47690":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf77c57b5a724ac3b329e6157e95147d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a2309e171814aa99a6fd9ff2b6888a2","placeholder":"​","style":"IPY_MODEL_19d6fee0d4bb41caa5952462527b0cf3","value":" 725/725 [00:08&lt;00:00, 86.7B/s]"}},"cfdf21900e3b47ed9a241a275596adf1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8635a8297d7d451ba258f9400c812a95","IPY_MODEL_e3bf0ef13c3f45f2953f1ed0ee871275"],"layout":"IPY_MODEL_8fcbb4b4be2c4d2484b73b0a59f791e6"}},"d024701890f341b6bb3dc929a1873626":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3730832e76e4a57b223f00f99a92786":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_476193bb5f4a4e0da534c3cb2d2da4fd","IPY_MODEL_9930891b9d934935b14ece78fb240b76"],"layout":"IPY_MODEL_a145c9254b5b45eea3c29200b11b533e"}},"d92d6fabf8174ec7b40d734921718325":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"db1215991a8f433dac8d5262d72bd982":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcdb84db38344248821b6a2f542940aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd17cceb388142139065c455e070b170":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3917508c2134e99bfe4376faab26a23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_65f88172c01f4e7884edfca2688d9435","max":344259,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c98997be7484f779f6708e16297b00e","value":344259}},"e3bf0ef13c3f45f2953f1ed0ee871275":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eff79f1e5189420b8542e497535acdf5","placeholder":"​","style":"IPY_MODEL_9452949bced246a3b5836672bb64b0b1","value":" 4.56k/? [00:04&lt;00:00, 1.11kB/s]"}},"e45ff997653049d0aa169477eb0ea2d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7389412279248f5a288e6c3ee9a00fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_550a559e977b4dc58ab9c961b7b0fff9","placeholder":"​","style":"IPY_MODEL_9179d2ba6ac94120a35a2afdf9dacdf4","value":" 2.24k/? [00:00&lt;00:00, 2.82kB/s]"}},"e7e2670aaab747c8a93df57925d3a294":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea073b6c203549a8b29dc885cc0436bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efadd2807af14d56a43a5dcd37e38f4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9ad2a0792c4480da5e0f00826fc334b","placeholder":"​","style":"IPY_MODEL_25d58375a88f47c3939665e846184cda","value":"60407 examples [00:12, 16958.44 examples/s]"}},"eff79f1e5189420b8542e497535acdf5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f23292867a1242e98d7c404308a8bf29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4433e92fa1cd45f9b9ca8059815ddb46","IPY_MODEL_e7389412279248f5a288e6c3ee9a00fa"],"layout":"IPY_MODEL_dd17cceb388142139065c455e070b170"}},"f338007664d4406d80b96cda58220447":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f35f8171392c4367a9bb4f6d07ddd4c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: ","description_tooltip":null,"layout":"IPY_MODEL_28914da5acd440aea31de315ec453c2b","max":770480,"min":0,"orientation":"horizontal","style":"IPY_MODEL_175a62ae56354ee881db9221d95b7165","value":770480}},"f4fa398a13854ec6b85e787e69e9da5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7ed8d296c344ef98b1ef51f770c13e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f87214c9c27d4253bd1190f230764148":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"fa17d14263d34a8cbc36a8f5d6f3cf6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f831b68e6ca482db36f7ccacbccde30","IPY_MODEL_0c14b3dab4374f50b45c3ff336ad874d"],"layout":"IPY_MODEL_0b4987d3f2c943e48cdb538649d16eb4"}}}}},"nbformat":4,"nbformat_minor":0}
