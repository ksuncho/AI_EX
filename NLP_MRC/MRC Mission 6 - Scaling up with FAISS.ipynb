{"cells":[{"cell_type":"markdown","metadata":{"id":"WjDdziEN_VCt"},"source":["# FAISS 실습\n","\n","훈련은 미션 5에서 훈련해둔 인코드를 가져와서 수행\n","\n","두 가지 Corpus 에 대해서 평가\n","\n","평가하고자 하는 Corpus: korquad - 검증 데이터셋\n","\n","평가하고자 하는 Corpus: wikipedia 문서들"]},{"cell_type":"markdown","metadata":{"id":"1NWluWk3_VCu"},"source":["### Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"eGqFS4EEBF_Z"},"outputs":[{"name":"stderr","output_type":"stream","text":["������ ��θ� ã�� �� �����ϴ�.\n","������ ��θ� ã�� �� �����ϴ�.\n","������ ��θ� ã�� �� �����ϴ�.\n","������ ��θ� ã�� �� �����ϴ�.\n"]}],"source":["# > /dev/null 2>&1 # execute command in silence\n","!pip install datasets==1.4.1 > /dev/null 2>&1 # execute command in silence\n","!pip install transformers==4.4.1 > /dev/null 2>&1\n","!pip install tqdm==4.41.1 > /dev/null 2>&1\n","!pip install faiss-cpu > /dev/null 2>&1"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vcy_UzFfaxkr"},"outputs":[],"source":["import torch\n","import random\n","from pprint import pprint"]},{"cell_type":"markdown","metadata":{"id":"255WXyL_R8C4"},"source":["## Passage retrieval 준비하기"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Dg5gVKoHM_Eg"},"outputs":[],"source":["# 1. 미리 학습해둔 encoder file 다운로드\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=11XrSCVLrzN41S0ELe4Yd-NFJ21GSWmNx' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=11XrSCVLrzN41S0ELe4Yd-NFJ21GSWmNx\" -O encoders.tar.gz && rm -rf /tmp/cookies.txt\n","# 2. .tar.gz file 압축해제 \n","!tar -xf ./encoders.tar.gz"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wqZJSyAPY1tJ"},"outputs":[],"source":["from transformers import BertModel, BertPreTrainedModel, BertConfig, AutoTokenizer\n","\n","class BertEncoder(BertPreTrainedModel):\n","  def __init__(self, config):\n","    super(BertEncoder, self).__init__(config)\n","\n","    self.bert = BertModel(config)\n","    self.init_weights()\n","      \n","  def forward(self, input_ids, \n","              attention_mask=None, token_type_ids=None): \n","  \n","      outputs = self.bert(input_ids,\n","                          attention_mask=attention_mask,\n","                          token_type_ids=token_type_ids)\n","      \n","      pooled_output = outputs[1]\n","\n","      return pooled_output\n","\n","# TODO: load a model trained at Daily Mission 4\n","# use from_pretrained\n","# put your model to gpu by .cuda()\n","p_encoder = BertEncoder.from_pretrained('./p_encoder').cuda()\n","q_encoder = BertEncoder.from_pretrained('./q_encoder').cuda()\n","\n","# You can use \"bert-base-multilingual-cased\" for tokenizer, if you did not save tokenizer at Daily Mission 4.\n","model_checkpoint = \"bert-base-multilingual-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"hWfVZoLcSmOY"},"source":["Search corpus: KorQuAD validation context"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"8AOF4yiqROwK"},"outputs":[{"name":"stderr","output_type":"stream","text":["Reusing dataset squad_kor_v1 (C:\\Users\\AI_15\\.cache\\huggingface\\datasets\\squad_kor_v1\\squad_kor_v1\\1.0.0\\18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe4b4d84bb6e494f81645891fd59e3dc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["9606"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"squad_kor_v1\")\n","corpus = list(set([example['context'] for example in dataset['train']]))\n","len(corpus)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"WuP_PixqSllE"},"outputs":[{"data":{"text/plain":["960"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["search_corpus = list(set([example['context'] for example in dataset['validation']]))\n","len(search_corpus)"]},{"cell_type":"markdown","metadata":{"id":"D2y8jFLnS-N5"},"source":["Passage encoder를 활용하여 passage dense embedding 생성"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"5gc7iXXaSLtn"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a316758f73b64e2287d37a6494ba490a","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/120 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)\n","from tqdm.auto import tqdm, trange\n","\n","eval_batch_size = 8\n","\n","# Construct dataloader\n","valid_p_seqs = tokenizer(search_corpus, padding=\"max_length\", truncation=True, return_tensors='pt')\n","valid_dataset = TensorDataset(valid_p_seqs['input_ids'], valid_p_seqs['attention_mask'], valid_p_seqs['token_type_ids'])\n","valid_sampler = SequentialSampler(valid_dataset)\n","valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=eval_batch_size)\n","\n","# Inference using the passage encoder to get dense embeddeings\n","p_embs = []\n","\n","with torch.no_grad():\n","\n","  epoch_iterator = tqdm(valid_dataloader, desc=\"Iteration\", position=0, leave=True)\n","  p_encoder.eval()\n","\n","  for _, batch in enumerate(epoch_iterator):\n","    batch = tuple(t.cuda() for t in batch)\n","\n","    p_inputs = {'input_ids': batch[0],\n","                'attention_mask': batch[1],\n","                'token_type_ids': batch[2]\n","                }\n","        \n","    outputs = p_encoder(**p_inputs).to('cpu').numpy()\n","    p_embs.extend(outputs)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"MDwdcgL_uGfe"},"outputs":[{"data":{"text/plain":["(960, 768)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","p_embs = np.array(p_embs)\n","p_embs.shape  # (num_passage, emb_dim)"]},{"cell_type":"markdown","metadata":{"id":"8zBkp_wzg0fi"},"source":["Question encoder를 활용해여 question dense embedding 생성"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"WmteKZRVfcAn"},"outputs":[{"data":{"text/plain":["['대한민국 제14대 대통령으로 향년 89세를 일기로 서거한 김영삼 대통령의 묘소가 있는 곳은?',\n"," '금강산의 겨울 이름은?',\n"," '유관순 열사는 당시 어떤 종교를 믿고 있었는가?',\n"," '1997년 10월 23일, 국회 본회의 대표 연설에서 전두환, 노태우 전 대통령에 대한 사면을 촉구한 새정치 국민회의 의원은?',\n"," '셰르징거가 찾아왔다가 우연히 푸시캣 돌스에 영입된 곳은?']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["np.random.seed(1)\n","\n","sample_idx = np.random.choice(range(len(dataset['validation'])), 5)\n","query = dataset['validation'][sample_idx]['question']\n","ground_truth = dataset['validation'][sample_idx]['context']\n","\n","query"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"68khtz4ziZSF"},"outputs":[{"data":{"text/plain":["(5, 768)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["valid_q_seqs = tokenizer(query, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n","\n","with torch.no_grad():\n","  q_encoder.eval()\n","  q_embs = q_encoder(**valid_q_seqs).to('cpu').numpy()\n","\n","torch.cuda.empty_cache()\n","\n","q_embs.shape  # (num_query, emb_dim)"]},{"cell_type":"markdown","metadata":{"id":"owo71hnTSGnN"},"source":["## GPU를 활용하여 passage retrieval 수행하기"]},{"cell_type":"markdown","metadata":{"id":"iKrsJ3jOjDpf"},"source":["GPU에서 exhaustive search 수행"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"PUI0hmYaSBwS"},"outputs":[],"source":["if torch.cuda.is_available():\n","  p_embs_cuda = torch.Tensor(p_embs).to('cuda')\n","  q_embs_cuda = torch.Tensor(q_embs).to('cuda')"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"_menV6jNjM40"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[843, 241, 625,  ..., 885,  49, 127],\n","        [428, 340, 638,  ..., 557, 401, 719],\n","        [847, 339, 934,  ..., 130, 839, 406],\n","        [226, 216, 346,  ..., 279, 770, 348],\n","        [210, 917, 680,  ..., 130, 325, 760]], device='cuda:0')\n","--- 0.01795196533203125 seconds ---\n"]}],"source":["import time\n","start_time = time.time()\n","\n","dot_prod_scores = torch.matmul(q_embs_cuda, torch.transpose(p_embs_cuda, 0, 1))\n","\n","rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n","print(rank)\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"fGbwk2FjoIUm"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Search query]\n"," 대한민국 제14대 대통령으로 향년 89세를 일기로 서거한 김영삼 대통령의 묘소가 있는 곳은? \n","\n","[Ground truth passage]\n","2015년 11월 10일 건강검진 차 서울대학교 병원을 찾아 17일까지 입원한 뒤 퇴원했다. 그러다, 이틀 뒤인 19일 고열과 혈액감염 의심 증세로 서울대학교 병원에 다시 입원한 후, 11월 21일 오후에 증세가 급격히 악화됨에 따라 중환자실로 옮겨졌다. 상태가 전혀 호전되지 않던 김영삼은 결국 2015년 11월 22일 오전 0시 21분 32초에 병마를 물리치지 못하고 혈액 감염 의심으로 치료를 받던 중 향년 89세의 일기로 서거하였다. 사망에 이른 직접적인 원인은 허약한 전신 상태에 패혈증과 급성 심부전이 겹쳐 일어난 것으로 판단되었다. 장례는 대한민국 최초로 5일간 국가장으로 치뤄졌다. 이는 국장과 국민장이 통합된 이후 처음 치뤄지는 국가장이다. 이어 11월 26일 국회의사당에서 영결식이 있었고 국립서울현충원에 안장되었다. 묘소의 정확한 위치는 제3장군묘역 우측능선에 위치하고 있으며 단독 묘역이다. \n","\n","Top-1 passage with score 57.2859\n","2015년 11월 10일 건강검진 차 서울대학교 병원을 찾아 17일까지 입원한 뒤 퇴원했다. 그러다, 이틀 뒤인 19일 고열과 혈액감염 의심 증세로 서울대학교 병원에 다시 입원한 후, 11월 21일 오후에 증세가 급격히 악화됨에 따라 중환자실로 옮겨졌다. 상태가 전혀 호전되지 않던 김영삼은 결국 2015년 11월 22일 오전 0시 21분 32초에 병마를 물리치지 못하고 혈액 감염 의심으로 치료를 받던 중 향년 89세의 일기로 서거하였다. 사망에 이른 직접적인 원인은 허약한 전신 상태에 패혈증과 급성 심부전이 겹쳐 일어난 것으로 판단되었다. 장례는 대한민국 최초로 5일간 국가장으로 치뤄졌다. 이는 국장과 국민장이 통합된 이후 처음 치뤄지는 국가장이다. 이어 11월 26일 국회의사당에서 영결식이 있었고 국립서울현충원에 안장되었다. 묘소의 정확한 위치는 제3장군묘역 우측능선에 위치하고 있으며 단독 묘역이다.\n","Top-2 passage with score 56.0075\n","대통령 재임 기간 중 금융실명제(1993년)를 도입하고 차명 부정 계좌를 단속 및 처벌하였으며, 지방자치제를 전면 실시했다. 대한민국 임시정부 법통 명문화 등 역사 바로 세우기 정책의 일환으로 옛 조선총독부 건물을 폭파 철거하였고, 국군 내 사조직인 하나회를 척결했다. 또한 전두환·노태우 두 전직 대통령의 비자금을 수사하여 처벌하였고, 군사반란과 5.17 쿠데타 및 5.18 민주화 운동 진압의 책임을 물어 군사 정권 관계자들을 사법처리하였다. 1997년 외환위기로 IMF에 구제금융을 요청하였다. 2015년 11월 22일 0시 22분 서울대학교병원 중환자실에서 혈액감염 의심 증세로 치료를 받던 중 서거하였다. 서울대학교병원 측은 김영삼 전 대통령 서거 후 기자회견에서 급성 패혈증과 심부전증이 사인이라고 밝혔다. 그의 장례는 대한민국 최초로 5일 기간의 국가장으로 거행되었으며, 장지는 서울특별시 동작구의 국립서울현충원에 안장되었다.\n","Top-3 passage with score 53.6901\n","이듬해인 1968년 6월 17일, 박정희가 향토 예비군을 설치하자 그는 예비군 폐지안을 대표발의하였다. 6월 17일에 김영삼을 포함한 의원 41명은 향토예비군법 폐지안을 발표하였다. 박정희의 장기집권을 말하며 3선 개헌을 강도높게 비판하자, 공화당과 우익 인사들은 그를 좌파라며 공격했다. 그러자 김영삼 측에서는 1960년 당시 어머니가 무장공비에게 살해된 것을 들어, 만약 김영삼이 좌익이라면 무장공비들이 그의 어머니를 살해했겠느냐며 맞대응하였다. 같은 해에는 정치학도 서석재를 발탁하여 자신의 비서관으로 채용한다. 이후 서석재는 김영삼을 따랐고, 후일 1994년에는 김영삼의 리더십에 반발하는 군사 정권세력에 맞서 전직 대통령 비자금 사건을 폭로하여 군사 정권 세력을 몰락시키고, 신한국당에서 군사정권 세력을 축출, 타도하는데 기여한다.\n","Top-4 passage with score 53.5542\n","대학 2학년에 재학 중 김영삼은 서울특별시 명동 시공관에서 있은 정부수립기념 웅변대회에 참가하여 2등을 차지, 외무부 장관상을 수상했다. 당시 외무부 장관은 장택상이었다. 김영삼에게는 장택상이 처음 직접 대하는 거물 정치인이었고 장택상은 곱상하고 총명한 청년을 마음에 두게 되었다. 2대 민의원 선거 무렵 장택상과 가까워지게 되었다. 제2대 민의원 선거에 출마할 결심을 한 장택상은 서울대 문리대 교정으로 지프차를 보내 김영삼에게 선거 도움을 요청해 왔다. 이에 김영삼은 서울대 학생 20여 명과 함께 경상북도 칠곡으로 내려가 이때부터 40여 일간을 장택상과 침식을 함께 하며 찬조연설을 하는 등 장택상의 당선을 위해 노력하였다. 김영삼의 서울대학교 졸업에 대해선 이견이 존재했다. 1947년 청강생 명부에 김영삼이 있어서 정식 입학생이 아니라는 소문이 돌았지만, 서울대학교에서는 당시의 청강생은 이후와 달리 입학하는 6가지 방법 중 하나였으며, 실력고사를 통과하거나 학업성적이 우수하면 정식 학생이 되어 졸업하였다. 김영삼의 생가에는 김영삼이 1947년 5월에 서울대학교에 입학하여 1951년 2월에 졸업했음을 보여주는 졸업증명서가 전시되었다.\n","Top-5 passage with score 53.1381\n","1980년 봄, 그는 김대중, 김종필 등과 대권을 놓고 경쟁하였다. 그는 전두환과 신군부의 쿠데타를 그리 걱정하지 않았고, 결국 5·17 쿠데타로 좌절되었다. 1980년 5월 17일 오전 10시, 김영삼은 신군부 군인들에 의해 가택연금을 당했다. 김영삼은 5월 20일 상도동 자택에서 5.17 비상계엄 확대 조치를 내린 신군부를 규탄하는 기자회견을 열었다. 그는 \"오늘 계엄통치를 확대 강화한 5 ·17 사태를, 민주회복이라는 국민적 목표를 배신한 폭거로 규정한다. 계엄당국의 강압통치로 빚어진 유혈사태는 이 나라를 파국으로 몰아가고있다.\"면서 '국민적 목표를 배신한 5·17 폭거'라는 기자회견문을 발표했다. 이로 인해 김영삼은 신군부에 의해 5월 20일부터 가택연금 상태에 놓이게 되었다.\n","\n","\n"]}],"source":["k = 5 \n","\n","for i, q in enumerate(query[:1]):\n","  print(\"[Search query]\\n\", q, \"\\n\")\n","  print(\"[Ground truth passage]\")\n","  print(ground_truth[i], \"\\n\")\n","\n","  r = rank[i]\n","  for j in range(k):\n","    print(\"Top-%d passage with score %.4f\" % (j+1, dot_prod_scores[i][r[j]]))\n","    print(search_corpus[r[j]])\n","  print('\\n')"]},{"cell_type":"markdown","metadata":{"id":"XKGyIdQHkn-b"},"source":["## FAISS를 활용하여 CPU에서 passage retrieval 수행하기\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7PZH2Bu2kx4x"},"source":["FAISS SQ8, IVF 를 활용해서 cpu에서 passage retrieval 실습해보기"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"REChmwRnkw5P"},"outputs":[],"source":["import faiss\n","\n","num_clusters = 16\n","niter = 5\n","k = 5\n","\n","# 1. Clustering\n","emb_dim = p_embs.shape[-1]\n","index_flat = faiss.IndexFlatL2(emb_dim)\n","\n","clus = faiss.Clustering(emb_dim, num_clusters)\n","clus.verbose = True\n","clus.niter = niter\n","clus.train(p_embs, index_flat)\n","centroids = faiss.vector_float_to_array(clus.centroids)\n","centroids = centroids.reshape(num_clusters, emb_dim)\n","\n","quantizer = faiss.IndexFlatL2(emb_dim)\n","quantizer.add(centroids)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"GEiiBQLWkbJv"},"outputs":[],"source":["# 2. SQ8 + IVF indexer (IndexIVFScalarQuantizer)\n","indexer = faiss.IndexIVFScalarQuantizer(quantizer, quantizer.d, quantizer.ntotal, faiss.METRIC_L2)\n","indexer.train(p_embs)\n","indexer.add(p_embs)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"5tHmCilLnpO7"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- 0.0009968280792236328 seconds ---\n"]}],"source":["# 3. Search using indexer\n","\n","start_time = time.time()\n","D, I = indexer.search(q_embs, k)\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"baaWjLQlponb"},"outputs":[{"name":"stdout","output_type":"stream","text":["=======[Distance]=======\n","[[162.76018 166.36844 170.21155 172.91968 173.79454]\n"," [139.74776 142.22607 142.95209 151.57596 153.51622]\n"," [134.44823 137.05942 137.79669 138.13394 138.43002]\n"," [130.65106 130.69214 130.84744 134.1312  135.26862]\n"," [209.12695 209.20168 210.0505  212.58987 212.85425]]\n","\n","\n","=======[Index of Top-5 Passages]=======\n","[[882 742 507 619 104]\n"," [428 882 340 213 570]\n"," [ 12 728 865 720 591]\n"," [216 346 203 226  64]\n"," [480 279 629 657 641]]\n"]}],"source":["print('=======[Distance]=======')\n","print(D)\n","print('\\n')\n","print('=======[Index of Top-5 Passages]=======')\n","print(I)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"uGBAAIkWp4me"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Search query]\n"," 대한민국 제14대 대통령으로 향년 89세를 일기로 서거한 김영삼 대통령의 묘소가 있는 곳은? \n","\n","[Ground truth passage]\n","2015년 11월 10일 건강검진 차 서울대학교 병원을 찾아 17일까지 입원한 뒤 퇴원했다. 그러다, 이틀 뒤인 19일 고열과 혈액감염 의심 증세로 서울대학교 병원에 다시 입원한 후, 11월 21일 오후에 증세가 급격히 악화됨에 따라 중환자실로 옮겨졌다. 상태가 전혀 호전되지 않던 김영삼은 결국 2015년 11월 22일 오전 0시 21분 32초에 병마를 물리치지 못하고 혈액 감염 의심으로 치료를 받던 중 향년 89세의 일기로 서거하였다. 사망에 이른 직접적인 원인은 허약한 전신 상태에 패혈증과 급성 심부전이 겹쳐 일어난 것으로 판단되었다. 장례는 대한민국 최초로 5일간 국가장으로 치뤄졌다. 이는 국장과 국민장이 통합된 이후 처음 치뤄지는 국가장이다. 이어 11월 26일 국회의사당에서 영결식이 있었고 국립서울현충원에 안장되었다. 묘소의 정확한 위치는 제3장군묘역 우측능선에 위치하고 있으며 단독 묘역이다. \n","\n","Top-1 passage with distance 162.7602\n","이 지역은 대해 때문에 세상에 알려진 지 300년 밖에 되지 않았는데, 숙종 24년(1698년) 고성 군수로 있던 남택하(南宅夏)가 찾아내고 “금강산의 얼굴빛과 같다.” 하여 해금강이라 이름 붙였다. 본래 해안 암벽, 바위섬, 자연호, 모래사장, 하천이 어우러진 경승지다. 이중 개방된 곳은 삼일포와 향로봉이며, 관동 팔경의 하나인 총석정은 개방되어 있지 않다. 삼일포는 남한의 화진포와 송지호같이 석호(潟湖)이며, 총 넓이는 0.79km에 달한다. 이 호수에는 전설에 따르면 신선 또는 화랑들이 경치가 너무 좋아 3일 동안 머물고 갔기 때문에 삼일포라 한다. 봉래대에서 삼일포 전경을 볼 수 있다. 소가 누운 모양이라고 해서 와우섬이라 이름 붙은 큰 섬을 비롯해, 3개의 작은 섬이 떠있다. 또한 삼일포 기슭에는 4명의 신선이 놀고 간 것을 기념해 세웠다는 사선정터가 있다. 향로봉은 바다의 해만물상이라 불리며, 바닷가에 육지와는 거리를 두고 홀로 솟아 있는 봉우리이다. 비바람에 씻기고 바닷물에 깎이어 독특한 모양을 지니고 있다.\n","Top-2 passage with distance 166.3684\n","1932년 조선주택영단은 여러가지 주택 유형을 연구하기 위하여 대한민국에 여러 대안을 실현하게 된다. 그 중 하나가 르꼬르 뷔제의 대안이기도 했으며, 일전에 산업혁명에서 양산화된 아팔타멘토의 건축이 하나 둘씩 대안으로써 쓰였는데 그것이 바로 충정로에 남아있는 6층짜리 충정아파트이다. 초창기 건물소유주 토요다(豊田)씨의 이름을 따 한국식 발음인 풍전아파트로 개칭되었으나 혼용하다가 유림아파트로 바뀐다. 6.25 전쟁당시 인민군 재판소로 활용되어 지하실에서 처형소로도 쓰이고 그 뒤 유엔군호텔로 매입되었다가 아파트로 용도를 변경하였다. 1979년 도로확장으로 인해 절반이 철거되고 지금의 충정로 아파트로 보수를 거쳐 지금도 남아있다. 서울특별시청에서 서울속 미래유산에 선정되어 단계적 매입에 들어가고 있으며 문화공간으로 활용할 계획이다. 또한 미쿠니아파트가 관사로 지어진것에 반해 충정아파트는 대한민국 첫 임대아파트이다. 또한 시설도 처음으로 아파트에 맞는 형식으로 건설되어서 많은 사람들이 충정아파트가 국내 최초의 아파트라고 한다.\n","Top-3 passage with distance 170.2115\n","반포주공아파트는 1973년 대한주택공사가 건설한 대한민국 최초의 주공아파트 대단지이다. 22평~62평 3786가구로 지은 대단지로서 주공아파트가 강남과 더불어 대한민국을 \"아파트공화국\"으로 만드는데 큰 역할을 하였다. 또한 국내 최초로 복층 설계를 도입하여 6층이나 1,3,5층만 현관을 설치하였고 나머지 2,4,6층은 실내에 설치한 계단을 통하여 올라갈수 있다. 또한 지역난방시설도 설치한 최신형 아파트였다. 이후 반포주공아파트는 1단지에 이어 2단지와 3단지까지 건축하여 분양하였다. 1단지 이후에 지어졌던 반포주공 2단지와 3단지는 현재 재개발로 인하여 철거되었으나 1단지는 재개발을 기다리고 있는 실정이다. 서울시는 국내 최초의 주공아파트라는 점에서 반포주공아파트 1개동을 마을 박물관으로 조성한다는 계획을 밝혔다.\n","Top-4 passage with distance 172.9197\n","동대문아파트는 1965년에 완공된 7층짜리 중앙정원형 아파트이다. 대한주택공사가 지은 아파트로 초기에는 연예인들이 많이 살아 \"연예인아파트\"라는 별명도 있었다. 50년전만 해도 고급아파트였던 동대문아파트는 국내 최초로 중앙정원 방식을 적용했다. 이 방식은 지붕이 없는 형식으로 이후 대한주택공사의 건축의 기본이 된다. 고급아파트를 바로 이때부터 짓기 시작하여 이후에는 정부가 지원하는 외인아파트 시공도 담당한다. 또한 아파트의 고급화를 유지하기 위해 주민들이 물건을 복도에 내놓는 것을 철저히 경비실에서 금지하기도 했다. 1993년 안전진단에서 C등급을 이후 그 뒤로도 계속 중앙정원 굴뚝 부분이 전도될 위험이 있다는 우려가 있었으나 후원기업을 찾아 보강공사를 하여 현재는 B등급으로 남아있다. 서울특별시에 현존하는 아파트 중에 2번째로 오래된 동대문아파트는 충정아파트와 함께 서울속 미래유산 1000선으로 선정하려 하였으나 예산 부족으로 무산되고, 창신동 뉴타운으로 지정하려 하였으나, 이것 역시 무산되었다. 이후 서울특별시는 동대문아파트를 다시 미래유산 후보로 올려 미래유산 아파트로 선정되었다.\n","Top-5 passage with distance 173.7945\n","개포주공아파트는 1981년 현대건설이 지은 주공아파트이다. 또한 전두환 정권이 도입하였던 택지개발촉진법의 첫 사례이기도 하다. 1980년대 당시 강남개발로 인해 사람들이 강남에 몰리자 정부는 주택난을 해소하기 위하여 주공아파트를 여러 곳에 짓기로 결심한다. 이곳 중 잘 알려진 곳이 개포주공아파트와 개포시영아파트이다. 개포주공아파트는 1단지에서 7단지까지 지어졌으며 현재 재개발에 들어갈 예정이다. 서울시는 이전부터 재개발계획이 있었으나 여러 문제로 인해 미루어지다 최근 들어서 조합이 설립되는 등 재개발시행에 본격적으로 들어갔다. 하지만 서울시의 층수제한에 많은 논란이 일면서 갈등이 있었으나 현재는 용적률 300%로 동의가 된 상태로 이제 최종허가와 보상만 하면 되는 실정이다.\n","\n","\n"]}],"source":["for i, q in enumerate(query[:1]):\n","  print(\"[Search query]\\n\", q, \"\\n\")\n","  print(\"[Ground truth passage]\")\n","  print(ground_truth[i], \"\\n\")\n","\n","  d = D[i]\n","  i = I[i]\n","  for j in range(k):\n","    print(\"Top-%d passage with distance %.4f\" % (j+1, d[j]))\n","    print(search_corpus[i[j]])\n","  print('\\n')"]},{"cell_type":"markdown","metadata":{"id":"P2Hx3KBMO8z_"},"source":["## [과제] Wikipedia documents에 대해 FAISS retrieval 실습해보기"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"irAqHB5_PlCm"},"outputs":[{"name":"stderr","output_type":"stream","text":["Cannot open cookies file '/tmp/cookies.txt': No such file or directory\n","--2022-07-28 14:25:42--  https://docs.google.com/uc?export=download&confirm=$(wget%20--quiet%20--save-cookies%20/tmp/cookies.txt%20--keep-session-cookies%20--no-check-certificate%20'https://docs.google.com/uc?export=download&id=1eD57pCCvhiXLrHBb1j07QX1VQQDogejF'%20-O-%20%7C%20sed%20-rn%20's/.*confirm=([0-9A-Za-z_]+).*/%5C1%5Cn/p')&id=1eD57pCCvhiXLrHBb1j07QX1VQQDogejF\n","Resolving docs.google.com (docs.google.com)... 172.217.161.238\n","Connecting to docs.google.com (docs.google.com)|172.217.161.238|:443... connected.\n","HTTP request sent, awaiting response... 404 Not Found\n","2022-07-28 14:25:42 ERROR 404: Not Found.\n","\n"]}],"source":["# 3. 미리 올려둔 wiki docs 다운로드\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1eD57pCCvhiXLrHBb1j07QX1VQQDogejF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1eD57pCCvhiXLrHBb1j07QX1VQQDogejF\" -O wikipedia_documents.json && rm -rf /tmp/cookies.txt"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"G1SMReLpWsTr"},"outputs":[],"source":["# First load wikipedia dump\n","import json\n","\n","dump_path = \"./wikipedia_documents.json\"\n","with open(dump_path, 'r', encoding='UTF-8') as f:\n","    wiki = json.load(f)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"47i7O_poum_2"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6caa0ea43ec940b5b4914adc5030ce22","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/60613 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["C:\\Users\\AI_15\\AppData\\Local\\Temp\\ipykernel_20016\\1027532420.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n","  wiki_embs = torch.Tensor(wiki_embs).squeeze()  # (num_passage, emb_dim)\n"]}],"source":["# TODO: Generate a dense embedding of wiki documents from wikipedia_documents.json\n","from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)\n","from tqdm.auto import tqdm, trange\n","\n","eval_batch_size = 8\n","# Construt dataloader\n","corpus = [document['text'] for document_id, document in wiki.items()]\n","# Inference using the passage encoder to get dense embeddings\n","\n","# wiki_embs = dense embedding of wiki documents\n","with torch.no_grad():\n","  wiki_embs = []\n","  for text in tqdm(corpus):\n","      p = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n","      wiki_emb = p_encoder(**p).to('cpu').numpy() # use p_encoder\n","      wiki_embs.append(wiki_emb)\n","\n","wiki_embs = torch.Tensor(wiki_embs).squeeze()  # (num_passage, emb_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lX-O4z28d6HN"},"outputs":[],"source":["# download wiki_p_embs \n","import torch\n","# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1uQ-k6lv86KXK0kRgsJw5qvFQg93s6GeI' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1uQ-k6lv86KXK0kRgsJw5qvFQg93s6GeI\" -O wiki_p_embs.pt && rm -rf /tmp/cookies.txt\n","wiki_embs = torch.load(\"wiki_p_embs.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1628823917225,"user":{"displayName":"JinUk Cho","photoUrl":"","userId":"10000090644219567406"},"user_tz":-540},"id":"Ov7Nm8DcvE7e","outputId":"7f088077-4b72-4c63-e857-da85ecef1b00"},"outputs":[{"data":{"text/plain":["torch.Size([60613, 768])"]},"execution_count":3,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# Answer: (60613, 768) (num_doc, hidden_dim)\n","wiki_embs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52ySj_G-wdo4"},"outputs":[],"source":["# TODO: build faiss indexer using your own parameters\n","import faiss\n","\n","num_clusters = ...\n","niter = ...\n","k = 5\n","\n","# 1. Clustering\n","emb_dim = wiki_embs.shape[-1]\n","index_flat = faiss.IndexFlatL2(emb_dim)\n","\n","clus = faiss.Clustering(emb_dim, num_clusters)\n","clus.verbose = True\n","clus.niter = niter\n","clus.train(wiki_embs, index_flat)\n","centroids = faiss.vector_float_to_array(clus.centroids)\n","centroids = centroids.reshape(num_clusters, emb_dim)\n","\n","quantizer = faiss.IndexFlatL2(emb_dim)\n","quantizer.add(centroids)\n","\n","# 2. SQ8 + IVF indexer (IndexIVFScalarQuantizer)\n","indexer = faiss.IndexIVFScalarQuantizer(quantizer, quantizer.d, quantizer.ntotal, faiss.METRIC_L2)\n","indexer.train(wiki_embs)\n","indexer.add(wiki_embs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXVKjrCXvSW0"},"outputs":[],"source":["# TODO: transform query to dense vector\n","query = '대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?'\n","\n","valid_q_seqs = ...\n","\n","with torch.no_grad():\n","  q_encoder.eval()\n","  q_embs = ...\n","  \n","q_embs.shape  # (num_query, emb_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtdvW3JPw7kn"},"outputs":[],"source":["# 3. Search using indexer\n","\n","start_time = time.time()\n","D, I = indexer.search(q_embs, k)\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-6layOyw7ko"},"outputs":[],"source":["print('=======[Distance]=======')\n","print(D)\n","print('\\n')\n","print('=======[Index of Top-5 Passages]=======')\n","print(I)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HycrRnySw7ko"},"outputs":[],"source":["print(\"[Search query]\\n\", q, \"\\n\")\n","# 주의: 우리는 wiki 문서 전체에서 임의의 질문을 물어보는 것이므로 ground truth 가 존재하지 않습니다\n","\n","d = D[0]\n","i = I[0]\n","for j in range(k):\n","    print(\"Top-%d passage with distance %.4f\" % (j+1, d[j]))\n","    print(search_corpus[i[j]])\n","    print('\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"MRC Mission 6 - Scaling up with FAISS.ipynb","provenance":[{"file_id":"15agT-smXbo9w0Un1QBfUvmRN_SOPxylJ","timestamp":1619890035444},{"file_id":"1c9Vr7z_LBG2l9K4lVb40pu7Kk22hXQCp","timestamp":1614240569955},{"file_id":"1Q7iAXm_kwF_NHfOEGdViMCiPHnqoZlXe","timestamp":1613491158162}]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"}}},"nbformat":4,"nbformat_minor":0}
