{"cells":[{"cell_type":"markdown","metadata":{"id":"Gp-fkNdjA-03"},"source":["# AI 전문가 교육과정 실습 2 - part Final\n","\n","***\n","### NLP응용: 토픽 추출\n","Applied Natrual Language Processing: Topic Modeling\n","\n","강사: 차미영 교수 (카이스트 전산학부)    \n","조교: 신민기, 정현규 (카이스트 전산학부)\n","\n","# LDA on Korean Data"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27987,"status":"ok","timestamp":1656947428629,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"7VJBYlQjMFz6","outputId":"32890bbf-0ee5-4452-d8c9-57a9782f4b4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyLDAvis==3.2.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.2.2)\n","Requirement already satisfied: wheel>=0.23.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (0.37.1)\n","Requirement already satisfied: numpy>=1.9.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.23.0)\n","Requirement already satisfied: scipy>=0.18.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.8.1)\n","Requirement already satisfied: joblib>=0.8.4 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.1.0)\n","Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (3.1.2)\n","Requirement already satisfied: numexpr in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (2.8.3)\n","Requirement already satisfied: future in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (0.18.2)\n","Requirement already satisfied: funcy in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.17)\n","Requirement already satisfied: pandas>=0.17.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.4.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis==3.2.2) (2.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2022.1)\n","Requirement already satisfied: packaging in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from numexpr->pyLDAvis==3.2.2) (21.3)\n","Requirement already satisfied: six>=1.5 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.17.0->pyLDAvis==3.2.2) (1.16.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->numexpr->pyLDAvis==3.2.2) (3.0.9)\n","Requirement already satisfied: konlpy in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from konlpy) (1.4.0)\n","Requirement already satisfied: lxml>=4.1.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from konlpy) (4.9.1)\n","Requirement already satisfied: numpy>=1.6 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from konlpy) (1.23.0)\n"]}],"source":["!pip install pyLDAvis==3.2.2\n","!pip install konlpy "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192214,"status":"ok","timestamp":1656947620820,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"zR9ObV64NzUe","outputId":"35ae59bc-a1f4-44ac-beeb-e87b0b8ac70c"},"outputs":[{"name":"stderr","output_type":"stream","text":["'apt-get'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n","��ġ ������ �ƴմϴ�.\n","'apt-get'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n","��ġ ������ �ƴմϴ�.\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: konlpy in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.6.0)\n","Collecting JPype1-py3\n","  Using cached JPype1-py3-0.5.5.4.tar.gz (88 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'error'\n"]},{"name":"stderr","output_type":"stream","text":["  error: subprocess-exited-with-error\n","  \n","  × python setup.py egg_info did not run successfully.\n","  │ exit code: 1\n","  ╰─> [37 lines of output]\n","      \n","      ********* DEPRECATION WARNING *********\n","      Warning: This version of JPype is now deprecated, see issue #29 for more details\n","      Warning:     https://github.com/tcalmant/jpype-py3/issues/29\n","      Warning:\n","      Warning: Please use this version instead:\n","      Warning:     https://github.com/jpype-project/jpype\n","      Warning:\n","      Warning: It can be installed using:\n","      Warning:     pip install JPype1\n","      ********* DEPRECATION WARNING *********\n","      \n","      Traceback (most recent call last):\n","        File \"C:\\Users\\AI_15\\AppData\\Local\\Temp\\pip-install-a7ihv65v\\jpype1-py3_90055769632145df87a93e5951694e72\\setup.py\", line 483, in <module>\n","          config = WindowsJDKFinder()\n","        File \"C:\\Users\\AI_15\\AppData\\Local\\Temp\\pip-install-a7ihv65v\\jpype1-py3_90055769632145df87a93e5951694e72\\setup.py\", line 206, in __init__\n","          java_home = self.find_jdk_home()\n","        File \"C:\\Users\\AI_15\\AppData\\Local\\Temp\\pip-install-a7ihv65v\\jpype1-py3_90055769632145df87a93e5951694e72\\setup.py\", line 254, in find_jdk_home\n","          raise NoJDKError(visited_folders)\n","      __main__.NoJDKError: No JDK found\n","      \n","      During handling of the above exception, another exception occurred:\n","      \n","      Traceback (most recent call last):\n","        File \"<string>\", line 2, in <module>\n","        File \"<pip-setuptools-caller>\", line 34, in <module>\n","        File \"C:\\Users\\AI_15\\AppData\\Local\\Temp\\pip-install-a7ihv65v\\jpype1-py3_90055769632145df87a93e5951694e72\\setup.py\", line 495, in <module>\n","          raise RuntimeError(\n","      RuntimeError: No Java/JDK could be found. I looked in the following directories:\n","      \n","      \n","      \n","      Please check that you have it installed.\n","      \n","      If you have and the destination is not in the above list, please find out where your java's home is, set your JAVA_HOME environment variable to that path and retry the installation.\n","      If this still fails please open a ticket or create a pull request with a fix on github:\n","      https://github.com/tcalmant/jpype/\n","      [end of output]\n","  \n","  note: This error originates from a subprocess, and is likely not a problem with pip.\n","error: metadata-generation-failed\n","\n","× Encountered error while generating package metadata.\n","╰─> See above for output.\n","\n","note: This is an issue with the package mentioned above, not pip.\n","hint: See above for details.\n","������ ������ ã�� �� �����ϴ�.\n"]}],"source":["!apt-get update\n","!apt-get install g++ openjdk-8-jdk \n","!pip3 install konlpy JPype1-py3\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1589,"status":"ok","timestamp":1656947622392,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"2zexZe0jL5Qt","outputId":"af3beb99-6835-495a-a630-e62d1270f114"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n","  from imp import reload\n"]}],"source":["import glob\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import re\n","##from konlpy.tag import Mecab\n","from konlpy.tag import Okt\n","from wordcloud import WordCloud\n","\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","from pprint import pprint\n","\n","import pyLDAvis\n","import pyLDAvis.gensim \n","%matplotlib inline\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('64bit', 'WindowsPE')\n"]}],"source":["from konlpy.tag import Okt\n","#okt = Okt()\n","import platform\n","print(platform.architecture())\n","#print(okt.pos(u'이것도 되나욬?ㅋㅋ', norm=True, stem=True))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1689,"status":"ok","timestamp":1656947624070,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"4vnXb_fTL-fC","outputId":"df6899c8-0e40-433e-dcf0-1b4525540f2f"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\AI_15\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\AI_15\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\AI_15\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"elapsed":163580,"status":"error","timestamp":1656947787622,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"IJWAURXtMbLn","outputId":"dc334ad3-cab3-440c-d11e-902e6ce59f1b"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":64,"status":"aborted","timestamp":1656947787591,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"0_x38ubtMgqf"},"outputs":[],"source":["#%cd drive/My\\ Drive"]},{"cell_type":"markdown","metadata":{"id":"SxnpzMVWL5Qx"},"source":["# Data load"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":65,"status":"aborted","timestamp":1656947787593,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"Sw0v4Yx3L5Qx"},"outputs":[],"source":["file_list = glob.glob(\"./data/*.json\")"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":66,"status":"aborted","timestamp":1656947787595,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"1SRvAs9WL5Qy"},"outputs":[],"source":["# total_df = None\n","# first = True\n","# for file_name in file_list:\n","#     print(file_name)\n","#     if first:   \n","#         total_df = pd.read_json(file_name, lines=True)\n","#         first = False\n","#     else:\n","#         temp = pd.read_json(file_name, lines=True)\n","#         total_df = pd.concat([total_df, temp])\n","# total_df.reset_index(inplace = True)\n","# original_df = total_df.copy()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":66,"status":"aborted","timestamp":1656947787598,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"qLkF92OgL5Qz"},"outputs":[],"source":["total_df = pd.read_json(file_list[0], lines=True)\n","total_df.reset_index(inplace = True)\n","original_df = total_df.copy()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":64,"status":"aborted","timestamp":1656947787601,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"dPGRklFBL5Qz"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>id</th>\n","      <th>conversation_id</th>\n","      <th>created_at</th>\n","      <th>date</th>\n","      <th>time</th>\n","      <th>timezone</th>\n","      <th>user_id</th>\n","      <th>username</th>\n","      <th>name</th>\n","      <th>...</th>\n","      <th>geo</th>\n","      <th>source</th>\n","      <th>user_rt_id</th>\n","      <th>user_rt</th>\n","      <th>retweet_id</th>\n","      <th>reply_to</th>\n","      <th>retweet_date</th>\n","      <th>translate</th>\n","      <th>trans_src</th>\n","      <th>trans_dest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1241514909659086849</td>\n","      <td>1241514909659086848</td>\n","      <td>2020-03-21 23:59:55</td>\n","      <td>2020-03-22</td>\n","      <td>08:59:55</td>\n","      <td>¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA</td>\n","      <td>1239857975398817792</td>\n","      <td>ch10v3</td>\n","      <td>천사님 모시는 ⑧</td>\n","      <td>...</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>[{'user_id': '1239857975398817792', 'username'...</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1241514884665184257</td>\n","      <td>1241514884665184256</td>\n","      <td>2020-03-21 23:59:49</td>\n","      <td>2020-03-22</td>\n","      <td>08:59:49</td>\n","      <td>¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA</td>\n","      <td>158630288</td>\n","      <td>seaok</td>\n","      <td>GOD BLESS YOU</td>\n","      <td>...</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>[{'user_id': '158630288', 'username': 'seaok'}]</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1241514882362675204</td>\n","      <td>1241475915298766848</td>\n","      <td>2020-03-21 23:59:48</td>\n","      <td>2020-03-22</td>\n","      <td>08:59:48</td>\n","      <td>¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA</td>\n","      <td>1138535280380719104</td>\n","      <td>lenallurekr</td>\n","      <td>레나</td>\n","      <td>...</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>[{'user_id': '1138535280380719104', 'username'...</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 35 columns</p>\n","</div>"],"text/plain":["   index                   id      conversation_id          created_at  \\\n","0      0  1241514909659086849  1241514909659086848 2020-03-21 23:59:55   \n","1      1  1241514884665184257  1241514884665184256 2020-03-21 23:59:49   \n","2      2  1241514882362675204  1241475915298766848 2020-03-21 23:59:48   \n","\n","        date      time                timezone              user_id  \\\n","0 2020-03-22  08:59:55  ¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA  1239857975398817792   \n","1 2020-03-22  08:59:49  ¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA            158630288   \n","2 2020-03-22  08:59:48  ¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA  1138535280380719104   \n","\n","      username           name  ... geo source user_rt_id user_rt retweet_id  \\\n","0       ch10v3      천사님 모시는 ⑧  ...                                            \n","1        seaok  GOD BLESS YOU  ...                                            \n","2  lenallurekr             레나  ...                                            \n","\n","                                            reply_to  retweet_date  translate  \\\n","0  [{'user_id': '1239857975398817792', 'username'...                            \n","1    [{'user_id': '158630288', 'username': 'seaok'}]                            \n","2  [{'user_id': '1138535280380719104', 'username'...                            \n","\n","  trans_src trans_dest  \n","0                       \n","1                       \n","2                       \n","\n","[3 rows x 35 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["total_df.head(3)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":64,"status":"aborted","timestamp":1656947787602,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"PhngsdXaL5Q0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>id</th>\n","      <th>conversation_id</th>\n","      <th>user_id</th>\n","      <th>replies_count</th>\n","      <th>retweets_count</th>\n","      <th>likes_count</th>\n","      <th>video</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>20069.000000</td>\n","      <td>2.006900e+04</td>\n","      <td>2.006900e+04</td>\n","      <td>2.006900e+04</td>\n","      <td>20069.000000</td>\n","      <td>20069.00000</td>\n","      <td>20069.000000</td>\n","      <td>20069.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>10034.000000</td>\n","      <td>1.241306e+18</td>\n","      <td>1.241103e+18</td>\n","      <td>6.051826e+17</td>\n","      <td>0.467039</td>\n","      <td>8.06094</td>\n","      <td>5.688774</td>\n","      <td>0.010015</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>5793.565612</td>\n","      <td>8.862836e+13</td>\n","      <td>4.615141e+15</td>\n","      <td>5.493509e+17</td>\n","      <td>2.448691</td>\n","      <td>138.42203</td>\n","      <td>61.655466</td>\n","      <td>0.099577</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>1.241153e+18</td>\n","      <td>9.818432e+17</td>\n","      <td>8.150500e+05</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>5017.000000</td>\n","      <td>1.241233e+18</td>\n","      <td>1.241219e+18</td>\n","      <td>4.891601e+08</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>10034.000000</td>\n","      <td>1.241302e+18</td>\n","      <td>1.241290e+18</td>\n","      <td>8.412471e+17</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>15051.000000</td>\n","      <td>1.241370e+18</td>\n","      <td>1.241361e+18</td>\n","      <td>1.154752e+18</td>\n","      <td>1.000000</td>\n","      <td>0.00000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>20068.000000</td>\n","      <td>1.241515e+18</td>\n","      <td>1.241515e+18</td>\n","      <td>1.241490e+18</td>\n","      <td>144.000000</td>\n","      <td>10116.00000</td>\n","      <td>3266.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              index            id  conversation_id       user_id  \\\n","count  20069.000000  2.006900e+04     2.006900e+04  2.006900e+04   \n","mean   10034.000000  1.241306e+18     1.241103e+18  6.051826e+17   \n","std     5793.565612  8.862836e+13     4.615141e+15  5.493509e+17   \n","min        0.000000  1.241153e+18     9.818432e+17  8.150500e+05   \n","25%     5017.000000  1.241233e+18     1.241219e+18  4.891601e+08   \n","50%    10034.000000  1.241302e+18     1.241290e+18  8.412471e+17   \n","75%    15051.000000  1.241370e+18     1.241361e+18  1.154752e+18   \n","max    20068.000000  1.241515e+18     1.241515e+18  1.241490e+18   \n","\n","       replies_count  retweets_count   likes_count         video  \n","count   20069.000000     20069.00000  20069.000000  20069.000000  \n","mean        0.467039         8.06094      5.688774      0.010015  \n","std         2.448691       138.42203     61.655466      0.099577  \n","min         0.000000         0.00000      0.000000      0.000000  \n","25%         0.000000         0.00000      0.000000      0.000000  \n","50%         0.000000         0.00000      0.000000      0.000000  \n","75%         1.000000         0.00000      1.000000      0.000000  \n","max       144.000000     10116.00000   3266.000000      1.000000  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["total_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"x1_ouHkmL5Q1"},"source":["# Pre-processing function"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":64,"status":"aborted","timestamp":1656947787603,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"UMjT93lyL5Q1"},"outputs":[],"source":["# Basic Cleaning Text Function\n","def CleanText(readData):\n","\n","    # Remove Retweets \n","    text = re.sub('RT @[\\w_]+: ', '', readData)\n","\n","    # Remove Mentions\n","    text = re.sub('@[\\w_]+', '', text)\n","\n","    # Remove or Replace URL \n","    # text = url_re.sub('URL', text)\n","    text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", ' ', text) # start with http\n","    text = re.sub(r\"[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{2,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", ' ', text) # Don't start with http\n","\n","    # Remove Hashtag\n","    text = re.sub('[#]+[0-9a-zA-Z_]+', ' ', text)\n","\n","    # Remove Garbage Words (ex. &lt, &gt, etc)\n","    text = re.sub('[&]+[a-z]+', ' ', text)\n","\n","    # Remove Special Characters\n","    text = re.sub('[^0-9a-zA-Zㄱ-ㅎ가-힣]', ' ', text)\n","\n","    # Remove Numbers (If you want, activate the code)\n","    text = re.sub(r'\\d+',' ',text)\n","\n","    # Remove English (If you want, activate the code)\n","    text = re.sub('[a-zA-Z]' , ' ', text)\n","\n","    # Remove newline\n","    text = text.replace('\\n',' ')\n","\n","    # Remove multi spacing & Reform sentence\n","    text = ' '.join(text.split())\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocessing_okt(readData):\n","    #### Clean text\n","    sentence = CleanText(readData)\n","\n","    #### Tokenize\n","    morphs = Okt.pos(sentence)\n","\n","    JOSA = [\"Josa\"]\n","    SIGN = [\"SF\", \"SE\", \"SSO\", \"SSC\", \"SC\", \"SY\"]\n","    TERMINATION = [\"EP\", \"EF\", \"EC\", \"ETN\", \"ETM\"] # 어미\n","    SUPPORT_VERB = [\"VX\"]\n","    NUMBER = [\"SN\"]\n","\n","    # Remove JOSA, EOMI, etc\n","    morphs[:] = (morph for morph in morphs if morph[1] not in JOSA+SIGN+TERMINATION+SUPPORT_VERB)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":60,"status":"aborted","timestamp":1656947787604,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"WjCi6KqYL5Q2"},"outputs":[],"source":["def preprocessing_mecab(readData):\n","    #### Clean text\n","    sentence = CleanText(readData)\n","\n","    #### Tokenize\n","    morphs = mecab.pos(sentence)\n","\n","    JOSA = [\"JKS\", \"JKC\", \"JKG\", \"JKO\", \"JKB\", \"JKV\", \"JKQ\", \"JX\", \"JC\"]\n","    SIGN = [\"SF\", \"SE\", \"SSO\", \"SSC\", \"SC\", \"SY\"]\n","    TERMINATION = [\"EP\", \"EF\", \"EC\", \"ETN\", \"ETM\"] # 어미\n","    SUPPORT_VERB = [\"VX\"]\n","    NUMBER = [\"SN\"]\n","\n","    # Remove JOSA, EOMI, etc\n","    morphs[:] = (morph for morph in morphs if morph[1] not in JOSA+SIGN+TERMINATION+SUPPORT_VERB)\n","\n","    # If you want to save only Nouns:\n","    # morphs = mecab.nouns(sentence)\n","\n","    # Remove Stopwords\n","    morphs[:] = (morph for morph in morphs if morph[0] not in korean_stopwords[\"형태\"].tolist())\n","    morphs[:] = (morph for morph in morphs if morph[0] not in my_korean_stopwords[\"형태\"].tolist())\n","\n","    # Remove length-1 words  \n","    morphs[:] = (morph for morph in morphs if not (len(morph[0]) == 1))\n","\n","    # Remove Numbers\n","    morphs[:] = (morph for morph in morphs if morph[1] not in NUMBER)\n","\n","    # Result pop-up\n","    result = []\n","    for morph in morphs:\n","        result.append(morph[0])\n","\n","    return result"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":60,"status":"aborted","timestamp":1656947787605,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"fp-dDQXhL5Q3"},"outputs":[{"ename":"Exception","evalue":"The MeCab dictionary does not exist at \"/usr/local/lib/mecab/dic/mecab-ko-dic\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\konlpy\\tag\\_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagger \u001b[39m=\u001b[39m Tagger(\u001b[39m'\u001b[39;49m\u001b[39m-d \u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m dicpath)\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagset \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/data/tagset/mecab.json\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m utils\u001b[39m.\u001b[39minstallpath)\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\MeCab.py:355\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m--> 355\u001b[0m     _MeCab\u001b[39m.\u001b[39mTagger_swiginit(\u001b[39mself\u001b[39m, _MeCab\u001b[39m.\u001b[39;49mnew_Tagger(\u001b[39m*\u001b[39;49margs))\n","\u001b[1;31mRuntimeError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\lsa_lda\\NLP-실습2차시 - part3 (Korean).ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/lsa_lda/NLP-%EC%8B%A4%EC%8A%B52%EC%B0%A8%EC%8B%9C%20-%20part3%20%28Korean%29.ipynb#ch0000016?line=8'>9</a>\u001b[0m my_korean_stopwords \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(my_data, columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m형태\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m품사\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/lsa_lda/NLP-%EC%8B%A4%EC%8A%B52%EC%B0%A8%EC%8B%9C%20-%20part3%20%28Korean%29.ipynb#ch0000016?line=10'>11</a>\u001b[0m \u001b[39m# Main \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/lsa_lda/NLP-%EC%8B%A4%EC%8A%B52%EC%B0%A8%EC%8B%9C%20-%20part3%20%28Korean%29.ipynb#ch0000016?line=11'>12</a>\u001b[0m mecab \u001b[39m=\u001b[39m Mecab(dicpath\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/usr/local/lib/mecab/dic/mecab-ko-dic\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\konlpy\\tag\\_mecab.py:80\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagset \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/data/tagset/mecab.json\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m utils\u001b[39m.\u001b[39minstallpath)\n\u001b[0;32m     79\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe MeCab dictionary does not exist at \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Is the dictionary correctly installed?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMecab(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m/some/dic/path\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m dicpath)\n\u001b[0;32m     81\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;31mException\u001b[0m: The MeCab dictionary does not exist at \"/usr/local/lib/mecab/dic/mecab-ko-dic\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\""]}],"source":["# Korean Stopwords Load\n","korean_stopwords = pd.read_csv(\"./data/korean_stopwords.txt\", delimiter='\\t', names=[\"형태\", \"품사\", \"비율\"])\n","\n","# Add Custom Korean Stopwords \n","my_data = [[\"님\", \"NNG\"], [\"들\", \"XSN\"], [\"ㅋㅋㄱㅋㄱㅋ\", \"NNG\"], \n","           [\"오늘\", \"NNG\"], [\"얘기\", \"NNG\"], [\"ㅠㅠ\", \"NNG\"], [\"없이\", ], [\"딱히\", ], \n","           ['ㅋㅋ', ], ['ㅋㅋㅋ', ], [\"그런데\", ], [\"누구\", ], [\"여기저기\", ]]\n","\n","my_korean_stopwords = pd.DataFrame(my_data, columns = ['형태', '품사'])\n","\n","# Main \n","#mecab = Mecab(dicpath=\"/usr/local/lib/mecab/dic/mecab-ko-dic\") # Mecab Dictionary Path"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":62,"status":"aborted","timestamp":1656947787607,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"wWRXiATSL5Q3"},"outputs":[],"source":["SAMPLE_TEXT = \"RT @boxplus01: 美언론, 'abc한국 코로나 확산주범은 신천지와 보수세력' https://t.co/Phq0l48aUm\"\n","print(\"Before preprocessing : {}\".format(SAMPLE_TEXT))\n","print(\"After preprocessing : {}\".format(preprocessing_mecab(SAMPLE_TEXT)))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":62,"status":"aborted","timestamp":1656947787608,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"SwoCc4eYL5Q4"},"outputs":[],"source":["total_df['tweet'] = total_df['tweet'].apply(lambda x: preprocessing_mecab(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":63,"status":"aborted","timestamp":1656947787609,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"Mv2Le5qUL5Q4"},"outputs":[],"source":["total_df['tweet']"]},{"cell_type":"markdown","metadata":{"id":"6U0lFfLXL5Q4"},"source":["# # Find topics"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":63,"status":"aborted","timestamp":1656947787610,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"LTLzMFUBL5Q5"},"outputs":[],"source":["data_lemmatized = total_df['tweet'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":64,"status":"aborted","timestamp":1656947787611,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"A6AW2u_qL5Q5"},"outputs":[],"source":["id2word = corpora.Dictionary(data_lemmatized)\n","texts = data_lemmatized\n","\n","corpus = [id2word.doc2bow(text) for text in texts]\n","print(corpus[:1])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":63,"status":"aborted","timestamp":1656947787611,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"cN6-VLB-L5Q5"},"outputs":[],"source":["[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":64,"status":"aborted","timestamp":1656947787612,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"c3xWn1SYL5Q6"},"outputs":[],"source":["lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=20, \n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=2,\n","                                           alpha='auto',\n","                                           per_word_topics=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":62,"status":"aborted","timestamp":1656947787613,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"7fnJKsjYL5Q6","scrolled":true},"outputs":[],"source":["pprint(lda_model.print_topics())\n","doc_lda = lda_model[corpus]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":59,"status":"aborted","timestamp":1656947787614,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"J8X1QdbLL5Q7"},"outputs":[],"source":["# Compute Perplexity\n","print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n","\n","# Compute Coherence Score\n","coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","coherence_lda = coherence_model_lda.get_coherence()\n","print('\\nCoherence Score: ', coherence_lda)"]},{"cell_type":"markdown","metadata":{"id":"sVj0c2KYL5Q7"},"source":["# Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":58,"status":"aborted","timestamp":1656947787615,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"pcl9EBeGL5Q8"},"outputs":[],"source":["# Visualize the topics\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n","vis"]},{"cell_type":"markdown","metadata":{"id":"-uYO6CewL5Q8"},"source":["# Choose the number of topics"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"aborted","timestamp":1656947787615,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"sCB48HmmL5Q8"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"aborted","timestamp":1656947787616,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"15F-Pim4L5Q8"},"outputs":[],"source":["def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n","    coherence_values = []\n","    model_list = []\n","    for num_topics in tqdm(range(start, limit, step)):\n","        model =  gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=num_topics, \n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=2,\n","                                           alpha='auto',\n","                                           per_word_topics=True)\n","        \n","        model_list.append(model)\n","        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n","        coherence_values.append(coherencemodel.get_coherence())\n","    return model_list, coherence_values"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"aborted","timestamp":1656947787616,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"bZdsHvU_L5Q9"},"outputs":[],"source":["model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=35, step=6)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":58,"status":"aborted","timestamp":1656947787617,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"-4i3Eq3qL5Q9"},"outputs":[],"source":["limit=35; start=2; step=6;\n","x = range(start, limit, step)\n","plt.plot(x, coherence_values)\n","plt.xlabel(\"Num Topics\")\n","plt.ylabel(\"Coherence score\")\n","plt.legend((\"coherence_values\"), loc='best')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"g232GiiSL5Q-"},"source":["# Finding the dominant topic in each sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"aborted","timestamp":1656947787617,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"EBtR9a5YL5Q-"},"outputs":[],"source":["optimal_model = model_list[5]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":58,"status":"aborted","timestamp":1656947787618,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"kEqwwwMVL5Q-"},"outputs":[],"source":["optimal_model.per_word_topics = False"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":58,"status":"aborted","timestamp":1656947787619,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"AYELdP2GL5Q-"},"outputs":[],"source":["# Compute Perplexity\n","print('\\nPerplexity: ', optimal_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n","\n","# Compute Coherence Score\n","coherence_model_lda = CoherenceModel(model=optimal_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","coherence_lda = coherence_model_lda.get_coherence()\n","print('\\nCoherence Score: ', coherence_lda)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":58,"status":"aborted","timestamp":1656947787620,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"PQ03fhC_L5Q-"},"outputs":[],"source":["def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=None):\n","    sent_topics_df = pd.DataFrame()\n","\n","    # Get main topic in each document\n","    for i, row in enumerate(tqdm(ldamodel[corpus])):\n","        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n","        # Get the Dominant topic, Perc Contribution and Keywords for each document\n","        for j, (topic_num, prop_topic) in enumerate(row):\n","            if j == 0:  # => dominant topic\n","                wp = ldamodel.show_topic(topic_num)\n","                topic_keywords = \", \".join([word for word, prop in wp])\n","                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n","            else:\n","                break\n","    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n","\n","    # Add original text to the end of the output\n","    contents = pd.Series(texts)\n","    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n","    return(sent_topics_df)\n","\n","\n","df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=total_df['tweet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"aborted","timestamp":1656947787621,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"JEDsaRxOL5Q_"},"outputs":[],"source":["df_dominant_topic = df_topic_sents_keywords.reset_index()\n","df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"aborted","timestamp":1656947787621,"user":{"displayName":"Mingi Shin","userId":"15085003669572053319"},"user_tz":-540},"id":"Ltk9s-LzL5Q_","scrolled":true},"outputs":[],"source":["df_dominant_topic.head(50)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"NLP-실습2차시 - part Final_practice (example).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"}}},"nbformat":4,"nbformat_minor":0}
