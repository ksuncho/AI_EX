{"cells":[{"cell_type":"markdown","metadata":{"id":"XYF3SEhVbRgu"},"source":["# AI 전문가 교육과정 실습 2 - part 2\n","\n","***\n","### NLP응용: 토픽 추출\n","Applied Natrual Language Processing: Topic Modeling\n","\n","강사: 차미영 교수 (카이스트 전산학부)    \n","조교: 신민기, 정현규 (카이스트 전산학부)\n","\n","# Gensim LDA, Visualization"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6Q_Dgpdobwu7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyLDAvis==3.2.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.2.2)\n","Requirement already satisfied: wheel>=0.23.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (0.37.1)\n","Requirement already satisfied: numpy>=1.9.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.23.0)\n","Requirement already satisfied: scipy>=0.18.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.8.1)\n","Requirement already satisfied: joblib>=0.8.4 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.1.0)\n","Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (3.1.2)\n","Requirement already satisfied: numexpr in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (2.8.3)\n","Requirement already satisfied: future in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (0.18.2)\n","Requirement already satisfied: funcy in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.17)\n","Requirement already satisfied: pandas>=0.17.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis==3.2.2) (1.4.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis==3.2.2) (2.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2022.1)\n","Requirement already satisfied: packaging in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from numexpr->pyLDAvis==3.2.2) (21.3)\n","Requirement already satisfied: six>=1.5 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.17.0->pyLDAvis==3.2.2) (1.16.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->numexpr->pyLDAvis==3.2.2) (3.0.9)\n"]}],"source":["!pip install pyLDAvis==3.2.2"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"HijOdqKabRgv"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n","  from imp import reload\n"]}],"source":["import re\n","import numpy as np\n","import pandas as pd\n","from pprint import pprint\n","\n","# Gensim\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.datasets import fetch_20newsgroups\n","\n","# spacy for lemmatization\n","import spacy\n","import en_core_web_sm\n","\n","# Plotting tools\n","import pyLDAvis\n","import pyLDAvis.gensim  # don't skip this\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Enable logging for gensim - optional\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TK5yt7Gycbjg"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\AI_15\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\AI_15\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\AI_15\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","metadata":{"id":"ZswZYT0CbRgw"},"source":["# Topic modeling with Gensim LDA\n","\n","Gensim: Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible. Gensim is designed to process raw, unstructured digital texts (“plain text”) using unsupervised machine learning algorithms.\n","\n","More detailed information: https://radimrehurek.com/gensim/index.html"]},{"cell_type":"markdown","metadata":{"id":"5vanLtWIbRgx"},"source":["### Load data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ceqK7KbLbRgx"},"outputs":[],"source":["dataset = fetch_20newsgroups(shuffle=True,\n","                            random_state=32,\n","                            remove=('headers', 'footers', 'qutes'))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KsTaaV4FbRgy"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>News</th>\n","      <th>Target</th>\n","      <th>Target_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The real question here in my opinion is what M...</td>\n","      <td>4</td>\n","      <td>comp.sys.mac.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Please could someone in the US give me the cur...</td>\n","      <td>4</td>\n","      <td>comp.sys.mac.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Can somebody please help me with information a...</td>\n","      <td>12</td>\n","      <td>sci.electronics</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In article &lt;2077@rwing.UUCP&gt; pat@rwing.UUCP (P...</td>\n","      <td>16</td>\n","      <td>talk.politics.guns</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From article &lt;1pq6i2$a1f@news.ysu.edu&gt;, by ak2...</td>\n","      <td>7</td>\n","      <td>rec.autos</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11309</th>\n","      <td>In article &lt;1qvs9t$q3f@usenet.INS.CWRU.Edu&gt; Ch...</td>\n","      <td>4</td>\n","      <td>comp.sys.mac.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>11310</th>\n","      <td>Hi,\\nI've got a Multi I/O card (IDE controller...</td>\n","      <td>3</td>\n","      <td>comp.sys.ibm.pc.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>11311</th>\n","      <td>\\n     As a person who has rarely even SEEN Do...</td>\n","      <td>10</td>\n","      <td>rec.sport.hockey</td>\n","    </tr>\n","    <tr>\n","      <th>11312</th>\n","      <td>&gt;&gt; So they should sue the newspaper I got it f...</td>\n","      <td>1</td>\n","      <td>comp.graphics</td>\n","    </tr>\n","    <tr>\n","      <th>11313</th>\n","      <td>If your buying a compact pickup do yourself a ...</td>\n","      <td>7</td>\n","      <td>rec.autos</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11314 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                    News  Target  \\\n","0      The real question here in my opinion is what M...       4   \n","1      Please could someone in the US give me the cur...       4   \n","2      Can somebody please help me with information a...      12   \n","3      In article <2077@rwing.UUCP> pat@rwing.UUCP (P...      16   \n","4      From article <1pq6i2$a1f@news.ysu.edu>, by ak2...       7   \n","...                                                  ...     ...   \n","11309  In article <1qvs9t$q3f@usenet.INS.CWRU.Edu> Ch...       4   \n","11310  Hi,\\nI've got a Multi I/O card (IDE controller...       3   \n","11311  \\n     As a person who has rarely even SEEN Do...      10   \n","11312  >> So they should sue the newspaper I got it f...       1   \n","11313  If your buying a compact pickup do yourself a ...       7   \n","\n","                    Target_name  \n","0         comp.sys.mac.hardware  \n","1         comp.sys.mac.hardware  \n","2               sci.electronics  \n","3            talk.politics.guns  \n","4                     rec.autos  \n","...                         ...  \n","11309     comp.sys.mac.hardware  \n","11310  comp.sys.ibm.pc.hardware  \n","11311          rec.sport.hockey  \n","11312             comp.graphics  \n","11313                 rec.autos  \n","\n","[11314 rows x 3 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["news_df = pd.DataFrame({'News': dataset.data,\n","                       'Target': dataset.target})\n","news_df['Target_name'] = news_df['Target'].apply(lambda x: dataset.target_names[x])\n","news_df"]},{"cell_type":"markdown","metadata":{"id":"uysgf60rbRgy"},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"sZ1iuI_nbRgz"},"outputs":[],"source":["data = news_df.News.values.tolist()\n","\n","# Remove Emails\n","data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n","\n","# Remove new line characters\n","data = [re.sub('\\s+', ' ', sent) for sent in data]\n","\n","# Remove distracting single quotes\n","data = [re.sub(\"\\'\", \"\", sent) for sent in data]"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"kkNwJcx-bRgz"},"outputs":[{"name":"stdout","output_type":"stream","text":["[['the', 'real', 'question', 'here', 'in', 'my', 'opinion', 'is', 'what', 'motorola', 'processors', 'running', 'system', 'on', 'mac', 'are', 'comparable', 'to', 'what', 'intel', 'processors', 'running', 'windows', 'on', 'pc', 'recall', 'there', 'being', 'conversation', 'here', 'that', 'running', 'windows', 'benchmarks', 'at', 'about', 'the', 'same', 'speed', 'as', 'mhz', 'in', 'system', 'dont', 'know', 'if', 'that', 'is', 'true', 'but', 'would', 'love', 'to', 'hear', 'if', 'anyone', 'has', 'any', 'technical', 'data', 'on', 'this', 'david']]\n"]}],"source":["def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n","    \n","data_words = list(sent_to_words(data))\n","print(data_words[:1])"]},{"cell_type":"markdown","metadata":{"id":"UF8JWCyAbRg0"},"source":["### Adding bigrams + trigrams"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Yuh3Ux0fbRg0","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['the', 'real', 'question', 'here', 'in', 'my', 'opinion', 'is', 'what', 'motorola', 'processors', 'running', 'system', 'on', 'mac', 'are', 'comparable', 'to', 'what', 'intel', 'processors', 'running', 'windows', 'on', 'pc', 'recall', 'there', 'being', 'conversation', 'here', 'that', 'running', 'windows', 'benchmarks', 'at', 'about', 'the', 'same', 'speed', 'as', 'mhz', 'in', 'system', 'dont', 'know', 'if', 'that', 'is', 'true', 'but', 'would', 'love', 'to', 'hear', 'if', 'anyone', 'has', 'any', 'technical', 'data', 'on', 'this', 'david']\n"]}],"source":["# Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100) #\n","\n","# Faster way to get a sentence clubbed as a trigram/bigram\n","bigram_mod = gensim.models.phrases.Phraser(bigram)\n","trigram_mod = gensim.models.phrases.Phraser(trigram)\n","\n","print(trigram_mod[bigram_mod[data_words[0]]])"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"WVmbDeUxbRg0"},"outputs":[],"source":["stop_words = stopwords.words('english')\n","stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n","\n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n","\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    \"\"\"https://spacy.io/api/annotation\"\"\"\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","    return texts_out"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"oLZ_h0JrbRg1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[['real', 'question', 'opinion', 'processor', 'run', 'system', 'comparable', 'intel', 'processor', 'run', 'pc', 'recall', 'conversation', 'run', 'benchmark', 'speed', 'mhz', 'system', 'know', 'true', 'love', 'hear', 'technical', 'datum']]\n"]}],"source":["# Remove Stop Words\n","data_words_nostops = remove_stopwords(data_words)\n","\n","# Form Bigrams\n","data_words_bigrams = make_bigrams(data_words_nostops)\n","\n","# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n","# python3 -m spacy download en\n","nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n","\n","# Do lemmatization keeping only noun, adj, vb, adv\n","data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","\n","print(data_lemmatized[:1])"]},{"cell_type":"markdown","metadata":{"id":"1MXIF3MkbRg2"},"source":["### Create the dictionary and the corpus"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"M2jnwSW3bRg2"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 3), (16, 1), (17, 2), (18, 1), (19, 1)]]\n"]}],"source":["id2word = corpora.Dictionary(data_lemmatized)\n","texts = data_lemmatized\n","\n","corpus = [id2word.doc2bow(text) for text in texts]\n","print(corpus[:1])"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"y8xCruT0bRg2"},"outputs":[{"data":{"text/plain":["[[('benchmark', 1),\n","  ('comparable', 1),\n","  ('conversation', 1),\n","  ('datum', 1),\n","  ('hear', 1),\n","  ('intel', 1),\n","  ('know', 1),\n","  ('love', 1),\n","  ('mhz', 1),\n","  ('opinion', 1),\n","  ('pc', 1),\n","  ('processor', 2),\n","  ('question', 1),\n","  ('real', 1),\n","  ('recall', 1),\n","  ('run', 3),\n","  ('speed', 1),\n","  ('system', 2),\n","  ('technical', 1),\n","  ('true', 1)]]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"]},{"cell_type":"markdown","metadata":{"id":"9tfMsIpVbRg3"},"source":["### Build LDA model"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Cglidst_bRg3","scrolled":true},"outputs":[],"source":["lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=20, \n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=2,\n","                                           alpha='auto',\n","                                           per_word_topics=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"gVddpN2ybRg4","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[(0,\n","  '0.071*\"internal\" + 0.047*\"creation\" + 0.047*\"mb\" + 0.034*\"brand\" + '\n","  '0.032*\"external\" + 0.031*\"justify\" + 0.031*\"split\" + 0.029*\"category\" + '\n","  '0.028*\"status\" + 0.022*\"usenet\"'),\n"," (1,\n","  '0.029*\"build\" + 0.026*\"return\" + 0.026*\"number\" + 0.024*\"player\" + '\n","  '0.022*\"chip\" + 0.020*\"point\" + 0.016*\"line\" + 0.012*\"date\" + 0.012*\"round\" '\n","  '+ 0.011*\"charge\"'),\n"," (2,\n","  '0.077*\"ground\" + 0.044*\"fall\" + 0.039*\"land\" + 0.034*\"center\" + '\n","  '0.030*\"city\" + 0.030*\"input\" + 0.029*\"obvious\" + 0.027*\"russian\" + '\n","  '0.023*\"edge\" + 0.020*\"item\"'),\n"," (3,\n","  '0.053*\"mhz\" + 0.045*\"cell\" + 0.044*\"investigation\" + 0.027*\"processor\" + '\n","  '0.023*\"sheet\" + 0.019*\"socket\" + 0.019*\"app\" + 0.017*\"intel\" + '\n","  '0.016*\"diamond\" + 0.013*\"wipe\"'),\n"," (4,\n","  '0.026*\"believe\" + 0.025*\"say\" + 0.024*\"argument\" + 0.022*\"christian\" + '\n","  '0.022*\"claim\" + 0.019*\"true\" + 0.017*\"evidence\" + 0.016*\"man\" + '\n","  '0.015*\"life\" + 0.015*\"conclusion\"'),\n"," (5,\n","  '0.142*\"key\" + 0.076*\"image\" + 0.048*\"compile\" + 0.042*\"avoid\" + '\n","  '0.034*\"copyright\" + 0.030*\"police\" + 0.027*\"title\" + 0.027*\"king\" + '\n","  '0.024*\"define\" + 0.023*\"public\"'),\n"," (6,\n","  '0.086*\"space\" + 0.073*\"object\" + 0.048*\"earth\" + 0.042*\"purpose\" + '\n","  '0.040*\"launch\" + 0.031*\"authority\" + 0.029*\"merely\" + 0.028*\"reality\" + '\n","  '0.027*\"safety\" + 0.026*\"surface\"'),\n"," (7,\n","  '0.096*\"cable\" + 0.082*\"bike\" + 0.075*\"sale\" + 0.056*\"boot\" + 0.055*\"ride\" + '\n","  '0.049*\"request\" + 0.036*\"controller\" + 0.034*\"battery\" + 0.030*\"protection\" '\n","  '+ 0.026*\"pair\"'),\n"," (8,\n","  '0.032*\"go\" + 0.027*\"get\" + 0.021*\"say\" + 0.016*\"come\" + 0.015*\"year\" + '\n","  '0.015*\"see\" + 0.014*\"s\" + 0.013*\"well\" + 0.013*\"good\" + 0.012*\"game\"'),\n"," (9,\n","  '0.048*\"satellite\" + 0.045*\"drug\" + 0.044*\"study\" + 0.040*\"orbit\" + '\n","  '0.039*\"doctor\" + 0.039*\"research\" + 0.037*\"disease\" + 0.036*\"medical\" + '\n","  '0.034*\"treatment\" + 0.028*\"patient\"'),\n"," (10,\n","  '0.640*\"ax\" + 0.013*\"shipping\" + 0.009*\"km\" + 0.007*\"tm\" + 0.006*\"mf\" + '\n","  '0.002*\"wm\" + 0.002*\"impulse\" + 0.001*\"m\" + 0.001*\"giz_giz\" + 0.001*\"mi\"'),\n"," (11,\n","  '0.063*\"metal\" + 0.062*\"print\" + 0.059*\"printer\" + 0.054*\"character\" + '\n","  '0.044*\"page\" + 0.040*\"totally\" + 0.038*\"mirror\" + 0.031*\"mit\" + '\n","  '0.029*\"roll\" + 0.026*\"printing\"'),\n"," (12,\n","  '0.077*\"science\" + 0.039*\"objective\" + 0.037*\"unfortunately\" + 0.034*\"moral\" '\n","  '+ 0.028*\"animal\" + 0.027*\"self\" + 0.025*\"absolute\" + 0.023*\"harm\" + '\n","  '0.023*\"scientific\" + 0.020*\"ship\"'),\n"," (13,\n","  '0.164*\"drive\" + 0.152*\"car\" + 0.040*\"driver\" + 0.031*\"engine\" + '\n","  '0.028*\"brake\" + 0.028*\"tire\" + 0.026*\"mile\" + 0.026*\"speed\" + '\n","  '0.025*\"private\" + 0.024*\"owner\"'),\n"," (14,\n","  '0.026*\"use\" + 0.016*\"system\" + 0.015*\"get\" + 0.013*\"work\" + 0.013*\"bit\" + '\n","  '0.012*\"problem\" + 0.011*\"new\" + 0.011*\"need\" + 0.011*\"thank\" + 0.010*\"run\"'),\n"," (15,\n","  '0.023*\"write\" + 0.016*\"people\" + 0.016*\"article\" + 0.014*\"make\" + '\n","  '0.013*\"know\" + 0.009*\"think\" + 0.008*\"many\" + 0.008*\"thing\" + 0.008*\"give\" '\n","  '+ 0.007*\"say\"'),\n"," (16,\n","  '0.097*\"file\" + 0.068*\"window\" + 0.061*\"program\" + 0.046*\"color\" + '\n","  '0.044*\"do\" + 0.038*\"disk\" + 0.028*\"package\" + 0.025*\"software\" + '\n","  '0.025*\"mission\" + 0.024*\"driver\"'),\n"," (17,\n","  '0.134*\"church\" + 0.109*\"frequency\" + 0.085*\"commercial\" + '\n","  '0.023*\"presentation\" + 0.023*\"please\" + 0.022*\"emit\" + 0.019*\"detector\" + '\n","  '0.012*\"radar_detector\" + 0.005*\"vector\" + 0.000*\"bulletin_board\"'),\n"," (18,\n","  '0.064*\"map\" + 0.052*\"border\" + 0.022*\"window_manag\" + 0.011*\"width_height\" '\n","  '+ 0.008*\"window_manager\" + 0.000*\"position\" + 0.000*\"ozone\" + 0.000*\"gif\" + '\n","  '0.000*\"bitmap\" + 0.000*\"xsizehint\"'),\n"," (19,\n","  '0.061*\"location\" + 0.036*\"dual\" + 0.022*\"hook\" + 0.010*\"parking\" + '\n","  '0.008*\"desk\" + 0.007*\"bedroom\" + 0.006*\"downtown\" + 0.001*\"room\" + '\n","  '0.000*\"wire\" + 0.000*\"university\"')]\n"]}],"source":["pprint(lda_model.print_topics())\n","doc_lda = lda_model[corpus]"]},{"cell_type":"markdown","metadata":{"id":"bzYxSLGPbRg4"},"source":["### Evaluate the LDA model"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"uxdn0WATbRg4"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Perplexity:  -13.528916135797832\n"]}],"source":["# Compute Perplexity\n","print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n","\n","# Compute Coherence Score\n","coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","coherence_lda = coherence_model_lda.get_coherence()\n","print('\\nCoherence Score: ', coherence_lda)"]},{"cell_type":"markdown","metadata":{"id":"utVhD-yYbRg5"},"source":["### Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1KPhRspbRg5"},"outputs":[],"source":["# Visualize the topics\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n","vis"]},{"cell_type":"markdown","metadata":{"id":"Kp_3FW7VbRg5"},"source":["### Choose the number of topics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djo2jokabRg6"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBuRgbW5bRg6"},"outputs":[],"source":["def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n","    coherence_values = []\n","    model_list = []\n","    for num_topics in tqdm(range(start, limit, step)):\n","        model =  gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=num_topics, \n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=2,\n","                                           alpha='auto',\n","                                           per_word_topics=True)\n","        \n","        model_list.append(model)\n","        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n","        coherence_values.append(coherencemodel.get_coherence())\n","    return model_list, coherence_values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPzqu0QWbRg6"},"outputs":[],"source":["model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=35, step=6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwOJa4axbRg6"},"outputs":[],"source":["limit=35; start=2; step=6;\n","x = range(start, limit, step)\n","plt.plot(x, coherence_values)\n","plt.xlabel(\"Num Topics\")\n","plt.ylabel(\"Coherence score\")\n","plt.legend((\"coherence_values\"), loc='best')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rBlasqHibRg7"},"source":["### Finding the dominant topic in each sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"no-zspfabRg7"},"outputs":[],"source":["optimal_model = model_list[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xR3PX_6bRg7"},"outputs":[],"source":["optimal_model.per_word_topics = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96lLbITNbRg7"},"outputs":[],"source":["def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n","    sent_topics_df = pd.DataFrame()\n","\n","    # Get main topic in each document\n","    for i, row in enumerate(ldamodel[corpus]):\n","        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n","        # Get the Dominant topic, Perc Contribution and Keywords for each document\n","        for j, (topic_num, prop_topic) in enumerate(row):\n","            if j == 0:  # => dominant topic\n","                wp = ldamodel.show_topic(topic_num)\n","                topic_keywords = \", \".join([word for word, prop in wp])\n","                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n","            else:\n","                break\n","    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n","\n","    # Add original text to the end of the output\n","    contents = pd.Series(texts)\n","    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n","    return(sent_topics_df)\n","\n","\n","df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kCxkBhCbRg8"},"outputs":[],"source":["df_dominant_topic = df_topic_sents_keywords.reset_index()\n","df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORhkDRf2bRg8","scrolled":true},"outputs":[],"source":["df_dominant_topic.head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5sKZX6tLYTI"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"NLP-실습2차시 - part2.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"}}},"nbformat":4,"nbformat_minor":0}
