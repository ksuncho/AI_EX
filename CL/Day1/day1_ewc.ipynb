{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import data_handler\n",
    "from sklearn.utils import shuffle\n",
    "import trainer\n",
    "import networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "# general experimental setting \n",
    "args['date'] = '210806'\n",
    "args['dataset'] = 'MNIST'\n",
    "args['trainer'] = 'ewc' # ER or EWC\n",
    "args['seed'] = 0\n",
    "args['output_path'] = '' # this is corrected at the next cell\n",
    "\n",
    "# setting for continual learning \n",
    "args['tasknum'] = 5\n",
    "\n",
    "#hyperparameter for optimization\n",
    "args['batch_size'] = 256\n",
    "args['lr'] = 0.001\n",
    "args['epochs'] = 10\n",
    "args['decay'] = 0 # weight decay (L2 penaly for general regularization)\n",
    "\n",
    "args['schedule_milestone'] = [7] #Decrease learning rate at these epochs\n",
    "args['gamma'] = 0.2\n",
    "\n",
    "\n",
    "#hyperparameter for EWC\n",
    "args['lamb'] = 1\n",
    "\n",
    "# for GPU, if you cannot use GPU, set device as None\n",
    "device = 0\n",
    "if device is not None:\n",
    "    torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for saving datasets, output files and trained models.\n",
    "if not os.path.isdir('dat'):\n",
    "    print('Make directory for dataset')\n",
    "    os.makedirs('dat')\n",
    "\n",
    "if not os.path.isdir('result_data'):\n",
    "    print('Make directory for saving results')\n",
    "    os.makedirs('result_data')\n",
    "\n",
    "if not os.path.isdir('trained_model'):\n",
    "    print('Make directory for saving trained models')\n",
    "    os.makedirs('trained_model')\n",
    "\n",
    "# Make filename of a file for logging a result\n",
    "log_name = '{}_{}_{}_{}_lamb_{}_lr_{}_batch_{}_epoch_{}'.format(args['date'], args['dataset'], args['trainer'],args['seed'], \n",
    "                                                                       args['lamb'], args['lr'], args['batch_size'], args['epochs'])\n",
    "\n",
    "if args['output_path'] == '':\n",
    "    args['output_path'] = './result_data/' + log_name + '.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed for deterministic results\n",
    "np.random.seed(args['seed'])\n",
    "random.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# device = torch.device(\"gpu\")\n",
    "# torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "\n",
      "Task info = [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "c:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "c:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "c:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "#Load a dataset and a dataloader that outputs a task sequentially\n",
    "print('Load data...')\n",
    "data_dict = None\n",
    "dataset = data_handler.DatasetFactory.get_dataset(args['dataset'], args['tasknum'])\n",
    "task_info = dataset.task_info\n",
    "print('\\nTask info =', task_info)\n",
    "\n",
    "# Loader used for training data\n",
    "shuffle_idx = shuffle(np.arange(dataset.classes), random_state=args['seed'])\n",
    "\n",
    "# list of dataloaders: it consists of dataloaders for each task\n",
    "train_dataset_loaders = data_handler.make_ContinualLoaders(dataset.train_data,\n",
    "                                                        dataset.train_labels,\n",
    "                                                        task_info,\n",
    "                                                        transform=dataset.train_transform,\n",
    "                                                        shuffle_idx = shuffle_idx,\n",
    "                                                        data_dict = data_dict,\n",
    "                                                       )\n",
    "\n",
    "test_dataset_loaders = data_handler.make_ContinualLoaders(dataset.test_data,\n",
    "                                                       dataset.test_labels,\n",
    "                                                       task_info,\n",
    "                                                       transform=dataset.test_transform,\n",
    "                                                       shuffle_idx = shuffle_idx,\n",
    "                                                       data_dict = data_dict,\n",
    "                                                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required model\n",
    "if device is not None:\n",
    "    myModel = networks.ModelFactory.get_model(args['dataset'], args['trainer'], task_info).to(device)\n",
    "else:\n",
    "    myModel = networks.ModelFactory.get_model(args['dataset'], args['trainer'], task_info)\n",
    "\n",
    "# Define the optimizer used in the experiment\n",
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=args['lr'], weight_decay=args['decay'])\n",
    "\n",
    "# Initilize the evaluators used to measure the performance of the system.\n",
    "t_classifier = trainer.EvaluatorFactory.get_evaluator(\"trainedClassifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(trainer.GenericTrainer):\n",
    "    def __init__(self, model, args, optimizer, evaluator, task_info):\n",
    "        super().__init__(model, args, optimizer, evaluator, task_info)\n",
    "        \n",
    "        self.lamb=args['lamb']\n",
    "        \n",
    "\n",
    "    def train(self, train_loader, test_loader, t, device = None):\n",
    "        \n",
    "        self.device = device\n",
    "        self.setup_training(self.lr)\n",
    "        # Do not update self.t\n",
    "        if t>0: # update fisher before starting training new task\n",
    "            self.update_frozen_model()\n",
    "            self.update_fisher(device)\n",
    "        \n",
    "        # Now, you can update self.t\n",
    "        self.t = t\n",
    "        \n",
    "        self.train_iterator = torch.utils.data.DataLoader(train_loader, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_iterator = torch.utils.data.DataLoader(test_loader, 100, shuffle=False)\n",
    "        self.fisher_iterator = torch.utils.data.DataLoader(train_loader, batch_size=20, shuffle=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            for samples in self.train_iterator:\n",
    "                data, target = samples\n",
    "                if device is not None:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = self.model(data)[t]\n",
    "                loss_CE = self.criterion(output,target)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                (loss_CE).backward()\n",
    "                self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "            train_loss,train_acc = self.evaluator.evaluate(self.model, self.train_iterator, t, self.device)\n",
    "            num_batch = len(self.train_iterator)\n",
    "            print('| Epoch {:3d} | Train: loss={:.3f}, acc={:5.1f}% |'.format(epoch+1,train_loss,100*train_acc),end='')\n",
    "            test_loss,test_acc=self.evaluator.evaluate(self.model, self.test_iterator, t, self.device)\n",
    "            print(' Test: loss={:.3f}, acc={:5.1f}% |'.format(test_loss,100*test_acc),end='')\n",
    "            print()\n",
    "        \n",
    "    def criterion(self,output,targets):\n",
    "        \"\"\"\n",
    "        Arguments: output (The output logit of self.model), targets (Ground truth label)\n",
    "        Return: loss function for the regularization-based continual learning\n",
    "        \n",
    "        For the hyperparameter on regularization, please use self.lamb\n",
    "        \"\"\"\n",
    "        #######################################################################################\n",
    "        # Write youre code here\n",
    "        #######################################################################################\n",
    "        reg = 0\n",
    "        if self.t > 0:\n",
    "            for (name, param), (_, param_fixed) in zip(self.model.named_parameters(), self.model_fixed.named_parameters()):\n",
    "                reg += torch.sum(self.fisher[name] * (param - param_fixed).pow(2))\n",
    "\n",
    "        return self.ce(output,targets) + self.lamb * reg\n",
    "    \n",
    "    def compute_diag_fisher(self, device=None):\n",
    "        \"\"\"\n",
    "        Arguments: None. Just use global variables (self.model, self.criterion, ...)\n",
    "        Return: Diagonal Fisher matrix. \n",
    "        \n",
    "        This function will be used in the function 'update_fisher'\n",
    "        \"\"\"\n",
    "        #######################################################################################\n",
    "        # Write youre code here\n",
    "        #######################################################################################\n",
    "        fisher = {}\n",
    "        for (name, param) in self.model.named_parameters():\n",
    "            fisher[name]= 0 * param.data # torch.zeros_like(param)\n",
    "\n",
    "        # accumulate the diag of FIM\n",
    "        total = 0\n",
    "        for data, target in self.fisher_iterator:\n",
    "            output = self.model(data)[self.t]\n",
    "            loss = self.ce(output, target)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "        \n",
    "        for (name, param) in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                fisher[name] += param.grad.pow(2)\n",
    "        \n",
    "        total +=1\n",
    "\n",
    "        # average\n",
    "        for (name, param) in self.model.named_parameters():\n",
    "            fisher[name] /= total\n",
    "            \n",
    "        return fisher\n",
    "    \n",
    "    def update_fisher(self, device):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments: None. Just use global variables (self.model, self.fisher, ...)\n",
    "        Return: None. Just update the global variable self.fisher\n",
    "        Use 'compute_diag_fisher' to compute the fisher matrix\n",
    "        \n",
    "        hint : pytorch function\n",
    "        pseudo code\n",
    "        \"\"\"\n",
    "        #######################################################################################\n",
    "        #  Write youre code here\n",
    "        #######################################################################################\n",
    "        fisher = self.compute_diag_fisher()\n",
    "        if self.t > 0:\n",
    "            for key in self.fisher.keys():\n",
    "                self.fisher[key] += fisher[key]\n",
    "        else:\n",
    "            self.fisher = fisher\n",
    "        return fisher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer object used for training\n",
    "myTrainer = Trainer(myModel, args, optimizer, t_classifier, task_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "MLP_Net(\n",
      "  (relu): ReLU()\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=784, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (last): ModuleList(\n",
      "    (0): Linear(in_features=400, out_features=2, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=2, bias=True)\n",
      "    (2): Linear(in_features=400, out_features=2, bias=True)\n",
      "    (3): Linear(in_features=400, out_features=2, bias=True)\n",
      "    (4): Linear(in_features=400, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Dimensions = torch.Size([400, 784]) torch.Size([400]) torch.Size([400, 400]) torch.Size([400]) torch.Size([2, 400]) torch.Size([2]) torch.Size([2, 400]) torch.Size([2]) torch.Size([2, 400]) torch.Size([2]) torch.Size([2, 400]) torch.Size([2]) torch.Size([2, 400]) torch.Size([2]) \n",
      "Num parameters = 478.4K\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ") = lr: 0.001, betas: (0.9, 0.999), eps: 1e-08, weight_decay: 0, amsgrad: False, maximize: False, foreach: None, capturable: False, initial_lr: 0.001, \n",
      "----------------------------------------------------------------------------------------------------\n",
      "tasknum: 0\n",
      "0\n",
      "Setting LR to 0.0010\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CL\\Day1\\Day1\\day1_ewc.ipynb 셀 10\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=14'>15</a>\u001b[0m test_loader \u001b[39m=\u001b[39m test_dataset_loaders[t]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=16'>17</a>\u001b[0m myTrainer\u001b[39m.\u001b[39;49mtrain(train_loader, test_loader, t, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m u \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=19'>20</a>\u001b[0m     test_loader \u001b[39m=\u001b[39m test_dataset_loaders[u]\n",
      "\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CL\\Day1\\Day1\\day1_ewc.ipynb 셀 10\u001b[0m in \u001b[0;36mTrainer.train\u001b[1;34m(self, train_loader, test_loader, t, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=28'>29</a>\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=30'>31</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(data)[t]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=31'>32</a>\u001b[0m loss_CE \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(output,target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=34'>35</a>\u001b[0m (loss_CE)\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CL\\Day1\\Day1\\day1_ewc.ipynb 셀 10\u001b[0m in \u001b[0;36mTrainer.criterion\u001b[1;34m(self, output, targets)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=57'>58</a>\u001b[0m     \u001b[39mfor\u001b[39;00m (name, param), (_, param_fixed) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnamed_parameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_fixed\u001b[39m.\u001b[39mnamed_parameters()):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=58'>59</a>\u001b[0m         reg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfisher[name] \u001b[39m*\u001b[39m (param \u001b[39m-\u001b[39m param_fixed)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CL/Day1/Day1/day1_ewc.ipynb#ch0000009?line=60'>61</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mce(output,targets) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlamb \u001b[39m*\u001b[39m reg\n",
      "File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "\n",
    "utils.print_model_report(myModel)\n",
    "utils.print_optimizer_config(optimizer)\n",
    "print('-' * 100)\n",
    "\n",
    "# Loop tasks\n",
    "acc = np.zeros((len(task_info), len(task_info)), dtype=np.float32)\n",
    "lss = np.zeros((len(task_info), len(task_info)), dtype=np.float32)\n",
    "for t, ncla in task_info:\n",
    "    print(\"tasknum:\", t)\n",
    "    # Add new classes to the train, and test iterator\n",
    "\n",
    "    train_loader = train_dataset_loaders[t]\n",
    "    test_loader = test_dataset_loaders[t]\n",
    "    print(device)\n",
    "    myTrainer.train(train_loader, test_loader, t, device)\n",
    "\n",
    "    for u in range(t+1):\n",
    "        test_loader = test_dataset_loaders[u]\n",
    "        test_iterator = torch.utils.data.DataLoader(test_loader, 100, shuffle=False)\n",
    "        test_loss, test_acc = t_classifier.evaluate(myTrainer.model, test_iterator, u, device)\n",
    "        print('>>> Test on task {:2d}: loss={:.3f}, acc={:5.1f}% <<<'.format(u, test_loss, 100 * test_acc))\n",
    "        acc[t, u] = test_acc\n",
    "        lss[t, u] = test_loss\n",
    "\n",
    "    print('Average accuracy={:5.1f}%'.format(100 * np.mean(acc[t,:t+1])))\n",
    "\n",
    "    print('Save at ' + args['output_path'])\n",
    "    np.savetxt(args['output_path'], acc, '%.4f')\n",
    "    torch.save(myModel.state_dict(), './trained_model/' + log_name + '_task_{}.pt'.format(t))\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print('Accuracies =')\n",
    "for i in range(acc.shape[0]):\n",
    "    print('\\t', end='')\n",
    "    for j in range(acc.shape[1]):\n",
    "        print('{:5.1f}% '.format(100 * acc[i, j]), end='')\n",
    "    print()\n",
    "print('*' * 100)\n",
    "print('Done!')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9954     0.9499     0.96476667 0.963475   0.95766   ]\n"
     ]
    }
   ],
   "source": [
    "def avg_acc(file_name):\n",
    "    acc_arr = np.loadtxt(file_name)\n",
    "    avg_acc_arr = np.zeros(acc_arr.shape[1])\n",
    "    for i in range(acc_arr.shape[1]):\n",
    "        avg_acc_arr[i] = np.mean(acc_arr[i][:i+1])\n",
    "    \n",
    "    return avg_acc_arr\n",
    "filename = 'result_data/210806_MNIST_ewc_0_lamb_1_lr_0.001_batch_256_epoch_10.txt'\n",
    "results = avg_acc(filename)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "task_num = args['tasknum']\n",
    "task = np.arange(task_num) +1\n",
    "ax = plt.subplot(111)\n",
    "    \n",
    "# for key in results.keys():\n",
    "ax.plot(task, results, label = 'ewc', linestyle = '-', marker = '.')\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "plt.xticks(task)\n",
    "\n",
    "plt.xlabel('Task',fontsize = 20)\n",
    "plt.ylabel('Accuracy',fontsize = 20)\n",
    "\n",
    "ax.legend(loc = 'center right', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
