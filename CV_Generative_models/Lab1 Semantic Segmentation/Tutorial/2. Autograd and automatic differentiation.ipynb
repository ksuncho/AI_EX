{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Autograd and automatic differentiation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_fY5HjBoM79k"},"source":["Recommended materials\n","====\n","\n","1. Pytorch Official Tutorial \\[[Link](https://pytorch.org/tutorials/)\\]\n","2. DeepLearning Zero to All \\[[English](https://www.youtube.com/playlist?list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m)\\] \\[[Korean](https://www.youtube.com/playlist?list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv)\\]\n","3. Neural Network Programming - Deep Learning with Pytorch \\[[English](https://www.youtube.com/playlist?list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG)\\]"]},{"cell_type":"markdown","metadata":{"id":"yO7iCNrFydnp"},"source":["Autograd and Automatic Differentiation\n","====\n","\n","[reference](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n","\n","The **`autograd`** package provides automatic differentiation for all operations on **`Tensors`**.   \n","It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."]},{"cell_type":"markdown","metadata":{"id":"WIPFLC2r0FAz"},"source":["## Forward Function\n","![](https://drive.google.com/uc?export=view&id=1h8IKgTjOOci0Q75Y9XyA9X7m49bUgX04)\n","\n","Lets take a look at this simple feedforward function that takes `p`, `q`, and `s` as input variables and computes the output variable `t`.\n","```\n","r = p + q\n","t = r * s = (p + q) * s\n","```\n","  \n","If we assign 3, 5, 2 to `p`, `q`, and `s`, respectively, then we can get `t` that is equal to 16\n","```\n","p = 3  \n","q = 5  \n","s = 2  \n","r = p + q = 8  \n","t = r * s = (p + q) * s = 8 * 2 = 16\n","```\n","\n","This simple process can be easily described using **pytorch**, as follows"]},{"cell_type":"code","metadata":{"id":"cvJ0Hc32vYum"},"source":["import torch\n","\n","p = torch.tensor(3.)\n","q = torch.tensor(5.)\n","s = torch.tensor(2.)\n","r = p + q\n","t = r * s\n","\n","print('r :', r)\n","print('t :', t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tgHheOwQzE3l"},"source":["## Backward Function\n","![](https://drive.google.com/uc?export=view&id=1kCKhSO3HFcm_bygQBO2fyluvq1Ku5ov0)\n","\n","The core of deep learning is to train deep neural networks using back-propagated gradients given some outputs (and labels).  \n","In the case of this feedforward function, we can easily compute each element in backward pass and get gradients for the components that we are interested in,  \n","e.g. `dt/dp`, `dt/dq`, and `dt/ds`.\n","\n","```\n","p = 3  \n","q = 5  \n","s = 2  \n","r = p + q = 8  \n","t = r * s = (p + q) * s = 8 * 2 = 16\n","\n","dt/dp = dt/dr * dr/dp = s * 1 = s = 2  \n","dt/dq = dt/dr * dr/dq = s * 1 = s = 2  \n","dt/ds = r = 8\n","```\n","\n","However, this manual computation of gradients becomes labor-consuming and even intractible when our function is very complex, or a neural net is very **deep**"]},{"cell_type":"markdown","metadata":{"id":"PWZqdZAn0juS"},"source":["## Autograd automatically tracks all operations on Tensors and gives us gradients\n","\n","**`torch.Tensor`** is the central class of the **`pytorch`** package. If you set its attribute **`.requires_grad`** as **`True`**, it starts to track all operations on it. When you finish your computation you can call **`.backward()`** and have all the gradients computed automatically. The gradient for this tensor will be accumulated into **`.grad`** attribute."]},{"cell_type":"code","metadata":{"id":"sgzGDXYgywE5"},"source":["import torch\n","\n","p = torch.tensor([3.], requires_grad=True)\n","q = torch.tensor([5.], requires_grad=True)\n","s = torch.tensor([2.], requires_grad=True)\n","r = p + q\n","t = r * s\n","\n","print('r :', r.item())\n","print('t :', t.item())\n","# below will invoke errors for now because we didn't perform backpropagte yet\n","# print('dt/dp :', p.grad.item())  \n","# print('dt/dq :', q.grad.item())  \n","# print('dt/ds :', s.grad.item())  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSZ52gnmzrOu"},"source":["# now do backpropagation starting from output variable t\n","t.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1zQ-mtMFzxTa"},"source":["print('dt/dp :', p.grad.item())\n","print('dt/dq :', q.grad.item())  \n","print('dt/ds :', s.grad.item())  "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"S_pfrMi9ptDh"},"execution_count":null,"outputs":[]}]}