{"cells":[{"cell_type":"markdown","metadata":{"id":"4iySKmlBg97f"},"source":["# GANs for Image Generation tasks"]},{"cell_type":"markdown","metadata":{"id":"szfSTP2MkCZs"},"source":["## 2. Conditional GANs - AC-GAN"]},{"cell_type":"markdown","metadata":{"id":"dJjSXzlAmCOI"},"source":["### Prepare DataLoader for MNIST dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"i5Fuxt5dP7IZ"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","from torchvision import transforms\n","\n","# fix manual seed.\n","torch.manual_seed(1234)\n","\n","# set batch size.\n","BATCH_SIZE = 256\n","\n","# prepare dataloader.\n","tf = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","train_dataset = MNIST(root='./datasets', train=True, download=False, transform=tf)\n","loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"GFVC5C4wmQ5k"},"source":["#### Define Generator"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"d1FepF4OmKVY"},"outputs":[],"source":["import torch.nn as nn\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        # ============================================================ #\n","        # TODO : Fill fully connected layers upconvolution layers\n","        # * Specificl details of model architectures are on the slides. \n","        # * Hint : Use following functions : \n","        #   nn.Linear(), nn.BatchNorm1d(), nn.ConvTranspose2d(), \n","        #   nn.BatchNorm2d(), nn.ReLU()\n","        # ============================================================ #\n","\n","        self.z_dim = 64\n","        self.num_class = 10\n","        self.hidden_dim = 256\n","        self.img_dim = 28 * 28\n","\n","        self.fc = nn.Sequential(\n","            # Fill here.\n","            nn.Linear(self.z_dim + self.num_class, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(),\n","            nn.Linear(1024,128*7*7),\n","            nn.BatchNorm1d(128*7*7),\n","            nn.ReLU()    \n","        )\n","        \n","        self.upconv = nn.Sequential(\n","            # Fill here.\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh()    \n","        )\n","        \n","    def forward(self, z, y):\n","        # ============================================================ #\n","        # TODO : Complete forward function. \n","        # * Hint : Use self.fc and self.upconv defined above\n","        # ============================================================ #\n","\n","        # Fill here. \n","        out = torch.cat((z,y), dim=1)\n","        out = self.fc(out)\n","        out = out.view(-1, 128, 7, 7)\n","        out = self.upconv(out)\n","        return out\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ajYUrgp0mUt8"},"source":["#### Define Discriminator"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5upJT6CemOz6"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        # ============================================================ #\n","        # TODO : Fill convolution layers and fully connected layers.\n","        # * Specificl details of model architectures are on the slides. \n","        # * Hint : Use following functions : \n","        #   nn.Conv2d(), nn.LeaklyReLU(), nn.Linear(), nn.BatchNorm1d(),\n","        #   nn.Sigmoid()\n","        # ============================================================ #\n","        \n","        self.img_dim = 28 * 28\n","        self.hidden_dim = 256\n","        self.num_class = 10\n","\n","        self.conv = nn.Sequential(\n","            # Fill here.\n","            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2)    \n","\n","        )\n","        self.fc = nn.Sequential(\n","            # Fill here.\n","            nn.Linear(128*7*7, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.LeakyReLU(0.2)    \n","\n","        )\n","        self.fc_disc = nn.Sequential(\n","            # Fill here. \n","            nn.Linear(1024, 1),\n","            nn.Sigmoid()   \n","\n","        )\n","        \n","        self.fc_cls = nn.Sequential(\n","            # Fill here. \n","            nn.Linear(1024, 10)\n","        \n","        )\n","\n","    def forward(self, x):\n","        # ============================================================ #\n","        # TODO : Complete forward function. \n","        # * Hint : Use self.fc and self.upconv defined above\n","        #    - out_disc : head for real/fake discrimination \n","        #    - out_cls : head for classification\n","        # ============================================================ #\n","\n","        # Fill here.   \n","        out = self.conv(x)\n","        out = out.view(-1, 128 * 7 * 7)\n","        out = self.fc(out)\n","        out_disc = self.fc_disc(out) \n","        out_cls = self.fc_cls(out)\n","\n","        return out_disc, out_cls\n"]},{"cell_type":"markdown","metadata":{"id":"z7yQUvj2rMLU"},"source":["#### Prepare GAN model and Optimizers"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Irabx2XtoRhY"},"outputs":[],"source":["# weight initialization function. \n","def weights_init(net):\n","    for m in net.modules():\n","        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","            m.weight.data.normal_(0, 0.02)\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","# define GAN model.\n","G = Generator().cuda()\n","D = Discriminator().cuda()\n","\n","# weight initialization & set both modes to train mode.\n","G.apply(weights_init)\n","D.apply(weights_init)\n","\n","# define optimizer. Here we use Adam optimizer. \n","optimizer_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5,0.999))\n","optimizer_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5,0.999))\n"]},{"cell_type":"markdown","metadata":{"id":"MhfKpgHmnQz_"},"source":["#### Start training GAN"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"pJWTZUxxeQ2I"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch : 0 || 0/234 || loss_G=3.057 loss_D=6.106\n","Epoch : 0 || 100/234 || loss_G=1.001 loss_D=1.260\n","Epoch : 0 || 200/234 || loss_G=1.253 loss_D=0.925\n","Epoch : 0 has done. AVG loss : loss_G=1.163 loss_D=1.404\n","Epoch : 1 || 0/234 || loss_G=1.269 loss_D=0.868\n","Epoch : 1 || 100/234 || loss_G=1.349 loss_D=0.889\n","Epoch : 1 || 200/234 || loss_G=1.134 loss_D=1.036\n","Epoch : 1 has done. AVG loss : loss_G=1.306 loss_D=0.964\n","Epoch : 2 || 0/234 || loss_G=1.053 loss_D=1.127\n","Epoch : 2 || 100/234 || loss_G=1.108 loss_D=1.093\n","Epoch : 2 || 200/234 || loss_G=0.989 loss_D=1.194\n","Epoch : 2 has done. AVG loss : loss_G=1.056 loss_D=1.150\n","Epoch : 3 || 0/234 || loss_G=1.090 loss_D=1.125\n","Epoch : 3 || 100/234 || loss_G=1.012 loss_D=1.252\n","Epoch : 3 || 200/234 || loss_G=0.937 loss_D=1.171\n","Epoch : 3 has done. AVG loss : loss_G=0.984 loss_D=1.184\n","Epoch : 4 || 0/234 || loss_G=1.013 loss_D=1.149\n","Epoch : 4 || 100/234 || loss_G=0.956 loss_D=1.185\n","Epoch : 4 || 200/234 || loss_G=0.916 loss_D=1.241\n","Epoch : 4 has done. AVG loss : loss_G=0.949 loss_D=1.200\n","Epoch : 5 || 0/234 || loss_G=0.945 loss_D=1.238\n","Epoch : 5 || 100/234 || loss_G=0.922 loss_D=1.208\n","Epoch : 5 || 200/234 || loss_G=0.884 loss_D=1.220\n","Epoch : 5 has done. AVG loss : loss_G=0.937 loss_D=1.202\n","Epoch : 6 || 0/234 || loss_G=0.838 loss_D=1.258\n","Epoch : 6 || 100/234 || loss_G=1.045 loss_D=1.168\n","Epoch : 6 || 200/234 || loss_G=0.902 loss_D=1.188\n","Epoch : 6 has done. AVG loss : loss_G=0.930 loss_D=1.201\n","Epoch : 7 || 0/234 || loss_G=0.856 loss_D=1.177\n","Epoch : 7 || 100/234 || loss_G=0.889 loss_D=1.324\n","Epoch : 7 || 200/234 || loss_G=0.918 loss_D=1.240\n","Epoch : 7 has done. AVG loss : loss_G=0.926 loss_D=1.205\n","Epoch : 8 || 0/234 || loss_G=0.945 loss_D=1.214\n","Epoch : 8 || 100/234 || loss_G=0.899 loss_D=1.187\n","Epoch : 8 || 200/234 || loss_G=0.903 loss_D=1.209\n","Epoch : 8 has done. AVG loss : loss_G=0.925 loss_D=1.210\n","Epoch : 9 || 0/234 || loss_G=0.903 loss_D=1.226\n","Epoch : 9 || 100/234 || loss_G=0.992 loss_D=1.162\n","Epoch : 9 || 200/234 || loss_G=0.848 loss_D=1.196\n","Epoch : 9 has done. AVG loss : loss_G=0.926 loss_D=1.197\n","Epoch : 10 || 0/234 || loss_G=0.882 loss_D=1.194\n","Epoch : 10 || 100/234 || loss_G=0.808 loss_D=1.228\n","Epoch : 10 || 200/234 || loss_G=0.893 loss_D=1.180\n","Epoch : 10 has done. AVG loss : loss_G=0.931 loss_D=1.193\n","Epoch : 11 || 0/234 || loss_G=0.955 loss_D=1.184\n","Epoch : 11 || 100/234 || loss_G=0.953 loss_D=1.160\n","Epoch : 11 || 200/234 || loss_G=0.968 loss_D=1.165\n","Epoch : 11 has done. AVG loss : loss_G=0.941 loss_D=1.191\n","Epoch : 12 || 0/234 || loss_G=0.924 loss_D=1.170\n","Epoch : 12 || 100/234 || loss_G=1.005 loss_D=1.185\n","Epoch : 12 || 200/234 || loss_G=1.015 loss_D=1.144\n","Epoch : 12 has done. AVG loss : loss_G=0.946 loss_D=1.175\n","Epoch : 13 || 0/234 || loss_G=0.951 loss_D=1.216\n","Epoch : 13 || 100/234 || loss_G=0.946 loss_D=1.112\n","Epoch : 13 || 200/234 || loss_G=0.942 loss_D=1.131\n","Epoch : 13 has done. AVG loss : loss_G=0.957 loss_D=1.169\n","Epoch : 14 || 0/234 || loss_G=0.941 loss_D=1.104\n","Epoch : 14 || 100/234 || loss_G=1.005 loss_D=1.184\n","Epoch : 14 || 200/234 || loss_G=0.957 loss_D=1.119\n","Epoch : 14 has done. AVG loss : loss_G=0.974 loss_D=1.154\n","Epoch : 15 || 0/234 || loss_G=1.025 loss_D=1.160\n","Epoch : 15 || 100/234 || loss_G=0.975 loss_D=1.196\n","Epoch : 15 || 200/234 || loss_G=0.962 loss_D=1.198\n","Epoch : 15 has done. AVG loss : loss_G=0.988 loss_D=1.149\n","Epoch : 16 || 0/234 || loss_G=0.947 loss_D=1.132\n","Epoch : 16 || 100/234 || loss_G=1.014 loss_D=1.118\n","Epoch : 16 || 200/234 || loss_G=0.974 loss_D=1.065\n","Epoch : 16 has done. AVG loss : loss_G=1.001 loss_D=1.123\n","Epoch : 17 || 0/234 || loss_G=1.264 loss_D=1.223\n","Epoch : 17 || 100/234 || loss_G=1.020 loss_D=1.043\n","Epoch : 17 || 200/234 || loss_G=0.995 loss_D=1.077\n","Epoch : 17 has done. AVG loss : loss_G=1.017 loss_D=1.141\n","Epoch : 18 || 0/234 || loss_G=1.101 loss_D=1.065\n","Epoch : 18 || 100/234 || loss_G=1.107 loss_D=1.157\n","Epoch : 18 || 200/234 || loss_G=1.223 loss_D=1.088\n","Epoch : 18 has done. AVG loss : loss_G=1.035 loss_D=1.088\n","Epoch : 19 || 0/234 || loss_G=0.973 loss_D=1.020\n","Epoch : 19 || 100/234 || loss_G=0.968 loss_D=1.056\n","Epoch : 19 || 200/234 || loss_G=1.210 loss_D=1.239\n","Epoch : 19 has done. AVG loss : loss_G=1.060 loss_D=1.091\n","Epoch : 20 || 0/234 || loss_G=1.171 loss_D=1.028\n","Epoch : 20 || 100/234 || loss_G=1.037 loss_D=1.012\n","Epoch : 20 || 200/234 || loss_G=0.943 loss_D=1.025\n","Epoch : 20 has done. AVG loss : loss_G=1.075 loss_D=1.060\n","Epoch : 21 || 0/234 || loss_G=1.119 loss_D=1.043\n","Epoch : 21 || 100/234 || loss_G=1.023 loss_D=1.047\n","Epoch : 21 || 200/234 || loss_G=1.347 loss_D=1.131\n","Epoch : 21 has done. AVG loss : loss_G=1.099 loss_D=1.042\n","Epoch : 22 || 0/234 || loss_G=1.204 loss_D=1.031\n","Epoch : 22 || 100/234 || loss_G=1.230 loss_D=0.994\n","Epoch : 22 || 200/234 || loss_G=1.246 loss_D=1.053\n","Epoch : 22 has done. AVG loss : loss_G=1.128 loss_D=1.023\n","Epoch : 23 || 0/234 || loss_G=1.060 loss_D=0.972\n","Epoch : 23 || 100/234 || loss_G=1.307 loss_D=1.116\n","Epoch : 23 || 200/234 || loss_G=1.064 loss_D=0.984\n","Epoch : 23 has done. AVG loss : loss_G=1.144 loss_D=1.008\n","Epoch : 24 || 0/234 || loss_G=1.541 loss_D=1.003\n","Epoch : 24 || 100/234 || loss_G=1.296 loss_D=1.014\n","Epoch : 24 || 200/234 || loss_G=1.362 loss_D=0.980\n","Epoch : 24 has done. AVG loss : loss_G=1.166 loss_D=0.985\n","Epoch : 25 || 0/234 || loss_G=1.191 loss_D=0.999\n","Epoch : 25 || 100/234 || loss_G=1.195 loss_D=0.991\n","Epoch : 25 || 200/234 || loss_G=1.154 loss_D=0.977\n","Epoch : 25 has done. AVG loss : loss_G=1.208 loss_D=0.976\n","Epoch : 26 || 0/234 || loss_G=0.843 loss_D=1.036\n","Epoch : 26 || 100/234 || loss_G=1.363 loss_D=0.875\n","Epoch : 26 || 200/234 || loss_G=1.262 loss_D=0.927\n","Epoch : 26 has done. AVG loss : loss_G=1.225 loss_D=0.943\n","Epoch : 27 || 0/234 || loss_G=1.325 loss_D=0.928\n","Epoch : 27 || 100/234 || loss_G=1.161 loss_D=0.882\n","Epoch : 27 || 200/234 || loss_G=1.471 loss_D=0.919\n","Epoch : 27 has done. AVG loss : loss_G=1.244 loss_D=0.947\n","Epoch : 28 || 0/234 || loss_G=1.332 loss_D=0.923\n","Epoch : 28 || 100/234 || loss_G=1.302 loss_D=0.838\n","Epoch : 28 || 200/234 || loss_G=1.409 loss_D=0.938\n","Epoch : 28 has done. AVG loss : loss_G=1.263 loss_D=0.962\n","Epoch : 29 || 0/234 || loss_G=1.398 loss_D=0.840\n","Epoch : 29 || 100/234 || loss_G=1.171 loss_D=0.923\n","Epoch : 29 || 200/234 || loss_G=1.348 loss_D=0.816\n","Epoch : 29 has done. AVG loss : loss_G=1.282 loss_D=0.902\n","Epoch : 30 || 0/234 || loss_G=1.446 loss_D=0.854\n","Epoch : 30 || 100/234 || loss_G=1.088 loss_D=0.874\n","Epoch : 30 || 200/234 || loss_G=1.391 loss_D=0.824\n","Epoch : 30 has done. AVG loss : loss_G=1.316 loss_D=0.883\n","Epoch : 31 || 0/234 || loss_G=1.419 loss_D=0.849\n","Epoch : 31 || 100/234 || loss_G=1.355 loss_D=0.901\n","Epoch : 31 || 200/234 || loss_G=1.346 loss_D=0.902\n","Epoch : 31 has done. AVG loss : loss_G=1.340 loss_D=0.883\n","Epoch : 32 || 0/234 || loss_G=1.610 loss_D=0.904\n","Epoch : 32 || 100/234 || loss_G=0.946 loss_D=1.056\n","Epoch : 32 || 200/234 || loss_G=1.454 loss_D=0.877\n","Epoch : 32 has done. AVG loss : loss_G=1.364 loss_D=0.876\n","Epoch : 33 || 0/234 || loss_G=1.680 loss_D=0.827\n","Epoch : 33 || 100/234 || loss_G=1.084 loss_D=0.787\n","Epoch : 33 || 200/234 || loss_G=1.474 loss_D=0.844\n","Epoch : 33 has done. AVG loss : loss_G=1.386 loss_D=0.848\n","Epoch : 34 || 0/234 || loss_G=1.255 loss_D=0.811\n","Epoch : 34 || 100/234 || loss_G=1.694 loss_D=0.762\n","Epoch : 34 || 200/234 || loss_G=1.246 loss_D=0.769\n","Epoch : 34 has done. AVG loss : loss_G=1.411 loss_D=0.841\n","Epoch : 35 || 0/234 || loss_G=1.641 loss_D=0.872\n","Epoch : 35 || 100/234 || loss_G=1.287 loss_D=0.958\n","Epoch : 35 || 200/234 || loss_G=1.161 loss_D=0.823\n","Epoch : 35 has done. AVG loss : loss_G=1.431 loss_D=0.832\n","Epoch : 36 || 0/234 || loss_G=1.633 loss_D=0.958\n","Epoch : 36 || 100/234 || loss_G=1.558 loss_D=0.785\n","Epoch : 36 || 200/234 || loss_G=1.240 loss_D=0.867\n","Epoch : 36 has done. AVG loss : loss_G=1.464 loss_D=0.815\n","Epoch : 37 || 0/234 || loss_G=1.204 loss_D=0.768\n","Epoch : 37 || 100/234 || loss_G=1.708 loss_D=0.783\n","Epoch : 37 || 200/234 || loss_G=1.620 loss_D=0.850\n","Epoch : 37 has done. AVG loss : loss_G=1.471 loss_D=0.806\n","Epoch : 38 || 0/234 || loss_G=1.919 loss_D=0.676\n","Epoch : 38 || 100/234 || loss_G=1.163 loss_D=0.794\n","Epoch : 38 || 200/234 || loss_G=1.456 loss_D=0.824\n","Epoch : 38 has done. AVG loss : loss_G=1.508 loss_D=0.778\n","Epoch : 39 || 0/234 || loss_G=1.768 loss_D=0.745\n","Epoch : 39 || 100/234 || loss_G=1.604 loss_D=0.866\n","Epoch : 39 || 200/234 || loss_G=1.827 loss_D=0.825\n","Epoch : 39 has done. AVG loss : loss_G=1.540 loss_D=0.780\n","Epoch : 40 || 0/234 || loss_G=1.624 loss_D=0.953\n","Epoch : 40 || 100/234 || loss_G=1.683 loss_D=0.645\n","Epoch : 40 || 200/234 || loss_G=1.744 loss_D=0.782\n","Epoch : 40 has done. AVG loss : loss_G=1.555 loss_D=0.767\n","Epoch : 41 || 0/234 || loss_G=1.353 loss_D=0.705\n","Epoch : 41 || 100/234 || loss_G=1.078 loss_D=0.874\n","Epoch : 41 || 200/234 || loss_G=2.122 loss_D=0.815\n","Epoch : 41 has done. AVG loss : loss_G=1.582 loss_D=0.752\n","Epoch : 42 || 0/234 || loss_G=1.937 loss_D=0.682\n","Epoch : 42 || 100/234 || loss_G=1.748 loss_D=0.881\n","Epoch : 42 || 200/234 || loss_G=1.638 loss_D=0.633\n","Epoch : 42 has done. AVG loss : loss_G=1.605 loss_D=0.726\n","Epoch : 43 || 0/234 || loss_G=1.887 loss_D=0.738\n","Epoch : 43 || 100/234 || loss_G=1.831 loss_D=0.694\n","Epoch : 43 || 200/234 || loss_G=2.085 loss_D=0.651\n","Epoch : 43 has done. AVG loss : loss_G=1.625 loss_D=0.733\n","Epoch : 44 || 0/234 || loss_G=1.879 loss_D=0.921\n","Epoch : 44 || 100/234 || loss_G=1.349 loss_D=0.618\n","Epoch : 44 || 200/234 || loss_G=1.271 loss_D=0.834\n","Epoch : 44 has done. AVG loss : loss_G=1.650 loss_D=0.922\n","Epoch : 45 || 0/234 || loss_G=1.413 loss_D=0.670\n","Epoch : 45 || 100/234 || loss_G=1.932 loss_D=0.657\n","Epoch : 45 || 200/234 || loss_G=1.151 loss_D=0.784\n","Epoch : 45 has done. AVG loss : loss_G=1.632 loss_D=0.681\n","Epoch : 46 || 0/234 || loss_G=1.493 loss_D=0.611\n","Epoch : 46 || 100/234 || loss_G=1.465 loss_D=0.652\n","Epoch : 46 || 200/234 || loss_G=2.053 loss_D=0.709\n","Epoch : 46 has done. AVG loss : loss_G=1.680 loss_D=0.679\n","Epoch : 47 || 0/234 || loss_G=2.387 loss_D=0.663\n","Epoch : 47 || 100/234 || loss_G=1.711 loss_D=0.633\n","Epoch : 47 || 200/234 || loss_G=1.896 loss_D=0.733\n","Epoch : 47 has done. AVG loss : loss_G=1.703 loss_D=0.685\n","Epoch : 48 || 0/234 || loss_G=2.430 loss_D=0.742\n","Epoch : 48 || 100/234 || loss_G=1.427 loss_D=0.603\n","Epoch : 48 || 200/234 || loss_G=1.582 loss_D=0.668\n","Epoch : 48 has done. AVG loss : loss_G=1.723 loss_D=0.684\n","Epoch : 49 || 0/234 || loss_G=1.901 loss_D=0.593\n","Epoch : 49 || 100/234 || loss_G=1.753 loss_D=0.594\n","Epoch : 49 || 200/234 || loss_G=1.930 loss_D=0.606\n","Epoch : 49 has done. AVG loss : loss_G=1.761 loss_D=0.655\n"]}],"source":["from torch.utils.tensorboard import SummaryWriter\n","from torchvision.utils import make_grid\n","\n","# Hyper-parameters. \n","# ====== You don't need to change here ===== #\n","EPOCHS = 50\n","Z_DIM = 64\n","NUM_CLASS = 10\n","# ========================================== #\n","\n","# logger for tensorboard.\n","logger = SummaryWriter()\n","\n","# Fixed latent variable z, label y for visualization. \n","FIXED_Z = torch.randn(size=(100,Z_DIM)).cuda()\n","FIXED_Y = torch.arange(10).repeat(10)\n","FIXED_Y = torch.zeros(size=(100,NUM_CLASS)).scatter_(1, FIXED_Y.unsqueeze(1), 1).cuda()\n","\n","# GT labels for calculating binary cross entropy loss. \n","real_label = torch.ones(size=(BATCH_SIZE,1)).cuda()\n","fake_label = torch.zeros(size=(BATCH_SIZE,1)).cuda()\n","\n","# criterion for binary cross entropy loss\n","BCE_criterion = torch.nn.BCELoss()\n","CE_criterion = torch.nn.CrossEntropyLoss()\n","\n","for epoch in range(EPOCHS):\n","    # Set both models to train modes.\n","    G.train()\n","    D.train()\n","\n","    # For logging in tensorboard\n","    loss_G_total, loss_D_total = 0., 0.\n","\n","    for batch_idx, (data, label) in enumerate(loader):\n","        data = data.cuda()\n","        label = label.cuda()\n","        \n","        # ============================================================ #\n","        # TODO : Fill the part for updating D&G.\n","        # First sample z and y. \n","        # z : (BATCH_SIZE, Z_DIM) size random latent variable\n","        # y : (BATCH_SIZE, NUM_CLASS) size random label\n","        # Then Calculate GAN loss (loss_D, loss_G)\n","        # * Don't forget, you should also consider classification loss!!!     \n","        # ============================================================ #\n","        z = torch.randn(size=(BATCH_SIZE, Z_DIM)).cuda()\n","        y = torch.zeros(size=(BATCH_SIZE, NUM_CLASS)).long().cuda()\n","        y = y.scatter_(1, label.unsqueeze(1), 1)\n","\n","        fake_img = G(z,y).detach()\n","        real_img = data\n","\n","        # ================= Update D ================== # \n","                       \n","        # Fill here. \n","        # First compute loss_D \n","        # Then update the network with loss_D using optimizer_D\n","        logit_disc_real, logit_cls_real = D(real_img)\n","        logit_disc_fake, logit_cls_fake = D(fake_img)\n","        disc_real = BCE_criterion(logit_disc_real, real_label)\n","        disc_fake = BCE_criterion(logit_disc_fake, fake_label)\n","        cls_real = CE_criterion(logit_cls_real,label)\n","        cls_fake = CE_criterion(logit_cls_fake,label)\n","        loss_D = disc_real + disc_fake + cls_real + cls_fake\n","\n","        optimizer_D.zero_grad()\n","        loss_D.backward()\n","        optimizer_D.step()\n","\n","        # ================= Update G ================== # \n"," \n","        # Fill here. \n","        z = torch.randn(size=(BATCH_SIZE, Z_DIM)).cuda()\n","        fake_img = G(z,y)\n","        # First compute loss_G \n","        logit_disc_fake, logit_cls_fake = D(fake_img)\n","        disc_fake = BCE_criterion(logit_disc_fake, real_label)\n","        cls_fake = CE_criterion(logit_cls_fake,label)\n","        loss_G = disc_fake + cls_fake\n","        # Then update the network with loss_G using optimizer_G.\n","        optimizer_G.zero_grad()\n","        loss_G.backward()\n","        optimizer_G.step()   \n","        # Note that we need additional auxiliary classification loss.\n","\n","\n","        loss_D_total += loss_D.item()\n","        loss_G_total += loss_G.item()\n","        \n","        # print current states\n","        if batch_idx % 100  == 0:\n","            print('Epoch : {} || {}/{} || loss_G={:.3f} loss_D={:.3f}'.format(\n","                epoch, batch_idx, len(loader), loss_G.item(), loss_D.item()\n","            ))\n","\n","    loss_G_total /= len(loader)\n","    loss_D_total /= len(loader)\n","\n","    # ================= Genearte example samples ================== # \n","    fake_img = G(FIXED_Z, FIXED_Y)\n","    fake_img = fake_img.view(fake_img.shape[0], 1, 28, 28)\n","    fake_img = (fake_img + 1)*0.5\n","    fake_img = make_grid(fake_img, nrow=10)\n","\n","    \n","    # ============================================================ #\n","    # TODO : Logging on the tensorboard\n","    # * log loss_G_total, loss_D_total, and fake_img\n","    # * use logger.add_scalar() and logger.add_image() for logging\n","    # ============================================================ #\n","\n","    # Fill here\n","    logger.add_scalar('loss_G', loss_G_total, epoch)\n","    logger.add_scalar('loss_D', loss_D_total, epoch)\n","    logger.add_image('Generated_samples', fake_img, epoch)\n","\n","    # print current states\n","    print('Epoch : {} has done. AVG loss : loss_G={:.3f} loss_D={:.3f}'.format(\n","        epoch, loss_G_total, loss_D_total\n","    ))\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"EV838kqpQKBC"},"outputs":[{"name":"stdout","output_type":"stream","text":[" C ����̺��� �������� �̸��� �����ϴ�.\n"," ���� �Ϸ� ��ȣ: 32F1-D8BD\n","\n"," c:\\Users\\AI_15\\momo\\python_prj\\CV_Generative_models\\Lab2 GAN\\runs ���͸�\n","\n","2022-07-12  ���� 04:31    <DIR>          .\n","2022-07-12  ���� 04:31    <DIR>          ..\n","2022-07-12  ���� 02:59    <DIR>          Jul12_14-59-27_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:39    <DIR>          Jul12_15-39-13_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:39    <DIR>          Jul12_15-39-55_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:40    <DIR>          Jul12_15-40-20_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:43    <DIR>          Jul12_15-43-43_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:44    <DIR>          Jul12_15-44-27_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:44    <DIR>          Jul12_15-44-55_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:46    <DIR>          Jul12_15-46-30_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:53    <DIR>          Jul12_15-53-54_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:55    <DIR>          Jul12_15-55-48_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 03:58    <DIR>          Jul12_15-58-49_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 04:01    <DIR>          Jul12_16-01-46_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 04:03    <DIR>          Jul12_16-03-31_LAPTOP-ASLN0PJR\n","2022-07-12  ���� 04:31    <DIR>          Jul12_16-31-30_LAPTOP-ASLN0PJR\n","               0�� ����                   0 ����Ʈ\n","              16�� ���͸�  325,677,088,768 ����Ʈ ����\n","The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"data":{"text/plain":["Reusing TensorBoard on port 8888 (pid 23112), started 0:06:50 ago. (Use '!kill 23112' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-e18dddb30da3d750\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-e18dddb30da3d750\");\n","          const url = new URL(\"http://localhost\");\n","          const port = 8888;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Check Tensorboard.\n","%ls runs\n","%load_ext tensorboard\n","%tensorboard --logdir runs --port 8888 --samples_per_plugin images=100"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"(Lab 1-2)AC_GAN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"}}},"nbformat":4,"nbformat_minor":0}
