{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvfuNONaN1as"
      },
      "source": [
        "Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLJIygzQLJNF",
        "outputId": "de1a5ed9-0cc7-4f3f-c28e-33340a8daf67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNN_NvDhBhVM",
        "outputId": "f7fd9bc6-8632-499a-8f8c-58fb94837d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: six in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gdown) (4.11.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gdown) (4.43.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gdown) (3.7.1)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gdown) (2.28.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests[socks]->gdown) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests[socks]->gdown) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests[socks]->gdown) (1.26.11)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (pyproject.toml): started\n",
            "  Building wheel for gdown (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14933 sha256=6e67b3cbe0e8026ea43df48b393a2d18dfb8697851fd422b15b97793f33662cd\n",
            "  Stored in directory: c:\\users\\ai_15\\appdata\\local\\pip\\cache\\wheels\\8d\\df\\71\\846b2aa0fabaac2af23fbc5214eeaa55f0616e9d1a05187d72\n",
            "Successfully built gdown\n",
            "Installing collected packages: PySocks, gdown\n",
            "Successfully installed PySocks-1.7.1 gdown-4.5.1\n",
            "\n",
            "[notice] A new release of pip available: 22.1.2 -> 22.2.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JAbmAmI1RRY",
        "outputId": "ac47b70a-7bc4-4dc5-c28f-4c84da00f9d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CV_Caption_VQA\\Day1\\SAMSUNG_ImageCaptioning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\users\\ai_15\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gdown\\cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18iwTb7eSaX3wMtt5zNWl-r8JBZQ35SIP\n",
            "To: c:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CV_Caption_VQA\\Day1\\SAMSUNG_ImageCaptioning\\SAMSUNG_ImageCaptioning.zip\n",
            "\n",
            "  0%|          | 0.00/755M [00:00<?, ?B/s]\n",
            "  0%|          | 524k/755M [00:00<04:59, 2.52MB/s]\n",
            "  0%|          | 2.10M/755M [00:00<03:46, 3.32MB/s]\n",
            "  0%|          | 2.62M/755M [00:00<03:22, 3.72MB/s]\n",
            "  0%|          | 3.67M/755M [00:00<02:47, 4.47MB/s]\n",
            "  1%|          | 8.39M/755M [00:00<02:04, 6.01MB/s]\n",
            "  1%|▏         | 10.5M/755M [00:00<01:38, 7.54MB/s]\n",
            "  2%|▏         | 13.1M/755M [00:00<01:18, 9.47MB/s]\n",
            "  2%|▏         | 15.7M/755M [00:01<01:04, 11.5MB/s]\n",
            "  2%|▏         | 17.8M/755M [00:01<00:58, 12.6MB/s]\n",
            "  3%|▎         | 20.4M/755M [00:01<00:49, 14.7MB/s]\n",
            "  3%|▎         | 22.5M/755M [00:01<00:48, 15.0MB/s]\n",
            "  3%|▎         | 25.7M/755M [00:01<00:42, 17.1MB/s]\n",
            "  4%|▎         | 27.8M/755M [00:01<00:41, 17.6MB/s]\n",
            "  4%|▍         | 30.4M/755M [00:01<00:39, 18.5MB/s]\n",
            "  4%|▍         | 32.5M/755M [00:01<00:38, 18.6MB/s]\n",
            "  5%|▍         | 35.1M/755M [00:02<00:36, 19.5MB/s]\n",
            "  5%|▍         | 37.2M/755M [00:02<00:37, 19.1MB/s]\n",
            "  5%|▌         | 39.8M/755M [00:02<00:36, 19.8MB/s]\n",
            "  6%|▌         | 41.9M/755M [00:02<00:36, 19.3MB/s]\n",
            "  6%|▌         | 44.6M/755M [00:02<00:35, 20.0MB/s]\n",
            "  6%|▌         | 46.7M/755M [00:02<00:36, 19.5MB/s]\n",
            "  7%|▋         | 49.3M/755M [00:02<00:34, 20.2MB/s]\n",
            "  7%|▋         | 51.4M/755M [00:02<00:36, 19.2MB/s]\n",
            "  7%|▋         | 54.0M/755M [00:02<00:34, 20.3MB/s]\n",
            "  7%|▋         | 56.1M/755M [00:03<00:35, 19.8MB/s]\n",
            "  8%|▊         | 58.2M/755M [00:03<00:35, 19.8MB/s]\n",
            "  8%|▊         | 60.3M/755M [00:03<00:38, 18.1MB/s]\n",
            "  8%|▊         | 62.9M/755M [00:03<00:37, 18.3MB/s]\n",
            "  9%|▉         | 66.1M/755M [00:03<00:33, 20.3MB/s]\n",
            "  9%|▉         | 68.7M/755M [00:03<00:33, 20.3MB/s]\n",
            "  9%|▉         | 70.8M/755M [00:03<00:33, 20.2MB/s]\n",
            " 10%|▉         | 72.9M/755M [00:03<00:37, 18.0MB/s]\n",
            " 10%|█         | 76.0M/755M [00:04<00:34, 19.9MB/s]\n",
            " 10%|█         | 78.6M/755M [00:04<00:33, 19.9MB/s]\n",
            " 11%|█         | 80.7M/755M [00:04<00:33, 20.1MB/s]\n",
            " 11%|█         | 82.8M/755M [00:04<00:37, 17.8MB/s]\n",
            " 11%|█▏        | 86.0M/755M [00:04<00:33, 20.0MB/s]\n",
            " 12%|█▏        | 88.6M/755M [00:04<00:36, 18.5MB/s]\n",
            " 12%|█▏        | 91.8M/755M [00:04<00:32, 20.3MB/s]\n",
            " 13%|█▎        | 94.4M/755M [00:04<00:32, 20.2MB/s]\n",
            " 13%|█▎        | 97.0M/755M [00:05<00:32, 20.2MB/s]\n",
            " 13%|█▎        | 99.1M/755M [00:05<00:32, 20.2MB/s]\n",
            " 13%|█▎        | 101M/755M [00:05<00:43, 14.9MB/s] \n",
            " 14%|█▎        | 103M/755M [00:05<00:40, 16.2MB/s]\n",
            " 14%|█▍        | 105M/755M [00:05<00:37, 17.1MB/s]\n",
            " 14%|█▍        | 107M/755M [00:05<00:36, 17.6MB/s]\n",
            " 15%|█▍        | 110M/755M [00:05<00:34, 18.6MB/s]\n",
            " 15%|█▍        | 112M/755M [00:05<00:34, 18.6MB/s]\n",
            " 15%|█▌        | 115M/755M [00:06<00:32, 19.5MB/s]\n",
            " 15%|█▌        | 117M/755M [00:06<00:34, 18.6MB/s]\n",
            " 16%|█▌        | 120M/755M [00:06<00:34, 18.5MB/s]\n",
            " 16%|█▌        | 122M/755M [00:06<00:32, 19.4MB/s]\n",
            " 16%|█▋        | 124M/755M [00:06<00:32, 19.4MB/s]\n",
            " 17%|█▋        | 126M/755M [00:06<00:31, 19.7MB/s]\n",
            " 17%|█▋        | 128M/755M [00:06<00:34, 18.1MB/s]\n",
            " 17%|█▋        | 131M/755M [00:06<00:33, 18.4MB/s]\n",
            " 18%|█▊        | 133M/755M [00:07<00:32, 18.9MB/s]\n",
            " 18%|█▊        | 135M/755M [00:07<00:32, 19.2MB/s]\n",
            " 18%|█▊        | 137M/755M [00:07<00:31, 19.6MB/s]\n",
            " 18%|█▊        | 139M/755M [00:07<00:31, 19.7MB/s]\n",
            " 19%|█▊        | 141M/755M [00:07<00:30, 19.8MB/s]\n",
            " 19%|█▉        | 143M/755M [00:07<00:31, 19.5MB/s]\n",
            " 19%|█▉        | 146M/755M [00:07<00:30, 20.0MB/s]\n",
            " 20%|█▉        | 148M/755M [00:07<00:30, 19.7MB/s]\n",
            " 20%|█▉        | 150M/755M [00:07<00:29, 20.2MB/s]\n",
            " 20%|██        | 153M/755M [00:08<00:30, 19.7MB/s]\n",
            " 20%|██        | 155M/755M [00:08<00:37, 16.2MB/s]\n",
            " 21%|██        | 157M/755M [00:08<00:34, 17.2MB/s]\n",
            " 21%|██        | 159M/755M [00:08<00:33, 18.0MB/s]\n",
            " 21%|██▏       | 161M/755M [00:08<00:31, 18.6MB/s]\n",
            " 22%|██▏       | 163M/755M [00:08<00:31, 19.1MB/s]\n",
            " 22%|██▏       | 165M/755M [00:08<00:32, 17.9MB/s]\n",
            " 22%|██▏       | 168M/755M [00:08<00:29, 19.8MB/s]\n",
            " 23%|██▎       | 170M/755M [00:09<00:31, 18.4MB/s]\n",
            " 23%|██▎       | 172M/755M [00:09<00:29, 20.1MB/s]\n",
            " 23%|██▎       | 175M/755M [00:09<00:29, 19.7MB/s]\n",
            " 24%|██▎       | 178M/755M [00:09<00:31, 18.1MB/s]\n",
            " 24%|██▍       | 181M/755M [00:09<00:28, 20.2MB/s]\n",
            " 24%|██▍       | 184M/755M [00:09<00:30, 18.6MB/s]\n",
            " 25%|██▍       | 187M/755M [00:09<00:30, 18.8MB/s]\n",
            " 25%|██▌       | 190M/755M [00:09<00:27, 20.5MB/s]\n",
            " 25%|██▌       | 192M/755M [00:10<00:30, 18.6MB/s]\n",
            " 26%|██▌       | 196M/755M [00:10<00:27, 20.6MB/s]\n",
            " 26%|██▋       | 198M/755M [00:10<00:28, 19.4MB/s]\n",
            " 27%|██▋       | 201M/755M [00:10<00:26, 20.6MB/s]\n",
            " 27%|██▋       | 203M/755M [00:10<00:27, 20.1MB/s]\n",
            " 27%|██▋       | 206M/755M [00:10<00:26, 20.5MB/s]\n",
            " 28%|██▊       | 209M/755M [00:10<00:26, 20.4MB/s]\n",
            " 28%|██▊       | 211M/755M [00:11<00:26, 20.3MB/s]\n",
            " 28%|██▊       | 213M/755M [00:11<00:26, 20.1MB/s]\n",
            " 28%|██▊       | 215M/755M [00:11<00:26, 20.2MB/s]\n",
            " 29%|██▉       | 217M/755M [00:11<00:29, 18.4MB/s]\n",
            " 29%|██▉       | 220M/755M [00:11<00:26, 20.2MB/s]\n",
            " 30%|██▉       | 223M/755M [00:11<00:26, 20.1MB/s]\n",
            " 30%|██▉       | 225M/755M [00:11<00:26, 20.1MB/s]\n",
            " 30%|███       | 227M/755M [00:11<00:27, 19.2MB/s]\n",
            " 30%|███       | 230M/755M [00:12<00:28, 18.4MB/s]\n",
            " 31%|███       | 233M/755M [00:12<00:25, 20.5MB/s]\n",
            " 31%|███       | 235M/755M [00:12<00:27, 18.7MB/s]\n",
            " 32%|███▏      | 239M/755M [00:12<00:25, 20.6MB/s]\n",
            " 32%|███▏      | 241M/755M [00:12<00:25, 20.2MB/s]\n",
            " 32%|███▏      | 244M/755M [00:12<00:25, 20.4MB/s]\n",
            " 33%|███▎      | 246M/755M [00:12<00:25, 20.2MB/s]\n",
            " 33%|███▎      | 249M/755M [00:12<00:24, 20.3MB/s]\n",
            " 33%|███▎      | 251M/755M [00:13<00:25, 19.7MB/s]\n",
            " 34%|███▎      | 253M/755M [00:13<00:24, 20.3MB/s]\n",
            " 34%|███▍      | 255M/755M [00:13<00:25, 19.5MB/s]\n",
            " 34%|███▍      | 258M/755M [00:13<00:24, 20.4MB/s]\n",
            " 34%|███▍      | 260M/755M [00:13<00:25, 19.1MB/s]\n",
            " 35%|███▍      | 263M/755M [00:13<00:24, 20.4MB/s]\n",
            " 35%|███▌      | 265M/755M [00:13<00:25, 19.2MB/s]\n",
            " 35%|███▌      | 267M/755M [00:13<00:23, 20.3MB/s]\n",
            " 36%|███▌      | 269M/755M [00:13<00:25, 18.9MB/s]\n",
            " 36%|███▌      | 272M/755M [00:14<00:25, 18.9MB/s]\n",
            " 36%|███▋      | 274M/755M [00:14<00:30, 15.7MB/s]\n",
            " 37%|███▋      | 276M/755M [00:14<00:29, 16.3MB/s]\n",
            " 37%|███▋      | 279M/755M [00:14<00:26, 17.8MB/s]\n",
            " 37%|███▋      | 281M/755M [00:14<00:26, 17.6MB/s]\n",
            " 38%|███▊      | 284M/755M [00:14<00:24, 19.1MB/s]\n",
            " 38%|███▊      | 286M/755M [00:14<00:26, 18.0MB/s]\n",
            " 38%|███▊      | 288M/755M [00:15<00:23, 19.8MB/s]\n",
            " 38%|███▊      | 290M/755M [00:15<00:25, 18.0MB/s]\n",
            " 39%|███▉      | 294M/755M [00:15<00:23, 19.9MB/s]\n",
            " 39%|███▉      | 296M/755M [00:15<00:22, 20.0MB/s]\n",
            " 40%|███▉      | 298M/755M [00:15<00:25, 17.8MB/s]\n",
            " 40%|███▉      | 301M/755M [00:15<00:22, 19.8MB/s]\n",
            " 40%|████      | 304M/755M [00:15<00:24, 18.5MB/s]\n",
            " 41%|████      | 307M/755M [00:15<00:22, 20.2MB/s]\n",
            " 41%|████      | 309M/755M [00:16<00:24, 18.4MB/s]\n",
            " 41%|████▏     | 312M/755M [00:16<00:23, 18.8MB/s]\n",
            " 42%|████▏     | 316M/755M [00:16<00:21, 20.9MB/s]\n",
            " 42%|████▏     | 318M/755M [00:16<00:23, 18.7MB/s]\n",
            " 43%|████▎     | 321M/755M [00:16<00:20, 20.9MB/s]\n",
            " 43%|████▎     | 324M/755M [00:16<00:22, 19.2MB/s]\n",
            " 43%|████▎     | 327M/755M [00:16<00:20, 20.7MB/s]\n",
            " 44%|████▎     | 330M/755M [00:17<00:20, 20.6MB/s]\n",
            " 44%|████▍     | 332M/755M [00:17<00:20, 20.4MB/s]\n",
            " 44%|████▍     | 335M/755M [00:17<00:20, 20.3MB/s]\n",
            " 45%|████▍     | 337M/755M [00:17<00:20, 20.3MB/s]\n",
            " 45%|████▍     | 339M/755M [00:17<00:20, 20.1MB/s]\n",
            " 45%|████▌     | 341M/755M [00:17<00:20, 20.3MB/s]\n",
            " 45%|████▌     | 343M/755M [00:17<00:22, 18.6MB/s]\n",
            " 46%|████▌     | 346M/755M [00:17<00:20, 20.4MB/s]\n",
            " 46%|████▌     | 349M/755M [00:18<00:20, 20.0MB/s]\n",
            " 46%|████▋     | 351M/755M [00:18<00:19, 20.3MB/s]\n",
            " 47%|████▋     | 353M/755M [00:18<00:21, 18.3MB/s]\n",
            " 47%|████▋     | 356M/755M [00:18<00:21, 18.7MB/s]\n",
            " 48%|████▊     | 359M/755M [00:18<00:19, 20.7MB/s]\n",
            " 48%|████▊     | 362M/755M [00:18<00:21, 18.6MB/s]\n",
            " 48%|████▊     | 365M/755M [00:18<00:20, 19.0MB/s]\n",
            " 49%|████▉     | 368M/755M [00:18<00:18, 20.5MB/s]\n",
            " 49%|████▉     | 371M/755M [00:19<00:18, 20.3MB/s]\n",
            " 49%|████▉     | 373M/755M [00:19<00:18, 20.3MB/s]\n",
            " 50%|████▉     | 375M/755M [00:19<00:22, 16.6MB/s]\n",
            " 50%|█████     | 377M/755M [00:19<00:24, 15.6MB/s]\n",
            " 50%|█████     | 380M/755M [00:19<00:24, 15.5MB/s]\n",
            " 51%|█████     | 383M/755M [00:19<00:21, 17.6MB/s]\n",
            " 51%|█████     | 385M/755M [00:19<00:20, 18.0MB/s]\n",
            " 51%|█████▏    | 387M/755M [00:20<00:19, 18.8MB/s]\n",
            " 52%|█████▏    | 390M/755M [00:20<00:19, 18.8MB/s]\n",
            " 52%|█████▏    | 392M/755M [00:20<00:20, 17.7MB/s]\n",
            " 52%|█████▏    | 395M/755M [00:20<00:17, 20.0MB/s]\n",
            " 53%|█████▎    | 398M/755M [00:20<00:19, 18.4MB/s]\n",
            " 53%|█████▎    | 401M/755M [00:20<00:17, 20.3MB/s]\n",
            " 53%|█████▎    | 404M/755M [00:20<00:17, 20.1MB/s]\n",
            " 54%|█████▍    | 406M/755M [00:21<00:17, 20.2MB/s]\n",
            " 54%|█████▍    | 408M/755M [00:21<00:17, 20.0MB/s]\n",
            " 54%|█████▍    | 411M/755M [00:21<00:16, 20.3MB/s]\n",
            " 55%|█████▍    | 413M/755M [00:21<00:18, 18.5MB/s]\n",
            " 55%|█████▌    | 416M/755M [00:21<00:16, 20.0MB/s]\n",
            " 55%|█████▌    | 418M/755M [00:21<00:16, 20.0MB/s]\n",
            " 56%|█████▌    | 420M/755M [00:21<00:16, 20.2MB/s]\n",
            " 56%|█████▌    | 423M/755M [00:21<00:17, 19.5MB/s]\n",
            " 56%|█████▋    | 425M/755M [00:21<00:16, 20.2MB/s]\n",
            " 57%|█████▋    | 427M/755M [00:22<00:17, 18.9MB/s]\n",
            " 57%|█████▋    | 430M/755M [00:22<00:16, 19.8MB/s]\n",
            " 57%|█████▋    | 432M/755M [00:22<00:16, 19.3MB/s]\n",
            " 58%|█████▊    | 435M/755M [00:22<00:16, 20.0MB/s]\n",
            " 58%|█████▊    | 437M/755M [00:22<00:16, 19.6MB/s]\n",
            " 58%|█████▊    | 439M/755M [00:22<00:16, 19.7MB/s]\n",
            " 58%|█████▊    | 441M/755M [00:22<00:16, 18.8MB/s]\n",
            " 59%|█████▉    | 444M/755M [00:22<00:15, 20.0MB/s]\n",
            " 59%|█████▉    | 446M/755M [00:23<00:16, 18.8MB/s]\n",
            " 59%|█████▉    | 448M/755M [00:23<00:15, 20.3MB/s]\n",
            " 60%|█████▉    | 450M/755M [00:23<00:16, 18.8MB/s]\n",
            " 60%|██████    | 453M/755M [00:23<00:16, 18.5MB/s]\n",
            " 60%|██████    | 456M/755M [00:23<00:14, 20.4MB/s]\n",
            " 61%|██████    | 459M/755M [00:23<00:14, 20.3MB/s]\n",
            " 61%|██████    | 461M/755M [00:23<00:14, 20.3MB/s]\n",
            " 61%|██████▏   | 463M/755M [00:23<00:14, 20.2MB/s]\n",
            " 62%|██████▏   | 465M/755M [00:23<00:14, 19.4MB/s]\n",
            " 62%|██████▏   | 467M/755M [00:24<00:14, 19.7MB/s]\n",
            " 62%|██████▏   | 469M/755M [00:24<00:15, 18.3MB/s]\n",
            " 63%|██████▎   | 472M/755M [00:24<00:14, 19.9MB/s]\n",
            " 63%|██████▎   | 474M/755M [00:24<00:15, 18.3MB/s]\n",
            " 63%|██████▎   | 477M/755M [00:24<00:13, 20.0MB/s]\n",
            " 64%|██████▎   | 480M/755M [00:24<00:14, 19.6MB/s]\n",
            " 64%|██████▍   | 482M/755M [00:24<00:13, 19.9MB/s]\n",
            " 64%|██████▍   | 484M/755M [00:24<00:14, 19.3MB/s]\n",
            " 64%|██████▍   | 487M/755M [00:25<00:14, 18.2MB/s]\n",
            " 65%|██████▍   | 490M/755M [00:25<00:13, 20.3MB/s]\n",
            " 65%|██████▌   | 492M/755M [00:25<00:14, 18.7MB/s]\n",
            " 66%|██████▌   | 495M/755M [00:25<00:12, 20.4MB/s]\n",
            " 66%|██████▌   | 498M/755M [00:25<00:12, 20.4MB/s]\n",
            " 66%|██████▋   | 501M/755M [00:25<00:12, 20.4MB/s]\n",
            " 67%|██████▋   | 503M/755M [00:25<00:12, 20.3MB/s]\n",
            " 67%|██████▋   | 505M/755M [00:25<00:12, 20.3MB/s]\n",
            " 67%|██████▋   | 507M/755M [00:26<00:12, 19.1MB/s]\n",
            " 68%|██████▊   | 510M/755M [00:26<00:12, 20.4MB/s]\n",
            " 68%|██████▊   | 512M/755M [00:26<00:12, 19.3MB/s]\n",
            " 68%|██████▊   | 514M/755M [00:26<00:11, 20.4MB/s]\n",
            " 68%|██████▊   | 516M/755M [00:26<00:12, 19.6MB/s]\n",
            " 69%|██████▉   | 519M/755M [00:26<00:11, 20.4MB/s]\n",
            " 69%|██████▉   | 521M/755M [00:26<00:12, 19.2MB/s]\n",
            " 69%|██████▉   | 524M/755M [00:26<00:11, 20.3MB/s]\n",
            " 70%|██████▉   | 526M/755M [00:26<00:11, 20.1MB/s]\n",
            " 70%|██████▉   | 528M/755M [00:27<00:11, 20.1MB/s]\n",
            " 70%|███████   | 530M/755M [00:27<00:12, 18.3MB/s]\n",
            " 71%|███████   | 533M/755M [00:27<00:11, 20.1MB/s]\n",
            " 71%|███████   | 536M/755M [00:27<00:11, 19.7MB/s]\n",
            " 71%|███████▏  | 538M/755M [00:27<00:10, 19.8MB/s]\n",
            " 72%|███████▏  | 540M/755M [00:27<00:10, 19.7MB/s]\n",
            " 72%|███████▏  | 542M/755M [00:27<00:11, 17.9MB/s]\n",
            " 72%|███████▏  | 545M/755M [00:27<00:10, 19.9MB/s]\n",
            " 73%|███████▎  | 548M/755M [00:28<00:10, 19.9MB/s]\n",
            " 73%|███████▎  | 550M/755M [00:28<00:10, 20.0MB/s]\n",
            " 73%|███████▎  | 552M/755M [00:28<00:10, 20.2MB/s]\n",
            " 73%|███████▎  | 554M/755M [00:28<00:10, 19.7MB/s]\n",
            " 74%|███████▍  | 557M/755M [00:28<00:09, 20.2MB/s]\n",
            " 74%|███████▍  | 559M/755M [00:28<00:09, 20.0MB/s]\n",
            " 74%|███████▍  | 562M/755M [00:28<00:09, 20.2MB/s]\n",
            " 75%|███████▍  | 564M/755M [00:28<00:09, 19.6MB/s]\n",
            " 75%|███████▌  | 566M/755M [00:29<00:09, 20.2MB/s]\n",
            " 75%|███████▌  | 568M/755M [00:29<00:09, 19.0MB/s]\n",
            " 76%|███████▌  | 571M/755M [00:29<00:09, 20.4MB/s]\n",
            " 76%|███████▌  | 573M/755M [00:29<00:09, 19.0MB/s]\n",
            " 76%|███████▋  | 576M/755M [00:29<00:08, 20.4MB/s]\n",
            " 77%|███████▋  | 578M/755M [00:29<00:08, 20.1MB/s]\n",
            " 77%|███████▋  | 580M/755M [00:29<00:09, 18.1MB/s]\n",
            " 77%|███████▋  | 584M/755M [00:29<00:08, 19.9MB/s]\n",
            " 78%|███████▊  | 586M/755M [00:29<00:08, 20.0MB/s]\n",
            " 78%|███████▊  | 588M/755M [00:30<00:08, 20.0MB/s]\n",
            " 78%|███████▊  | 590M/755M [00:30<00:08, 20.1MB/s]\n",
            " 78%|███████▊  | 592M/755M [00:30<00:08, 19.2MB/s]\n",
            " 79%|███████▉  | 595M/755M [00:30<00:07, 20.2MB/s]\n",
            " 79%|███████▉  | 597M/755M [00:30<00:08, 18.6MB/s]\n",
            " 79%|███████▉  | 599M/755M [00:30<00:08, 18.4MB/s]\n",
            " 80%|███████▉  | 601M/755M [00:30<00:07, 19.5MB/s]\n",
            " 80%|████████  | 605M/755M [00:30<00:07, 19.2MB/s]\n",
            " 81%|████████  | 608M/755M [00:31<00:07, 20.9MB/s]\n",
            " 81%|████████  | 610M/755M [00:31<00:07, 19.0MB/s]\n",
            " 81%|████████▏ | 613M/755M [00:31<00:06, 20.9MB/s]\n",
            " 82%|████████▏ | 616M/755M [00:31<00:06, 20.4MB/s]\n",
            " 82%|████████▏ | 619M/755M [00:31<00:06, 20.3MB/s]\n",
            " 82%|████████▏ | 621M/755M [00:31<00:06, 20.0MB/s]\n",
            " 83%|████████▎ | 623M/755M [00:31<00:07, 18.2MB/s]\n",
            " 83%|████████▎ | 626M/755M [00:31<00:06, 20.1MB/s]\n",
            " 83%|████████▎ | 629M/755M [00:32<00:06, 18.4MB/s]\n",
            " 84%|████████▎ | 632M/755M [00:32<00:06, 20.4MB/s]\n",
            " 84%|████████▍ | 634M/755M [00:32<00:06, 19.2MB/s]\n",
            " 84%|████████▍ | 637M/755M [00:32<00:05, 20.6MB/s]\n",
            " 85%|████████▍ | 640M/755M [00:32<00:05, 19.9MB/s]\n",
            " 85%|████████▌ | 642M/755M [00:32<00:05, 19.8MB/s]\n",
            " 85%|████████▌ | 644M/755M [00:32<00:05, 18.8MB/s]\n",
            " 86%|████████▌ | 646M/755M [00:33<00:06, 17.9MB/s]\n",
            " 86%|████████▌ | 648M/755M [00:33<00:06, 15.8MB/s]\n",
            " 86%|████████▌ | 650M/755M [00:33<00:06, 16.7MB/s]\n",
            " 86%|████████▋ | 653M/755M [00:33<00:05, 17.8MB/s]\n",
            " 87%|████████▋ | 655M/755M [00:33<00:05, 17.9MB/s]\n",
            " 87%|████████▋ | 657M/755M [00:33<00:05, 19.0MB/s]\n",
            " 87%|████████▋ | 660M/755M [00:33<00:05, 18.9MB/s]\n",
            " 88%|████████▊ | 662M/755M [00:33<00:04, 19.6MB/s]\n",
            " 88%|████████▊ | 664M/755M [00:34<00:04, 18.9MB/s]\n",
            " 88%|████████▊ | 667M/755M [00:34<00:04, 20.0MB/s]\n",
            " 89%|████████▊ | 669M/755M [00:34<00:04, 19.4MB/s]\n",
            " 89%|████████▉ | 672M/755M [00:34<00:04, 20.1MB/s]\n",
            " 89%|████████▉ | 674M/755M [00:34<00:04, 19.6MB/s]\n",
            " 90%|████████▉ | 676M/755M [00:34<00:03, 20.1MB/s]\n",
            " 90%|████████▉ | 678M/755M [00:34<00:03, 19.5MB/s]\n",
            " 90%|█████████ | 681M/755M [00:34<00:03, 20.1MB/s]\n",
            " 91%|█████████ | 683M/755M [00:34<00:03, 19.3MB/s]\n",
            " 91%|█████████ | 686M/755M [00:35<00:03, 20.3MB/s]\n",
            " 91%|█████████ | 688M/755M [00:35<00:03, 19.2MB/s]\n",
            " 91%|█████████▏| 690M/755M [00:35<00:03, 18.3MB/s]\n",
            " 92%|█████████▏| 694M/755M [00:35<00:02, 20.5MB/s]\n",
            " 92%|█████████▏| 696M/755M [00:35<00:03, 18.6MB/s]\n",
            " 93%|█████████▎| 699M/755M [00:35<00:02, 20.6MB/s]\n",
            " 93%|█████████▎| 702M/755M [00:35<00:02, 19.9MB/s]\n",
            " 93%|█████████▎| 705M/755M [00:36<00:02, 20.6MB/s]\n",
            " 94%|█████████▎| 707M/755M [00:36<00:02, 20.5MB/s]\n",
            " 94%|█████████▍| 710M/755M [00:36<00:02, 20.4MB/s]\n",
            " 94%|█████████▍| 712M/755M [00:36<00:02, 19.8MB/s]\n",
            " 95%|█████████▍| 714M/755M [00:36<00:02, 19.8MB/s]\n",
            " 95%|█████████▍| 716M/755M [00:36<00:02, 19.0MB/s]\n",
            " 95%|█████████▌| 719M/755M [00:36<00:01, 18.2MB/s]\n",
            " 96%|█████████▌| 722M/755M [00:36<00:01, 20.2MB/s]\n",
            " 96%|█████████▌| 725M/755M [00:37<00:01, 18.6MB/s]\n",
            " 96%|█████████▋| 728M/755M [00:37<00:01, 20.5MB/s]\n",
            " 97%|█████████▋| 730M/755M [00:37<00:01, 20.0MB/s]\n",
            " 97%|█████████▋| 732M/755M [00:37<00:01, 20.1MB/s]\n",
            " 97%|█████████▋| 735M/755M [00:37<00:01, 18.9MB/s]\n",
            " 98%|█████████▊| 737M/755M [00:37<00:00, 18.3MB/s]\n",
            " 98%|█████████▊| 740M/755M [00:37<00:00, 20.5MB/s]\n",
            " 98%|█████████▊| 743M/755M [00:37<00:00, 20.4MB/s]\n",
            " 99%|█████████▉| 746M/755M [00:38<00:00, 20.4MB/s]\n",
            " 99%|█████████▉| 748M/755M [00:38<00:00, 18.9MB/s]\n",
            " 99%|█████████▉| 751M/755M [00:38<00:00, 20.4MB/s]\n",
            "100%|█████████▉| 753M/755M [00:38<00:00, 19.9MB/s]\n",
            "100%|██████████| 755M/755M [00:38<00:00, 19.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "#%cd\n",
        "#%cd /content/gdrive/MyDrive\n",
        "import os\n",
        "if not (os.path.isdir(\"SAMSUNG_ImageCaptioning\")):\n",
        "    os.mkdir(\"SAMSUNG_ImageCaptioning\")\n",
        "%cd SAMSUNG_ImageCaptioning\n",
        "\n",
        "\n",
        "\n",
        "#Download pretrained weights, test sets\n",
        "!gdown --id 18iwTb7eSaX3wMtt5zNWl-r8JBZQ35SIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svsC8m897tgO",
        "outputId": "a0d9d5bb-7a34-4fd3-94bb-08f8de1d6963"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "!unzip -o SAMSUNG_ImageCaptioning.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIavgXcfauqz"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qwnfNGVoagq0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import skimage.transform\n",
        "import argparse\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import warnings\n",
        "import requests\n",
        "from models import Encoder as Encoder_answer\n",
        "from models import Attention as Attention_answer\n",
        "from models import DecoderWithAttention as Decoder_answer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4mQBs0HjFiw",
        "outputId": "cf17e966-ac6f-4c63-ab27-4df749a5433f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Set GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "# Load word map (word2ix)\n",
        "word_map='WORDMAP_coco_5_cap_per_img_5_min_word_freq.json'\n",
        "with open(word_map, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fFZwiMVT6Pfv"
      },
      "outputs": [],
      "source": [
        "#Fix seed\n",
        "random_seed=0\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve3WFmMvfOmX"
      },
      "source": [
        "1.Implement Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaM_ZPRaE0GQ"
      },
      "source": [
        "Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "1hmL2YTH9QL2",
        "outputId": "6538af7d-2c44-43fe-b7da-8933a1dafecd"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "        \"\"\"\n",
        "\n",
        "        ##################################### Fill in blank lines ############################################\n",
        "\n",
        "        # Fill in the empty line to complete following 'TODO'\n",
        "        # TODO: complete forwarding path of Encoder network and calculate final 'output' feature\n",
        "        # 1. calculate 'out' using 'images' $ 'self.resnet'\n",
        "        # 2. calculate 'out' using 'out' & 'self.adaptive_pool'\n",
        "        # 3. modify shape of 'out' by using tensor.permute() function\n",
        "\n",
        "\n",
        "        out = self.resnet(images) # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out) # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1) # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xn_Jp8rSgTg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v0A7Ld8fSVn"
      },
      "source": [
        "Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oL5TXEz3CJIT"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        ##################################### Fill in blank lines ############################################\n",
        "\n",
        "        # Fill in the empty line to complete following 'TODO'\n",
        "        # TODO:calculate attention_weighted_encoding (context vector) and alpha (weight of each pixel)\n",
        "        # 1. calculate 'embedding1' using 'self.encoder_att' $ 'encoder_output'\n",
        "        # 2. calculate 'embedding2' using 'self.decoder_att' $ 'decoder_hidden'\n",
        "        # 3. calculate 'alpha' using self.softmax & att\n",
        "        # 4. calculate 'attention_weighted_encoding' using encoder_out & alpha\n",
        "        # hint for 4: you can use tensor.unsqueeze(dim) to add new dimension\n",
        "\n",
        "\n",
        "        embedding1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        embedding2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim) -> (batch_size, 1, attention_dim)\n",
        "        att = self.full_att(self.relu(embedding1 + embedding2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)       # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1) # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30JxSw0OfX2g"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Zd9caLV3CLHb"
      },
      "outputs": [],
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Xcph11U-fdhH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to C:\\Users\\AI_15/.cache\\torch\\hub\\checkpoints\\resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:08<00:00, 20.2MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid Encoder!!\n",
            "Valid Decoder!!\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "embed_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "\n",
        "import copy\n",
        "\n",
        "\n",
        "encoder=Encoder().to(device)\n",
        "encoder.eval()\n",
        "decoder=DecoderWithAttention(attention_dim=attention_dim, embed_dim=embed_dim, decoder_dim=decoder_dim, vocab_size=len(word_map)).to(device)\n",
        "decoder.eval()\n",
        "\n",
        "\n",
        "encoder_answer=Encoder_answer().to(device)\n",
        "encoder_answer.eval()\n",
        "\n",
        "decoder_answer=Decoder_answer(attention_dim=attention_dim, embed_dim=embed_dim, decoder_dim=decoder_dim, vocab_size=len(word_map)).to(device)\n",
        "decoder_answer.eval()\n",
        "\n",
        "def make_parameters_same(net,answer):\n",
        "  state_dict=net.state_dict()\n",
        "  for name,weights in answer.named_parameters():\n",
        "    state_dict[name]=weights\n",
        "  net.load_state_dict(state_dict)\n",
        "  return net\n",
        "\n",
        "encoder=make_parameters_same(encoder,encoder_answer)\n",
        "decoder=make_parameters_same(decoder,decoder_answer)\n",
        "\n",
        "\n",
        "#Check Encoder\n",
        "img=torch.randn(5,3,256,256).to(device)\n",
        "encoder_out1=encoder(img)\n",
        "encoder_out2=encoder_answer(img)\n",
        "if (abs(encoder_out1-encoder_out2).sum()==0):\n",
        "  print(\"Valid Encoder!!\")\n",
        "  #Check Decoder\n",
        "  captions=torch.randint(0,10,(5,13),dtype=int).to(device)\n",
        "  caption_lengths=torch.randint(0,10,(5,1),dtype=int).to(device)\n",
        "  a0,a1,a2,a3,a4=decoder(encoder_out1.detach().clone(),copy.deepcopy(captions),copy.deepcopy(caption_lengths))\n",
        "  b0,b1,b2,b3,b4=decoder_answer(encoder_out1,captions,caption_lengths)\n",
        "  if ((abs(a0-b0).sum()+abs(a1-b1).sum()+abs(a3-b3).sum()+abs(a4-b4).sum())==0):\n",
        "    print(\"Valid Decoder!!\")\n",
        "  else:\n",
        "    print(\"Need to fix Decoder\")\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"Need to fix Encoder\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jNk9y79vWGj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u79_Uy8yhVC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktC_n0IFvTWP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vut1gy3mE3gI"
      },
      "source": [
        "2.Implement beam search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TF9QFXv0COJB"
      },
      "outputs": [],
      "source": [
        "def caption_image_beam_search(encoder, decoder, url, word_map, beam_size=3):\n",
        "    \"\"\"\n",
        "    Reads an image and captions it with beam search.\n",
        "\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param url: url of  image\n",
        "    :param word_map: word map\n",
        "    :param beam_size: number of sequences to consider at each decode-step\n",
        "    :return: caption, weights for visualization\n",
        "    \"\"\"\n",
        "\n",
        "    k = beam_size\n",
        "    vocab_size = len(word_map)\n",
        "\n",
        "    # Read image and process\n",
        "\n",
        "    image_nparray = np.asarray(bytearray(requests.get(url).content), dtype=np.uint8)\n",
        "    img = cv2.imdecode(image_nparray, cv2.IMREAD_COLOR)\n",
        "\n",
        "    #img = io.imread(image_path)\n",
        "    if len(img.shape) == 2:\n",
        "        img = img[:, :, np.newaxis]\n",
        "        img = np.concatenate([img, img, img], axis=2)\n",
        "    img = skimage.transform.resize(img, (256, 256))\n",
        "    img = img.transpose(2, 0, 1)\n",
        "    #img = img / 255.\n",
        "    img = torch.FloatTensor(img).to(device)\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    transform = transforms.Compose([normalize])\n",
        "    image = transform(img)  # (3, 256, 256)\n",
        "\n",
        "    # Encode\n",
        "    ############################# Fill in blank lines ###########################################################\n",
        "    image = image.unsqueeze(0)  # (1, 3, 256, 256). Use .unsqueeze(K) function to add new dimension into 'k' th dim\n",
        "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "    enc_image_size = encoder_out.size(1)\n",
        "    encoder_dim = encoder_out.size(3)\n",
        "\n",
        "    # Flatten encoding\n",
        "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "    num_pixels = encoder_out.size(1)\n",
        "\n",
        "    # We'll treat the problem as having a batch size of k\n",
        "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "    # Tensor to store top k previous words at each step; now they're just <start>\n",
        "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences; now they're just <start>\n",
        "    seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' scores; now they're just 0\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
        "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
        "\n",
        "    # Lists to store completed sequences, their alphas and scores\n",
        "    complete_seqs = list()\n",
        "    complete_seqs_alpha = list()\n",
        "    complete_seqs_scores = list()\n",
        "\n",
        "    # Start decoding\n",
        "    step = 1\n",
        "    h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "    while True:\n",
        "\n",
        "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
        "\n",
        "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "        awe = gate * awe\n",
        "\n",
        "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "        scores = decoder.fc(h)  # (s, vocab_size)\n",
        "        scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "        # Add\n",
        "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "        if step == 1:\n",
        "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "        else:\n",
        "            # Unroll and find top scores, and their unrolled indices\n",
        "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "        # Convert unrolled indices to actual indices of scores\n",
        "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
        "        next_word_inds = top_k_words % vocab_size  # (s)\n",
        "        prev_word_inds = prev_word_inds.type(torch.LongTensor).to(device)\n",
        "        next_word_inds = next_word_inds.type(torch.LongTensor).to(device)\n",
        "\n",
        "        # Add new words to sequences, alphas\n",
        "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
        "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
        "\n",
        "        # Which sequences are incomplete (didn't reach <end>)?\n",
        "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                           next_word != word_map['<end>']]\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        # Set aside complete sequences\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "        k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "        # Proceed with incomplete sequences\n",
        "        if k == 0:\n",
        "            break\n",
        "        seqs = seqs[incomplete_inds]\n",
        "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
        "        h = h[prev_word_inds[incomplete_inds]]\n",
        "        c = c[prev_word_inds[incomplete_inds]]\n",
        "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "        # Break if things have been going on too long\n",
        "        if step > 50:\n",
        "            break\n",
        "        step += 1\n",
        "\n",
        "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "    seq = complete_seqs[i]\n",
        "    alphas = complete_seqs_alpha[i]\n",
        "\n",
        "    return seq, alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kIH0AqZfPWAg"
      },
      "outputs": [],
      "source": [
        "def visualize_att(image, seq, alphas, rev_word_map, smooth=True):\n",
        "    \"\"\"\n",
        "    Visualizes caption with weights at every word.\n",
        "\n",
        "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
        "\n",
        "    :param image_path: path to image that has been captioned\n",
        "    :param seq: caption\n",
        "    :param alphas: weights\n",
        "    :param rev_word_map: reverse word mapping, i.e. ix2word\n",
        "    :param smooth: smooth weights?\n",
        "    \"\"\"\n",
        "    #image = Image.open(image_path)\n",
        "    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n",
        "\n",
        "    words = [rev_word_map[ind] for ind in seq]\n",
        "\n",
        "    for t in range(len(words)):\n",
        "        if t > 50:\n",
        "            break\n",
        "        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n",
        "\n",
        "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
        "        plt.imshow(image)\n",
        "        current_alpha = alphas[t, :]\n",
        "        if smooth:\n",
        "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n",
        "        else:\n",
        "            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n",
        "        if t == 0:\n",
        "            plt.imshow(alpha, alpha=0)\n",
        "        else:\n",
        "            plt.imshow(alpha, alpha=0.8)\n",
        "        plt.set_cmap(cm.Greys_r)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y7XAHsFbZCs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fZMJUNJbZEz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnaEdf_4bZju"
      },
      "source": [
        "Load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Wh10E4VBbZju"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings(action='ignore') \n",
        "MODEL_PATH='BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
        "\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=str(device))\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "\n",
        "warnings.filterwarnings(action='default') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "i7QsvrC1xSwV"
      },
      "outputs": [],
      "source": [
        "def caption_image( url,beam_size,encoder, decoder, word_map, rev_word_map):\n",
        "  img = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "  # Encode, decode with attention and beam search\n",
        "  seq, alphas = caption_image_beam_search(encoder, decoder, url, word_map, beam_size)\n",
        "  alphas = torch.FloatTensor(alphas)\n",
        "\n",
        "  # Visualize caption and attention of best sequence\n",
        "  visualize_att(img, seq, alphas, rev_word_map,True)\n",
        "  warnings.filterwarnings(action='default') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrwjVdcuE9ag"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRLN_pA0E-Ap"
      },
      "source": [
        "Check the image captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s2kqZ42FLwA"
      },
      "source": [
        "Get image url from internet( ex) https://cocodataset.org/#explore)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pWLRuktlz-Cj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\AI_15\\AppData\\Local\\Temp\\ipykernel_6180\\3438225867.py:14: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
            "  image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Number of rows must be a positive integer, not 2.0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CV_Caption_VQA\\Day1\\ImageCaptioning_Exercise.ipynb 셀 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=0'>1</a>\u001b[0m caption_image(url\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://img4.yna.co.kr/photo/reuters/2021/07/22/PRU20210722044001055_P4.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m,beam_size\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,encoder\u001b[39m=\u001b[39;49mencoder,decoder\u001b[39m=\u001b[39;49mdecoder,word_map\u001b[39m=\u001b[39;49mword_map, rev_word_map\u001b[39m=\u001b[39;49mrev_word_map)\n",
            "\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CV_Caption_VQA\\Day1\\ImageCaptioning_Exercise.ipynb 셀 33\u001b[0m in \u001b[0;36mcaption_image\u001b[1;34m(url, beam_size, encoder, decoder, word_map, rev_word_map)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=5'>6</a>\u001b[0m alphas \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(alphas)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=7'>8</a>\u001b[0m \u001b[39m# Visualize caption and attention of best sequence\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=8'>9</a>\u001b[0m visualize_att(img, seq, alphas, rev_word_map,\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=9'>10</a>\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(action\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\AI_15\\momo\\python_prj\\2022_AI_Expert\\CV_Caption_VQA\\Day1\\ImageCaptioning_Exercise.ipynb 셀 33\u001b[0m in \u001b[0;36mvisualize_att\u001b[1;34m(image, seq, alphas, rev_word_map, smooth)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m>\u001b[39m \u001b[39m50\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=19'>20</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=20'>21</a>\u001b[0m plt\u001b[39m.\u001b[39;49msubplot(np\u001b[39m.\u001b[39;49mceil(\u001b[39mlen\u001b[39;49m(words) \u001b[39m/\u001b[39;49m \u001b[39m5.\u001b[39;49m), \u001b[39m5\u001b[39;49m, t \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=22'>23</a>\u001b[0m plt\u001b[39m.\u001b[39mtext(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (words[t]), color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m'\u001b[39m, backgroundcolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhite\u001b[39m\u001b[39m'\u001b[39m, fontsize\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI_15/momo/python_prj/2022_AI_Expert/CV_Caption_VQA/Day1/ImageCaptioning_Exercise.ipynb#ch0000032?line=23'>24</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(image)\n",
            "File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\pyplot.py:1289\u001b[0m, in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1286\u001b[0m fig \u001b[39m=\u001b[39m gcf()\n\u001b[0;32m   1288\u001b[0m \u001b[39m# First, search for an existing subplot with a matching spec.\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m key \u001b[39m=\u001b[39m SubplotSpec\u001b[39m.\u001b[39;49m_from_subplot_args(fig, args)\n\u001b[0;32m   1291\u001b[0m \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m fig\u001b[39m.\u001b[39maxes:\n\u001b[0;32m   1292\u001b[0m     \u001b[39m# if we found an axes at the position sort out if we can re-use it\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(ax, \u001b[39m'\u001b[39m\u001b[39mget_subplotspec\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m ax\u001b[39m.\u001b[39mget_subplotspec() \u001b[39m==\u001b[39m key:\n\u001b[0;32m   1294\u001b[0m         \u001b[39m# if the user passed no kwargs, re-use\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\gridspec.py:597\u001b[0m, in \u001b[0;36mSubplotSpec._from_subplot_args\u001b[1;34m(figure, args)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msubplot() takes 1 or 3 positional arguments but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m}\u001b[39;00m\u001b[39m were given\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 597\u001b[0m gs \u001b[39m=\u001b[39m GridSpec\u001b[39m.\u001b[39;49m_check_gridspec_exists(figure, rows, cols)\n\u001b[0;32m    598\u001b[0m \u001b[39mif\u001b[39;00m gs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    599\u001b[0m     gs \u001b[39m=\u001b[39m GridSpec(rows, cols, figure\u001b[39m=\u001b[39mfigure)\n",
            "File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\gridspec.py:225\u001b[0m, in \u001b[0;36mGridSpecBase._check_gridspec_exists\u001b[1;34m(figure, nrows, ncols)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[39mreturn\u001b[39;00m gs\n\u001b[0;32m    224\u001b[0m \u001b[39m# else gridspec not found:\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m \u001b[39mreturn\u001b[39;00m GridSpec(nrows, ncols, figure\u001b[39m=\u001b[39;49mfigure)\n",
            "File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\gridspec.py:385\u001b[0m, in \u001b[0;36mGridSpec.__init__\u001b[1;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhspace \u001b[39m=\u001b[39m hspace\n\u001b[0;32m    383\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure \u001b[39m=\u001b[39m figure\n\u001b[1;32m--> 385\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(nrows, ncols,\n\u001b[0;32m    386\u001b[0m                  width_ratios\u001b[39m=\u001b[39;49mwidth_ratios,\n\u001b[0;32m    387\u001b[0m                  height_ratios\u001b[39m=\u001b[39;49mheight_ratios)\n",
            "File \u001b[1;32mc:\\Users\\AI_15\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\gridspec.py:49\u001b[0m, in \u001b[0;36mGridSpecBase.__init__\u001b[1;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m----------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39m    If not given, all rows will have the same height.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(nrows, Integral) \u001b[39mor\u001b[39;00m nrows \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     50\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of rows must be a positive integer, not \u001b[39m\u001b[39m{\u001b[39;00mnrows\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ncols, Integral) \u001b[39mor\u001b[39;00m ncols \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of columns must be a positive integer, not \u001b[39m\u001b[39m{\u001b[39;00mncols\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: Number of rows must be a positive integer, not 2.0"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "caption_image(url=\"https://img4.yna.co.kr/photo/reuters/2021/07/22/PRU20210722044001055_P4.jpg\",beam_size=25,encoder=encoder,decoder=decoder,word_map=word_map, rev_word_map=rev_word_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGHALBnFOekl"
      },
      "outputs": [],
      "source": [
        "url=input()\n",
        "caption_image(url=url,beam_size=25,encoder=encoder,decoder=decoder,word_map=word_map, rev_word_map=rev_word_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydipXmcSOemd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ImageCaptioning_Exercise.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
