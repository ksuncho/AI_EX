{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"iklGeXOmguf0"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"p0XzyD4pguf0"},"source":["\n","Autograd: Automatic Differentiation\n","===================================\n","\n","Central to all neural networks in PyTorch is the ``autograd`` package.\n","\n","The ``autograd`` package provides automatic differentiation for all operations\n","on Tensors. \n","\n","\n","Tensor\n","--------\n","\n","``torch.Tensor`` is the central class of the package. If you set its attribute\n","``.requires_grad`` as ``True``, it starts to track all operations on it. When\n","you finish your computation you can call ``.backward()`` and have all the\n","gradients computed automatically. The gradient for this tensor will be\n","accumulated into ``.grad`` attribute.\n","\n","To stop a tensor from tracking history, you can call ``.detach()`` to detach\n","it from the computation history, and to prevent future computation from being\n","tracked.\n","\n","To prevent tracking history (and using memory), you can also wrap the code block\n","in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n","model because the model may have trainable parameters with\n","``requires_grad=True``, but for which we don't need the gradients.\n","\n","If you want to compute the derivatives, you can call ``.backward()`` on\n","a ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element\n","data), you don’t need to specify any arguments to ``backward()``,\n","however if it has more elements, you need to specify a ``gradient``\n","argument that is a tensor of matching shape.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gSMMd57Nguf0"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","metadata":{"id":"9EPpwIcvguf0"},"source":["Create a tensor and set ``requires_grad=True`` to track computation with it\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NN81Q1X1guf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n"]}],"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"U2a0tsUFguf0"},"source":["Do a tensor operation:\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"797qSzTfguf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n"]}],"source":["y = x + 2\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"FMCThxZwguf0"},"source":["``y`` was created as a result of an operation, so it has a ``grad_fn``.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"UWadXkvbguf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["<AddBackward0 object at 0x00000215CBB97DC0>\n"]}],"source":["print(y.grad_fn)"]},{"cell_type":"markdown","metadata":{"id":"7mTqEndrguf0"},"source":["Do more operations on ``y``\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8gyn0Sm6guf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"]}],"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"]},{"cell_type":"markdown","metadata":{"id":"mqbbSNlgguf0"},"source":["``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n","flag in-place. The input flag defaults to ``False`` if not given.\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZYPQ5m9tguf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n","True\n","<SumBackward0 object at 0x00000215ACE619A0>\n"]}],"source":["a = torch.randn(2, 2)\n","a = ((a * 3) / (a - 1))\n","print(a.requires_grad)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.grad_fn)"]},{"cell_type":"markdown","metadata":{"id":"hVRmDA5Bguf0"},"source":["Gradients\n","---------\n","Let's backprop now.\n","Because ``out`` contains a single scalar, ``out.backward()`` is\n","equivalent to ``out.backward(torch.tensor(1.))``.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tOGGmG1bguf0"},"outputs":[],"source":["out.backward()"]},{"cell_type":"markdown","metadata":{"id":"a7_1Rjqqguf0"},"source":["Print gradients d(out)/dx\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"gB1zU63Bguf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"]}],"source":["print(x.grad)"]},{"cell_type":"markdown","metadata":{"id":"lGhzM9AJguf0"},"source":["You should have got a matrix of ``4.5``. Let’s call the ``out``\n","*Tensor* “$o$”.\n","We have that $o = \\frac{1}{4}\\sum_i z_i$,\n","$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n","Therefore,\n","$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n","$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L3V3qVguguf1"},"source":["Now let's take a look at an example of vector-Jacobian product:\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"jyXY1TVcguf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1085.2428,  -38.4281, -125.7706], grad_fn=<MulBackward0>)\n"]}],"source":["x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","while y.data.norm() < 1000:\n","    y = y * 2\n","\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"f-D4dRScguf1"},"source":["Now in this case ``y`` is no longer a scalar. ``torch.autograd``\n","could not compute the full Jacobian directly, but if we just\n","want the vector-Jacobian product, simply pass the vector to\n","``backward`` as argument:\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"YO9Hr30Gguf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"]}],"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","y.backward(v)\n","\n","print(x.grad)"]},{"cell_type":"markdown","metadata":{"id":"bwsmI_qsguf1"},"source":["You can also stop autograd from tracking history on Tensors\n","with ``.requires_grad=True`` either by wrapping the code block in\n","``with torch.no_grad():``\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"HtMH2wy-guf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","True\n","False\n"]}],"source":["print(x.requires_grad)\n","print((x ** 2).requires_grad)\n","\n","with torch.no_grad():\n","\tprint((x ** 2).requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"2Z5uyZVTguf1"},"source":["Or by using ``.detach()`` to get a new Tensor with the same\n","content but that does not require gradients:\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"g8ytMZWMguf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","False\n","tensor(True)\n"]}],"source":["print(x.requires_grad)\n","y = x.detach()\n","print(y.requires_grad)\n","print(x.eq(y).all())"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"FXH2jcc8xfI0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 0.8086,  1.3430, -0.6632], grad_fn=<MulBackward0>)\n","tensor([ 0.8086,  1.3430, -0.6632])\n"]}],"source":["# TODO: Compute the Jacobian-vector product J(x)v, \n","# where J(x) is the Jacobian of the function f: R^3->R^3, f(x) = x^3 + x^2 + exp(x) at x, \n","# and both x and v are random-sampled vector of size 3\n","x = torch.randn(3, requires_grad=True)\n","v = torch.randn(3)\n","y = x ** 3 + x ** 2 + torch.exp(x)\n","y.backward(v)\n","print((3*x**2 + 2*x + torch.exp(x)) * v)\n","print(x.grad)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"[Day 3-1 (solution)] autograd.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/009cea8b0f40dfcb55e3280f73b06cc2/autograd_tutorial.ipynb","timestamp":1606326591953}]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"bf789d3fe2e48003609d2b27099c8c5750f1d9c6ed54a4f20100144dcd5707b9"}}},"nbformat":4,"nbformat_minor":0}
